{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16 – Natural Language Processing with RNNs and Attention\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alirezatheh/handson-ml3-notes/blob/main/notebooks/16_natural_language_processing_with_rnns_and_attentione.ipynb)\n",
    "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/alirezatheh/handson-ml3-notes/blob/main/notebooks/16_natural_language_processing_with_rnns_and_attention.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Shakespearean Text Using a Character RNN\n",
    "In a famous 2015 blog post titled [“The Unreasonable Effectiveness of Recurrent Neural Networks”](https://homl.info/charrnn), Andrej Karpathy showed how to train an RNN to predict the next character in a sentence. This *char-RNN* can then be used to generate novel text, one character at a time. This is our first example of a *language model*; similar language models, are at the core of modern NLP.\n",
    "\n",
    "**Warning**: This chapter can be very slow without a GPU, so let’s make sure there’s one, or else issue a warning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ekxzo6pOpKzy"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print('No GPU was detected. Neural nets can be very slow without a GPU.')\n",
    "    if 'google.colab' in sys.modules:\n",
    "        print(\n",
    "            'Go to Runtime > Change runtime and select a GPU hardware '\n",
    "            'accelerator.'\n",
    "        )\n",
    "    if 'kaggle_secrets' in sys.modules:\n",
    "        print('Go to Settings > Accelerator and select GPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Training Dataset\n",
    "Let’s download the Shakespeare data from Andrej Karpathy’s [char-rnn project](https://github.com/karpathy/char-rnn/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://homl.info/shakespeare\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n",
      "1130496/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# Shortcut URL\n",
    "shakespeare_url = 'https://homl.info/shakespeare'\n",
    "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "# Shows a short text sample\n",
    "print(shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows all 39 distinct characters (after converting to lower case)\n",
    "''.join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = keras.layers.TextVectorization(\n",
    "    split='character', standardize='lower'\n",
    ")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn this very long sequence into a dataset of windows that we can then use to train a sequence-to-sequence RNN. The targets will be similar to the inputs, but shifted by one time step into the “future”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def to_dataset(\n",
    "    sequence: tf.Tensor,\n",
    "    length: int,\n",
    "    shuffle: Optional[bool] = False,\n",
    "    seed: Optional[int] = None,\n",
    "    batch_size: int = 32,\n",
    ") -> tf.data.Dataset:\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 4,  5,  2, 23]])>,\n",
       "  <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 5,  2, 23,  3]])>)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple example using to_dataset()\n",
    "# There’s just one sample in this dataset: the input represents 'to b'\n",
    "# and the output represents 'o be'\n",
    "list(to_dataset(text_vec_layer(['To be'])[0], length=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% of the text for training, 5% for validation, and 5% for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "keras.utils.set_random_seed(42)\n",
    "train_set = to_dataset(\n",
    "    encoded[:1_000_000], length=length, shuffle=True, seed=42\n",
    ")\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training the Char-RNN Model\n",
    "**Warning**: The following code may take one or two hours to run, depending on our GPU. Without a GPU, it may take over 24 hours. If we don’t want to wait, just skip the next two code cells and run the code below to download a pretrained model.\n",
    "\n",
    "**Note**: The `GRU` class will only use cuDNN acceleration (assuming we have a GPU) when using the default values for the following arguments: `activation`, `recurrent_activation`, `recurrent_dropout`, `unroll`, `use_bias` and `reset_after`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1407s 45ms/step - loss: 1.3873 - accuracy: 0.5754 - val_loss: 1.6155 - val_accuracy: 0.5333\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1376s 44ms/step - loss: 1.2921 - accuracy: 0.5973 - val_loss: 1.5881 - val_accuracy: 0.5401\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1379s 44ms/step - loss: 1.2743 - accuracy: 0.6015 - val_loss: 1.5885 - val_accuracy: 0.5407\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1381s 44ms/step - loss: 1.2654 - accuracy: 0.6031 - val_loss: 1.5701 - val_accuracy: 0.5418\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1379s 44ms/step - loss: 1.2594 - accuracy: 0.6045 - val_loss: 1.5674 - val_accuracy: 0.5450\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1386s 44ms/step - loss: 1.2545 - accuracy: 0.6058 - val_loss: 1.5587 - val_accuracy: 0.5492\n",
      "Epoch 7/10\n",
      "31247/31247 [==============================] - 1381s 44ms/step - loss: 1.2514 - accuracy: 0.6062 - val_loss: 1.5532 - val_accuracy: 0.5460\n",
      "Epoch 8/10\n",
      "31247/31247 [==============================] - 1381s 44ms/step - loss: 1.2485 - accuracy: 0.6067 - val_loss: 1.5522 - val_accuracy: 0.5479\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1382s 44ms/step - loss: 1.2460 - accuracy: 0.6073 - val_loss: 1.5521 - val_accuracy: 0.5497\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1385s 44ms/step - loss: 1.2436 - accuracy: 0.6080 - val_loss: 1.5477 - val_accuracy: 0.5513\n"
     ]
    }
   ],
   "source": [
    "# Ensures reproducibility on CPU\n",
    "keras.utils.set_random_seed(42)\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "        keras.layers.GRU(128, return_sequences=True),\n",
    "        keras.layers.Dense(n_tokens, activation='softmax'),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model_ckpt = keras.callbacks.ModelCheckpoint(\n",
    "    'my_shakespeare_model', monitor='val_accuracy', save_best_only=True\n",
    ")\n",
    "history = model.fit(\n",
    "    train_set, validation_data=valid_set, epochs=10, callbacks=[model_ckpt]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The inputs of the Embedding layer will be 2D tensors of shape [*batch size*, *window length*], the output of the Embedding layer will be a 3D tensor of shape [*batch size*, *window length*, *embedding size*].\n",
    "- Since the input windows overlap, the concept of epoch is not so clear in this case: during each epoch (as implemented by Keras), the model will actually see the same character multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = keras.Sequential(\n",
    "    [\n",
    "        text_vec_layer,\n",
    "        # No <PAD> or <UNK> tokens\n",
    "        keras.layers.Lambda(lambda X: X - 2),\n",
    "        model,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don’t want to wait for training to complete, we can download the pretrained model. Uncomment the last line to use it instead of the model trained above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "url = 'https://github.com/ageron/data/raw/main/shakespeare_model.tgz'\n",
    "path = keras.utils.get_file('shakespeare_model.tgz', url, extract=True)\n",
    "model_path = Path(path).with_name('shakespeare_model')\n",
    "# shakespeare_model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = shakespeare_model.predict(['To be or not to b'])[0, -1]\n",
    "# Choose the most probable character ID\n",
    "y_pred = tf.argmax(y_proba)\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Fake Shakespearean Text\n",
    "To generate new text using the char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it to the end of the text, then give the extended text to the model to guess the next letter, and so on. This is called *greedy decoding*. But in practice this often leads to the same words being repeated over and over again. Instead, we can sample the next character randomly, with a probability equal to the estimated probability, using TensorFlow’s `tf.random.categorical()` function. This will generate more diverse and interesting text. The `categorical()` function samples random class indices, given the class log probabilities (logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 1]])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])\n",
    "keras.utils.set_random_seed(42)\n",
    "# Draw 8 samples\n",
    "tf.random.categorical(log_probas, num_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have more control over the diversity of the generated text, we can divide the logits by a number called the *temperature*. A temperature close to zero favors high-probability characters, while a high temperature gives all characters an equal probability. Lower temperatures are typically preferred when generating fairly rigid and precise text, such as mathematical equations, while higher temperatures are preferred when generating more diverse and creative text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text: str, temperature: float = 1) -> str:\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text: str, n_chars: int = 50, temperature: float = 1) -> str:\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures reproducibility on CPU\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be the duke\n",
      "as it is a proper strange death,\n",
      "and the\n"
     ]
    }
   ],
   "source": [
    "print(extend_text('To be or not to be', temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to behold?\n",
      "\n",
      "second push:\n",
      "gremio, lord all, a sistermen,\n"
     ]
    }
   ],
   "source": [
    "print(extend_text('To be or not to be', temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to bef ,mt'&o3fpadm!$\n",
      "wh!nse?bws3est--vgerdjw?c-y-ewznq\n"
     ]
    }
   ],
   "source": [
    "print(extend_text('To be or not to be', temperature=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate more convincing text, a common technique is to sample only from the top *k* characters, or only from the smallest set of top characters whose total probability exceeds some threshold (this is called *nucleus sampling*). Alternatively, we could try using *beam search*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stateful RNN\n",
    "We have only used *stateless RNNs*: at each training iteration the model starts with a hidden state full of zeros, then it updates this state at each time step, and after the last time step, it throws it away as it is not needed anymore. If we instruct the RNN to preserve this final state after processing a training batch and use it as the initial state for the next training batch, the model could learn long-term patterns despite only backpropagating through short sequences. This is called a *stateful RNN*.\n",
    "\n",
    "A stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off. So we need to use sequential and nonoverlapping input sequences. When creating the `tf.data.Dataset`, we must therefore use `shift=length` when calling the `window()` method. Moreover, we must not call the `shuffle()` method.\n",
    "\n",
    "Batching is much harder. If we were to call `batch(32)`, then 32 consecutive windows would be put in the same batch, and the following batch would not continue each of these windows where it left off. The first batch would contain windows 1 to 32 and the second batch would contain windows 33 to 64, so if we consider, say, the first window of each batch (i.e., windows 1 and 33), we can see that they are not consecutive. The simplest solution to this problem is to just use a batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset_for_stateful_rnn(\n",
    "    sequence: tf.Tensor, length: int\n",
    ") -> tf.data.Dataset:\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window: window.batch(length + 1)).batch(1)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
    "\n",
    "\n",
    "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)\n",
    "stateful_valid_set = to_dataset_for_stateful_rnn(\n",
    "    encoded[1_000_000:1_060_000], length\n",
    ")\n",
    "stateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[0, 1, 2]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[1, 2, 3]], dtype=int32)>),\n",
       " (<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[3, 4, 5]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[4, 5, 6]], dtype=int32)>),\n",
       " (<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[6, 7, 8]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[7, 8, 9]], dtype=int32)>)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple example using to_dataset_for_stateful_rnn()\n",
    "list(to_dataset_for_stateful_rnn(tf.range(10), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching is harder, but it is not impossible. e.g., we could chop Shakespeare’s text into 32 texts of equal length, create one dataset of consecutive input sequences for each of them, and finally use `tf.data.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))` to create proper consecutive batches, where the $n^{\\text{th}}$ input sequence in a batch starts off exactly where the $n^{\\text{th}}$ input sequence ended in the previous batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 0,  1,  2],\n",
       "         [10, 11, 12]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 1,  2,  3],\n",
       "         [11, 12, 13]], dtype=int32)>),\n",
       " (<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 3,  4,  5],\n",
       "         [13, 14, 15]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 4,  5,  6],\n",
       "         [14, 15, 16]], dtype=int32)>),\n",
       " (<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 6,  7,  8],\n",
       "         [16, 17, 18]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 7,  8,  9],\n",
       "         [17, 18, 19]], dtype=int32)>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows one way to prepare a batched dataset for a stateful RNN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_non_overlapping_windows(\n",
    "    sequence: np.ndarray, length: int\n",
    ") -> tf.data.Dataset:\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
    "    return ds.flat_map(lambda window: window.batch(length + 1))\n",
    "\n",
    "\n",
    "def to_batched_dataset_for_stateful_rnn(\n",
    "    sequence: tf.Tensor, length: int, batch_size: int = 32\n",
    ") -> tf.data.Dataset:\n",
    "    parts = np.array_split(sequence, batch_size)\n",
    "    datasets = tuple(\n",
    "        to_non_overlapping_windows(part, length) for part in parts\n",
    "    )\n",
    "    ds = tf.data.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
    "\n",
    "\n",
    "list(to_batched_dataset_for_stateful_rnn(tf.range(20), length=3, batch_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set the `stateful` argument to `True` when creating each recurrent layer, and because the stateful RNN needs to know the batch size (since it will preserve a state for each input sequence in the batch). Therefore we must set the `batch_input_shape` argument in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures reproducibility on CPU\n",
    "keras.utils.set_random_seed(42)\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Embedding(\n",
    "            input_dim=n_tokens, output_dim=16, batch_input_shape=[1, None]\n",
    "        ),\n",
    "        keras.layers.GRU(128, return_sequences=True, stateful=True),\n",
    "        keras.layers.Dense(n_tokens, activation='softmax'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of each epoch, we need to reset the states before we go back to the beginning of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch: int, logs: dict[str, Any]) -> None:\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different directory to save the checkpoints\n",
    "model_ckpt = keras.callbacks.ModelCheckpoint(\n",
    "    'my_stateful_shakespeare_model',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: The following cell will take a while to run (possibly an hour if we are not using a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_stateful_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 213s 21ms/step - loss: 1.8690 - accuracy: 0.4494 - val_loss: 1.7632 - val_accuracy: 0.4672\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_stateful_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 211s 21ms/step - loss: 1.5635 - accuracy: 0.5284 - val_loss: 1.6334 - val_accuracy: 0.4994\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_stateful_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 209s 21ms/step - loss: 1.4875 - accuracy: 0.5478 - val_loss: 1.5788 - val_accuracy: 0.5153\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_stateful_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 208s 21ms/step - loss: 1.4483 - accuracy: 0.5579 - val_loss: 1.5471 - val_accuracy: 0.5236\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_stateful_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 213s 21ms/step - loss: 1.4241 - accuracy: 0.5643 - val_loss: 1.5270 - val_accuracy: 0.5286\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_stateful_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 215s 21ms/step - loss: 1.4074 - accuracy: 0.5686 - val_loss: 1.5109 - val_accuracy: 0.5338\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_stateful_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 210s 21ms/step - loss: 1.3953 - accuracy: 0.5714 - val_loss: 1.5008 - val_accuracy: 0.5361\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_stateful_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 212s 21ms/step - loss: 1.3863 - accuracy: 0.5737 - val_loss: 1.4938 - val_accuracy: 0.5381\n",
      "Epoch 9/10\n",
      "9999/9999 [==============================] - 207s 21ms/step - loss: 1.3790 - accuracy: 0.5757 - val_loss: 1.4890 - val_accuracy: 0.5380\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_stateful_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999/9999 [==============================] - 208s 21ms/step - loss: 1.3729 - accuracy: 0.5770 - val_loss: 1.4786 - val_accuracy: 0.5420\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "history = model.fit(\n",
    "    stateful_train_set,\n",
    "    validation_data=stateful_valid_set,\n",
    "    epochs=10,\n",
    "    callbacks=[ResetStatesCallback(), model_ckpt],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: After this model is trained, it will only be possible to use it to make predictions for batches of the same size as were used during training. To avoid this restriction, create an identical stateless model, and copy the stateful model’s weights to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "        keras.layers.GRU(128, return_sequences=True),\n",
    "        keras.layers.Dense(n_tokens, activation='softmax'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the weights, we first need to build the model (so the weights get created):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = keras.Sequential(\n",
    "    [\n",
    "        text_vec_layer,\n",
    "        # No <PAD> or <UNK> tokens\n",
    "        keras.layers.Lambda(lambda X: X - 2),\n",
    "        stateless_model,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be so in the world and the strangeness\n",
      "to see the wo\n"
     ]
    }
   ],
   "source": [
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "print(extend_text('to be or not to be', temperature=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2017 paper [“Learning to Generate Reviews and Discovering Sentiment”](https://homl.info/sentimentneuron) by Alec Radford and other OpenAI researchers describes how the authors trained a big char-RNN-like model on a large dataset, and found that one of the neurons acted as an excellent sentiment analysis classifier: although the model was trained without any labels, the *sentiment neuron* reached state-of-the-art performance on sentiment analysis benchmarks. This foreshadowed and motivated unsupervised pretraining in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "If image classification on the MNIST dataset is the “Hello world!” of computer vision, then sentiment analysis on the IMDb reviews dataset is the “Hello world!” of natural language processing. The IMDb dataset consists of 50,000 movie reviews in English (25,000 for training, 25,000 for testing) extracted from the [Internet Movie Database](https://imdb.com), along with a simple binary target for each review indicating whether it is negative (0) or positive (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /home/ageron/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055c0f544ac349d9a14da8f843651df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2abc244f4844d56919979b33cc2fa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af507eed124c4ff6900538205b1b00fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18cd596aa97b46f1aa3f93d0c29edd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c623038199e46909b7a8b0a39cecbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/ageron/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete0WPKUH/imdb_reviews-train.t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c0a46cc37b4eb6b9feb67d715d7022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abb656c416049c085e0f2f761d5bf9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/ageron/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete0WPKUH/imdb_reviews-test.tf…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb7ceb384634b8ebd766e55ba21c5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad80e1205d5e4914840999fcd3ae3b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/ageron/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete0WPKUH/imdb_reviews-unsuper…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /home/ageron/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
    "    name='imdb_reviews',\n",
    "    split=['train[:90%]', 'train[90%:]', 'test'],\n",
    "    as_supervised=True,\n",
    ")\n",
    "keras.utils.set_random_seed(42)\n",
    "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
    "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
    "test_set = raw_test_set.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Keras also has `keras.datasets.imdb.load_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
      "Label: 0\n",
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
      "Label: 0\n",
      "Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Moun ...\n",
      "Label: 0\n",
      "This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful perf ...\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "for review, label in raw_train_set.take(4):\n",
    "    print(review.numpy().decode('utf-8')[:200], '...')\n",
    "    print('Label:', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will chop it into words instead of characters. Note that it will not work well in some languages. e.g. Chinese writing does not use spaces between words, Vietnamese uses spaces even within words, and German often attaches multiple words together, without spaces. Even in English, spaces are not always the best way to tokenize text: think of “San Francisco” or “#ILoveDeepLearning”.\n",
    "\n",
    "In a 2016 paper [“Neural Machine Translation of Rare Words with Subword Units”](https://homl.info/rarewords) Rico Sennrich et al. from the University of Edinburgh explored several methods to tokenize and detokenize text at the subword level. This way, even if our model encounters a rare word it has never seen before, it can still reasonably guess what it means. e.g. if the model never saw the word “smartest” during training, if it learned the word “smart” and it also learned that the suffix “est” means “the most”, it can infer the meaning of “smartest”. One of the techniques the authors evaluated is *byte pair encoding* (BPE). BPE works by splitting the whole training set into individual characters (including spaces), then repeatedly merging the most frequent adjacent pairs until the vocabulary reaches the desired size.\n",
    "\n",
    "A 2018 paper [“Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates”](https://homl.info/subword) by Taku Kudo at Google further improved subword tokenization, often removing the need for language-specific preprocessing prior to tokenization. Moreover, the paper proposed a novel regularization technique called *subword regularization*, which improves accuracy and robustness by introducing some randomness in tokenization during training: e.g. “New England” may be tokenized as “New” + “England”, or “New” + “Eng” + “land”, or simply “New England” (just one token). Google’s [*SentencePiece*](https://github.com/google/sentencepiece) project provides an open source implementation, which is described in a 2018 paper [“SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing”](https://homl.info/sentencepiece) by Taku Kudo and John Richardson.\n",
    "\n",
    "The [TensorFlow Text](https://homl.info/tftext) library also implements various tokenization strategies, including WordPiece (a variant of BPE by Yonghui Wu et al., [“Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation”](https://homl.info/wordpiece) in 2016), and last but not least, the [Tokenizers library by Hugging Face](https://homl.info/tokenizers) implements a wide range of extremely fast tokenizers.\n",
    "\n",
    "However, for the IMDb task in English, using spaces for token boundaries should \n",
    "be good enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "text_vec_layer = keras.layers.TextVectorization(max_tokens=vocab_size)\n",
    "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: The following cell will take a few minutes to run and the model will probably not learn anything because we didn’t mask the padding tokens (that’s the point of the next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "704/704 [==============================] - 255s 359ms/step - loss: 0.6934 - accuracy: 0.4990 - val_loss: 0.6931 - val_accuracy: 0.5016\n",
      "Epoch 2/2\n",
      "704/704 [==============================] - 250s 355ms/step - loss: 0.6934 - accuracy: 0.5042 - val_loss: 0.6942 - val_accuracy: 0.5008\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "keras.utils.set_random_seed(42)\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        text_vec_layer,\n",
    "        keras.layers.Embedding(vocab_size, embed_size),\n",
    "        keras.layers.GRU(128),\n",
    "        keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "Making the model ignore padding tokens is trivial using Keras: simply add `mask_zero=True` when creating the `Embedding` layer. This means that padding tokens (whose ID is 0) will be ignored by all downstream layers.\n",
    "\n",
    "**Warning**: The following cell will take a while to run (possibly 30 minutes if we are not using a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "704/704 [==============================] - 303s 426ms/step - loss: 0.5296 - accuracy: 0.7234 - val_loss: 0.4045 - val_accuracy: 0.8244\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 295s 419ms/step - loss: 0.3702 - accuracy: 0.8418 - val_loss: 0.3390 - val_accuracy: 0.8532\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 298s 423ms/step - loss: 0.3057 - accuracy: 0.8747 - val_loss: 0.3196 - val_accuracy: 0.8696\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 294s 418ms/step - loss: 0.2784 - accuracy: 0.8871 - val_loss: 0.3162 - val_accuracy: 0.8596\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 293s 417ms/step - loss: 0.2597 - accuracy: 0.8961 - val_loss: 0.3209 - val_accuracy: 0.8548\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "keras.utils.set_random_seed(42)\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        text_vec_layer,\n",
    "        keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
    "        keras.layers.GRU(128),\n",
    "        keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way this works is that the `Embedding` layer creates a mask tensor equal to `tf.math.not_equal(inputs, 0)`: it is a Boolean tensor with the same shape as the inputs, and it is equal to `False` anywhere the token IDs are 0, or `True` otherwise. This mask tensor is then automatically propagated by the model to the next layer. If that layer’s `call()` method has a `mask` argument, then it automatically receives the mask.\n",
    "\n",
    "This allows the layer to ignore the appropriate time steps. Each layer may handle the mask differently, but in general they simply ignore masked time steps. e.g. when a recurrent layer encounters a masked time step, it simply copies the output from the previous time step.\n",
    "\n",
    "Next, if the layer’s `supports_masking` attribute is `True`, then the mask is automatically propagated to the next layer. It keeps propagating this way for as long as the layers have `supports_masking=True`. e.g. a recurrent layer’s `supports_masking` attribute is `True` when `return_sequences=True`, but it’s `False` when `return_sequences=False`.\n",
    "\n",
    "**Tip**: Some layers need to update the mask before propagating it to the next layer: they do so by implementing the `compute_mask()` method, which takes two arguments: the inputs and the previous mask. It then computes the updated mask and returns it. The default implementation of `compute_mask()` just returns the previous mask unchanged.\n",
    "\n",
    "If the mask propagates all the way to the output, then it gets applied to the losses as well, so the masked time steps will not contribute to the loss (their loss will be 0)\n",
    "\n",
    "**Warning**: The LSTM and GRU layers have an optimized implementation for GPUs, based on Nvidia’s cuDNN library. However, this implementation only supports masking if all the padding tokens are at the end of the sequences. It also requires us to use the default values for several hyperparameters: `activation`, `recurrent_activation`, `recurrent_dropout`, `unroll`, `use_bias`, and `reset_after`. If that’s not the case, then these layers will fall back to the (much slower) default GPU implementation.\n",
    "\n",
    "If our model does not start with an `Embedding` layer, we may use the `keras.layers.Masking` layer instead: by default, it sets the mask to `tf.math.reduce_any(tf.math.not_equal(X, 0), axis=-1)`, meaning that time steps where the last dimension is full of zeros will be masked out in subsequent layers.\n",
    "\n",
    "Convolutional layers (including `Conv1D`) do not support masking since it’s not obvious how they would do so anyway. Therefore we will need to explicitly compute the mask and pass it to the appropriate layers:\n",
    "\n",
    "Following model is equivalent to the previous model, built using the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures reproducibility on the CPU\n",
    "keras.utils.set_random_seed(42)\n",
    "inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "token_ids = text_vec_layer(inputs)\n",
    "mask = tf.math.not_equal(token_ids, 0)\n",
    "Z = keras.layers.Embedding(vocab_size, embed_size)(token_ids)\n",
    "Z = keras.layers.GRU(128, dropout=0.2)(Z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(Z)\n",
    "model = keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: The following cell will take a while to run (possibly 30 minutes if we are not using a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "704/704 [==============================] - 303s 427ms/step - loss: 0.5447 - accuracy: 0.7198 - val_loss: 0.4604 - val_accuracy: 0.7720\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 301s 427ms/step - loss: 0.3469 - accuracy: 0.8512 - val_loss: 0.3214 - val_accuracy: 0.8608\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 295s 419ms/step - loss: 0.3054 - accuracy: 0.8713 - val_loss: 0.3069 - val_accuracy: 0.8672\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 295s 420ms/step - loss: 0.2798 - accuracy: 0.8828 - val_loss: 0.3028 - val_accuracy: 0.8672\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 298s 423ms/step - loss: 0.2622 - accuracy: 0.8920 - val_loss: 0.2953 - val_accuracy: 0.8700\n"
     ]
    }
   ],
   "source": [
    "# Compiles and trains the model, as usual\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last approach to masking is to feed the model with ragged tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[86, 18], [11, 7, 1, 116, 217]]>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_ragged = keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size, ragged=True\n",
    ")\n",
    "text_vec_layer_ragged.adapt(train_set.map(lambda reviews, labels: reviews))\n",
    "text_vec_layer_ragged(['Great movie!', 'This is DiCaprio’s best role.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       "array([[ 86,  18,   0,   0,   0],\n",
       "       [ 11,   7,   1, 116, 217]])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer(['Great movie!', 'This is DiCaprio’s best role.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras’s recurrent layers have built-in support for ragged tensors:\n",
    "\n",
    "**Warning**: The following cell will take a while to run (possibly 30 minutes if we are not using a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "704/704 [==============================] - 280s 395ms/step - loss: 0.5038 - accuracy: 0.7496 - val_loss: 0.6706 - val_accuracy: 0.6752\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 277s 393ms/step - loss: 0.4499 - accuracy: 0.7892 - val_loss: 0.3494 - val_accuracy: 0.8500\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 276s 392ms/step - loss: 0.3270 - accuracy: 0.8592 - val_loss: 0.3855 - val_accuracy: 0.8260\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 277s 394ms/step - loss: 0.2935 - accuracy: 0.8760 - val_loss: 0.3401 - val_accuracy: 0.8520\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 275s 390ms/step - loss: 0.2742 - accuracy: 0.8854 - val_loss: 0.3971 - val_accuracy: 0.8208\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "keras.utils.set_random_seed(42)\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        text_vec_layer_ragged,\n",
    "        keras.layers.Embedding(vocab_size, embed_size),\n",
    "        keras.layers.GRU(128),\n",
    "        keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Embeddings and Language Models\n",
    "Instead of training word embeddings, we could just download and use pretrained embeddings, such as Google’s [Word2vec embeddings](https://homl.info/word2vec), Stanford’s [GloVe embeddings](https://homl.info/glove), or Facebook’s [FastText embeddings](https://fasttext.cc).\n",
    "\n",
    "Using pretrained word embeddings was popular for several years, but this approach has its limits. e.g. a word has a single representation, no matter the context.\n",
    "\n",
    "To address this limitation, a 2018 paper [“Deep Contextualized Word Representations”](https://homl.info/elmo) by Matthew Peters introduced *Embeddings from Language Models* (ELMo): these are contextualized word embeddings learned from the internal states of a deep bidirectional language model. Instead of just using pretrained embeddings in our model, we reuse part of a pretrained language model.\n",
    "\n",
    "At roughly the same time, the *Universal Language Model Fine-Tuning* (ULMFiT) paper [ “Universal Language Model Fine-Tuning for Text Classification”](https://homl.info/ulmfit) by Jeremy Howard and Sebastian Ruder demonstrated the effectiveness of unsupervised pretraining for NLP tasks: the authors trained an LSTM language model on a huge text corpus using self-supervised learning, then they fine-tuned it on various tasks. the authors showed a pretrained model fine-tuned on just 100 labeled examples could achieve the same performance as one trained from scratch on 10,000 examples. This paper marked the beginning of a new era in NLP: today, reusing pretrained language models is the norm.\n",
    "\n",
    "Let’s build a classifier based on the Universal Sentence Encoder, a model architecture introduced in a 2018 paper [“Universal Sentence Encoder”](https://homl.info/139) by a team of Google researchers.\n",
    "\n",
    "**Warning**: the following cell will take a while to run (possibly an hour if we are not using a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "704/704 [==============================] - 224s 303ms/step - loss: 0.3141 - accuracy: 0.8648 - val_loss: 0.2397 - val_accuracy: 0.9008\n",
      "Epoch 2/10\n",
      "704/704 [==============================] - 205s 291ms/step - loss: 0.0489 - accuracy: 0.9852 - val_loss: 0.3257 - val_accuracy: 0.8936\n",
      "Epoch 3/10\n",
      "704/704 [==============================] - 204s 290ms/step - loss: 0.0061 - accuracy: 0.9988 - val_loss: 0.3963 - val_accuracy: 0.8944\n",
      "Epoch 4/10\n",
      "704/704 [==============================] - 204s 290ms/step - loss: 9.4918e-04 - accuracy: 0.9999 - val_loss: 0.4291 - val_accuracy: 0.8924\n",
      "Epoch 5/10\n",
      "704/704 [==============================] - 203s 289ms/step - loss: 5.1920e-04 - accuracy: 1.0000 - val_loss: 0.4691 - val_accuracy: 0.8932\n",
      "Epoch 6/10\n",
      "704/704 [==============================] - 204s 289ms/step - loss: 5.0053e-04 - accuracy: 1.0000 - val_loss: 0.4687 - val_accuracy: 0.8912\n",
      "Epoch 7/10\n",
      "704/704 [==============================] - 208s 296ms/step - loss: 3.7360e-04 - accuracy: 1.0000 - val_loss: 0.5034 - val_accuracy: 0.8984\n",
      "Epoch 8/10\n",
      "704/704 [==============================] - 209s 297ms/step - loss: 2.3907e-05 - accuracy: 1.0000 - val_loss: 0.5773 - val_accuracy: 0.8924\n",
      "Epoch 9/10\n",
      "704/704 [==============================] - 204s 290ms/step - loss: 9.0970e-06 - accuracy: 1.0000 - val_loss: 0.6163 - val_accuracy: 0.8972\n",
      "Epoch 10/10\n",
      "704/704 [==============================] - 205s 291ms/step - loss: 5.2528e-06 - accuracy: 1.0000 - val_loss: 0.6455 - val_accuracy: 0.8956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89897f6d30>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "os.environ['TFHUB_CACHE_DIR'] = 'my_tfhub_cache'\n",
    "# Ensures reproducibility on CPU\n",
    "keras.utils.set_random_seed(42)\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        hub.KerasLayer(\n",
    "            'https://tfhub.dev/google/universal-sentence-encoder/4',\n",
    "            trainable=True,\n",
    "            dtype=tf.string,\n",
    "            input_shape=[],\n",
    "        ),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy']\n",
    ")\n",
    "model.fit(train_set, validation_data=valid_set, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: By default, TensorFlow Hub modules are saved to a temporary directory, and they get downloaded again and again every time we run our program. To avoid that, we must set the `TFHUB_CACHE_DIR` environment variable to a directory of our choice: the modules will then be saved there, and only downloaded once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Encoder–Decoder Network for Neural Machine Translation\n",
    "Let’s begin with a simple *neural machine translation* (NMT) model by Ilya Sutskever et al., [“Sequence to Sequence Learning with Neural Networks”](https://homl.info/103), that will translate English sentences to Spanish.\n",
    "\n",
    "<a id=\"simple-nmt-figure\"></a>\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/16/simple_nmt.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "The architecture is as follows: English sentences are fed as inputs to the encoder, and the decoder outputs the Spanish translations. Note that the Spanish translations are also used as inputs to the decoder during training, but shifted back by one step. In other words, during training the decoder is given as input the word that it *should* have output at the previous step, regardless of what it actually output. This is called *teacher forcing*, a technique that significantly speeds up training and improves the model’s performance. For the very first word, the decoder is given the start-of-sequence (SOS) token, and the decoder is expected to end the sentence with an end-of-sequence (EOS) token.\n",
    "\n",
    "Each word is initially represented by its ID. Next, an `Embedding` layer returns the word embedding. These word embeddings are then fed to the encoder and the decoder.\n",
    "\n",
    "At each step, the decoder outputs a score for each word in the output vocabulary (i.e., Spanish), then the softmax activation function turns these scores into probabilities.\n",
    "\n",
    "**Note**: At inference time, we will not have the target sentence to feed to the decoder. Instead, we need to feed it the word that it has just output at the previous step and this will require an embedding lookup.\n",
    "\n",
    "**Tip**: In a 2015 paper [“Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks”](https://homl.info/scheduledsampling), Samy Bengio et al. proposed gradually switching from feeding the decoder the previous *target* token to feeding it the previous *output* token during training.\n",
    "\n",
    "Let’s download a dataset of English/Spanish sentence pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip'\n",
    "path = keras.utils.get_file(\n",
    "    'spa-eng.zip', origin=url, cache_dir='datasets', extract=True\n",
    ")\n",
    "text = (Path(path).with_name('spa-eng') / 'spa.txt').read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is created by contributors of the [Tatoeba project](https://tatoeba.org). About 120,000 sentence pairs were selected by the authors of the website https://manythings.org/anki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Removing the Spanish characters “¡” and “¿”,\n",
    "text = text.replace('¡', '').replace('¿', '')\n",
    "pairs = [line.split('\\t') for line in text.splitlines()]\n",
    "# Ensures reproducibility on CPU\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(pairs)\n",
    "# Separates the pairs into 2 lists\n",
    "sentences_en, sentences_es = zip(*pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the first three sentence pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How boring! => Qué aburrimiento!\n",
      "I love sports. => Adoro el deporte.\n",
      "Would you like to swap jobs? => Te gustaría que intercambiemos los trabajos?\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(sentences_en[i], '=>', sentences_es[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "text_vec_layer_en = keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length\n",
    ")\n",
    "text_vec_layer_es = keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length\n",
    ")\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f'startofseq {s} endofseq' for s in sentences_es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We limit the vocabulary size to 1,000, which is quite small. That’s because the training set is not very large, and because using a small value will speed up training. State-of-the-art translation models typically use a much larger vocabulary (e.g., 30,000), a much larger training set (gigabytes), and a much larger model (hundreds or even thousands of megabytes).\n",
    "- For the Spanish text, we add “startofseq” and “endofseq” to each sentence when adapting the `TextVectorization` layer: we will use these words as SOS and EOS tokens. We could use any other words, as long as they are not actual Spanish words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_en.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_es.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "X_train_dec = tf.constant([f'startofseq {s}' for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f'startofseq {s}' for s in sentences_es[100_000:]])\n",
    "Y_train = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[100_000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures reproducibility on CPU\n",
    "keras.utils.set_random_seed(42)\n",
    "encoder_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = keras.layers.Embedding(\n",
    "    vocab_size, embed_size, mask_zero=True\n",
    ")\n",
    "decoder_embedding_layer = keras.layers.Embedding(\n",
    "    vocab_size, embed_size, mask_zero=True\n",
    ")\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: When the languages share many words, you may get better performance using the same embedding layer for both the encoder and the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = keras.layers.Dense(vocab_size, activation='softmax')\n",
    "Y_proba = output_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid;\">\n",
    "\n",
    "### Optimizing the Output Layer\n",
    "If the target vocabulary contained, say, 50,000 Spanish words instead of 1,000, then the decoder would output 50,000-dimensional vectors, and computing the softmax function over such a large vector would be very computationally intensive. To avoid this, one solution is to look only at the logits output by the model for the correct word and for a random sample of incorrect words, then compute an approximation of the loss based only on these logits. This *sampled softmax* technique was introduced in a 2015 paper [“On Using Very Large Target Vocabulary for Neural Machine Translation”](https://homl.info/104) by Sébastien Jean et al. In TensorFlow we can use the `tf.nn.sampled_softmax_loss()` function for this during training and use the normal softmax function at inference time (sampled softmax cannot be used at inference time because it requires knowing the target).\n",
    "\n",
    "An extra thing we can do to speed up training is to tie the weights of the output layer to the transpose of the decoder’s embedding matrix (See Chapter 17). This significantly reduces the number of model parameters, which speeds up training and may sometimes improve the model’s accuracy as well, especially if we don’t have a lot of training data. The embedding matrix is equivalent to one-hot encoding followed by a linear layer with no bias term and no activation function that maps the one-hot vectors to the embedding space. The output layer does the reverse. So, if the model can find an embedding matrix whose transpose is close to its inverse (such a matrix is called an *orthogonal matrix*), then there’s no need to learn a separate set of weights for the output layer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: The following cell will take a while to run (possibly a couple hours if we are not using a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 698s 221ms/step - loss: 0.4154 - accuracy: 0.4256 - val_loss: 0.3069 - val_accuracy: 0.5246\n",
      "Epoch 2/10\n",
      "3125/3125 [==============================] - 686s 219ms/step - loss: 0.2631 - accuracy: 0.5745 - val_loss: 0.2367 - val_accuracy: 0.6055\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - 686s 220ms/step - loss: 0.2066 - accuracy: 0.6457 - val_loss: 0.2061 - val_accuracy: 0.6500\n",
      "Epoch 4/10\n",
      "3125/3125 [==============================] - 682s 218ms/step - loss: 0.1740 - accuracy: 0.6907 - val_loss: 0.1920 - val_accuracy: 0.6691\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - 676s 216ms/step - loss: 0.1507 - accuracy: 0.7237 - val_loss: 0.1865 - val_accuracy: 0.6767\n",
      "Epoch 6/10\n",
      "3125/3125 [==============================] - 675s 216ms/step - loss: 0.1316 - accuracy: 0.7522 - val_loss: 0.1847 - val_accuracy: 0.6804\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - 675s 216ms/step - loss: 0.1154 - accuracy: 0.7774 - val_loss: 0.1866 - val_accuracy: 0.6822\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 673s 215ms/step - loss: 0.1011 - accuracy: 0.8007 - val_loss: 0.1907 - val_accuracy: 0.6829\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 673s 215ms/step - loss: 0.0888 - accuracy: 0.8215 - val_loss: 0.1961 - val_accuracy: 0.6792\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 673s 215ms/step - loss: 0.0782 - accuracy: 0.8402 - val_loss: 0.2027 - val_accuracy: 0.6763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f897878ac10>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    (X_train, X_train_dec),\n",
    "    Y_train,\n",
    "    epochs=10,\n",
    "    validation_data=((X_valid, X_valid_dec), Y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the decoder expects as input the word that was predicted at the previous time step. One way to do this is to write a custom memory cell that keeps track of the previous output and feeds it to the decoder at the next time step. However, to keep things simple, we can just call the model multiple times, predicting one extra word at each round:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence_en: str) -> str:\n",
    "    translation = ''\n",
    "    for word_idx in range(max_length):\n",
    "        # Encoder input\n",
    "        X = np.array([sentence_en])\n",
    "        # Decoder input\n",
    "        X_dec = np.array(['startofseq ' + translation])\n",
    "        # Last token’s probas\n",
    "        y_proba = model.predict((X, X_dec))[0, word_idx]\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == 'endofseq':\n",
    "            break\n",
    "        translation += ' ' + predicted_word\n",
    "    return translation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me gusta el fútbol'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('I like soccer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! However, the model struggles with longer sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me gusta el fútbol y a veces mismo al bus'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('I like soccer and also going to the beach')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional \n",
    "A regular recurrent layer is *causal*, meaning it cannot look into the future. This type of RNN makes sense when forecasting time series, or in the decoder of a sequence-to-sequence (seq2seq) model. But for tasks like text classification, or in the encoder of a seq2seq model, it is often preferable to look ahead at the next words before encoding a given word. Consider the phrases “the right arm”, “the right person”, and “the right to criticize”. One solution is to run two recurrent layers on the same inputs, one reading the words from left to right and the other reading them from right to left, then combine their outputs at each time step, typically by concatenating them. This is what a *bidirectional recurrent layer* does.\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/16/bidirectional_recurrent_layer.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "To create a bidirectional recurrent layer, just wrap a regular recurrent layer in a `Bidirectional` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures reproducibility on CPU\n",
    "keras.utils.set_random_seed(42)\n",
    "encoder = keras.layers.Bidirectional(keras.layers.LSTM(256, return_state=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [\n",
    "    # Short-term (0 & 2)\n",
    "    tf.concat(encoder_state[::2], axis=-1),\n",
    "    # Long-term (1 & 3)\n",
    "    tf.concat(encoder_state[1::2], axis=-1),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: The following cell will take a while to run (possibly a couple hours if we are not using a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 574s 181ms/step - loss: 0.3075 - accuracy: 0.5393 - val_loss: 0.2192 - val_accuracy: 0.6319\n",
      "Epoch 2/10\n",
      "3125/3125 [==============================] - 564s 180ms/step - loss: 0.1916 - accuracy: 0.6689 - val_loss: 0.1880 - val_accuracy: 0.6731\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - 566s 181ms/step - loss: 0.1602 - accuracy: 0.7119 - val_loss: 0.1751 - val_accuracy: 0.6916\n",
      "Epoch 4/10\n",
      "3125/3125 [==============================] - 566s 181ms/step - loss: 0.1395 - accuracy: 0.7415 - val_loss: 0.1715 - val_accuracy: 0.6979\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - 566s 181ms/step - loss: 0.1227 - accuracy: 0.7666 - val_loss: 0.1707 - val_accuracy: 0.7025\n",
      "Epoch 6/10\n",
      "3125/3125 [==============================] - 567s 181ms/step - loss: 0.1085 - accuracy: 0.7887 - val_loss: 0.1730 - val_accuracy: 0.6995\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - 571s 183ms/step - loss: 0.0961 - accuracy: 0.8089 - val_loss: 0.1764 - val_accuracy: 0.7000\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 567s 181ms/step - loss: 0.0852 - accuracy: 0.8273 - val_loss: 0.1821 - val_accuracy: 0.6981\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 565s 181ms/step - loss: 0.0759 - accuracy: 0.8438 - val_loss: 0.1881 - val_accuracy: 0.6956\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 565s 181ms/step - loss: 0.0682 - accuracy: 0.8577 - val_loss: 0.1951 - val_accuracy: 0.6906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f892d2d5fa0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Completes the model and trains it\n",
    "decoder = keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "output_layer = keras.layers.Dense(vocab_size, activation='softmax')\n",
    "Y_proba = output_layer(decoder_outputs)\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    (X_train, X_train_dec),\n",
    "    Y_train,\n",
    "    epochs=10,\n",
    "    validation_data=((X_valid, X_valid_dec), Y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me gusta el fútbol'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('I like soccer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "We can give the model a chance to go back and fix mistakes it made earlier. *Beam search* keeps track of a short list of the *k* most promising sentences (say, the top three), and at each decoder step it tries to extend them by one word, keeping only the *k* most likely sentences. The parameter *k* is called the *beam width*.\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/16/beam_search.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "**Tip**: The TensorFlow Addons library includes a full seq2seq API that lets us build encoder–decoder models with attention, including beam search, and more. However, its documentation is currently very limited.\n",
    "\n",
    "Here is a very basic implementation of beam search. It is readable and understandable, but it’s definitely not optimized for speed! The function first uses the model to find the top *k* words to start the translations (where *k* is the beam width). For each of the top *k* translations, it evaluates the conditional probabilities of top *k* words it could add to that translation. These extended translations and their probabilities are added to the list of candidates. Once we’ve gone through all top *k* translations and all top *k* words that could complete them, we keep only the top *k* candidates with the highest probability, and we iterate over and over until they all finish with an EOS token. The top translation is then returned (after removing its EOS token).\n",
    "\n",
    "**Note**: If $p(S)$ is the probability of sentence $S$, and $p(W|S)$ is the conditional probability of the word $W$ given that the translation starts with $S$, then the probability of the sentence $S^\\prime=\\text{concat}(S,W)$ is $p(S^\\prime)=p(S)\\times p(W|S)$. As we add more words, the probability gets smaller and smaller. To avoid the risk of it getting too small, which could cause floating point precision errors, the function keeps track of log probabilities instead of probabilities: recall that $\\log(ab)=log(a)+log(b)$, therefore $\\log(p(S^\\prime))=\\log(p(S))+\\log(p(W|S))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A basic implementation of beam search\n",
    "\n",
    "\n",
    "def beam_search(\n",
    "    sentence_en: str, beam_width: int, verbose: bool = False\n",
    ") -> str:\n",
    "    # Encoder input\n",
    "    X = np.array([sentence_en])\n",
    "    # Decoder input\n",
    "    X_dec = np.array(['startofseq'])\n",
    "    # First token’s probas\n",
    "    y_proba = model.predict((X, X_dec))[0, 0]\n",
    "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
    "    # List of best (log_proba, translation)\n",
    "    top_translations = [\n",
    "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
    "        for word_id, word_proba in zip(top_k.indices, top_k.values)\n",
    "    ]\n",
    "\n",
    "    # Displays the top first words in verbose mode\n",
    "    if verbose:\n",
    "        print('Top first words:', top_translations)\n",
    "\n",
    "    for idx in range(1, max_length):\n",
    "        candidates = []\n",
    "        for log_proba, translation in top_translations:\n",
    "            if translation.endswith('endofseq'):\n",
    "                candidates.append((log_proba, translation))\n",
    "                # Translation is finished, so don’t try to extend it\n",
    "                continue\n",
    "            # Encoder input\n",
    "            tf.math.top_k\n",
    "            X = np.array([sentence_en])\n",
    "            # Decoder input\n",
    "            X_dec = np.array(['startofseq ' + translation])\n",
    "            # Last token’s proba\n",
    "            y_proba = model.predict((X, X_dec))[0, idx]\n",
    "            top_k = tf.math.top_k(y_proba, k=beam_width)\n",
    "            for word_id, word_proba in zip(top_k.indices, top_k.values):\n",
    "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
    "                candidates.append(\n",
    "                    (log_proba + np.log(word_proba), f'{translation} {word}')\n",
    "                )\n",
    "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
    "\n",
    "        # Displays the top translation so far in verbose mode\n",
    "        if verbose:\n",
    "            print('Top translations so far:', top_translations)\n",
    "\n",
    "        if all([tr.endswith('endofseq') for _, tr in top_translations]):\n",
    "            return top_translations[0][1].replace('endofseq', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me [UNK] los gatos y los gatos'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows how the model making an error\n",
    "sentence_en = 'I love cats and dogs'\n",
    "translate(sentence_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top first words: [(-0.012974381, 'me'), (-4.592527, '[UNK]'), (-6.314033, 'yo')]\n",
      "Top translations so far: [(-0.4831518, 'me [UNK]'), (-1.4920667, 'me encanta'), (-1.986235, 'me gustan')]\n",
      "Top translations so far: [(-0.6793061, 'me [UNK] los'), (-1.9889652, 'me gustan los'), (-2.0470557, 'me encanta los')]\n",
      "Top translations so far: [(-0.7609749, 'me [UNK] los gatos'), (-2.0677316, 'me gustan los gatos'), (-2.26029, 'me encanta los gatos')]\n",
      "Top translations so far: [(-0.76985043, 'me [UNK] los gatos y'), (-2.0701222, 'me gustan los gatos y'), (-2.2649746, 'me encanta los gatos y')]\n",
      "Top translations so far: [(-0.81283045, 'me [UNK] los gatos y los'), (-2.118244, 'me gustan los gatos y los'), (-2.96167, 'me encanta los gatos y los')]\n",
      "Top translations so far: [(-1.2259341, 'me [UNK] los gatos y los gatos'), (-1.9556838, 'me [UNK] los gatos y los perros'), (-2.7524388, 'me gustan los gatos y los perros')]\n",
      "Top translations so far: [(-1.2261332, 'me [UNK] los gatos y los gatos endofseq'), (-1.9560521, 'me [UNK] los gatos y los perros endofseq'), (-2.7566314, 'me gustan los gatos y los perros endofseq')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'me [UNK] los gatos y los gatos'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows how beam search can help\n",
    "beam_search(sentence_en, beam_width=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct translation is in the top 3 sentences found by beam search, but it’s not the first. Since we’re using a small vocabulary, the [UNK] token is quite frequent, so we may want to penalize it (e.g., divide its probability by 2 in the beam search function): this will discourage beam search from using it too much.\n",
    "\n",
    "**Note**: The most common metric used in NMT is the *bilingual evaluation understudy* (BLEU) score, which compares each translation produced by the model with several good translations produced by humans: it counts the number of *n*-grams that appear in any of the target translations and adjusts the score to take into account the frequency of the produced *n*-grams in the target translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanisms\n",
    "<style>ul {list-style-type: none;}</style>\n",
    "\n",
    "*Bahdanau attention*\n",
    "- Consider the path from the word “soccer” to its translation “fútbol” in [previous network](#simple-nmt-figure). It is quite long! This means that a representation of this word (and other words) needs to be carried over many steps before it is actually used. To make this path shorter, Dzmitry Bahdanau et al. in a landmark 2014 paper [“Neural Machine Translation by Jointly Learning to Align and Translate”](https://homl.info/attention), introduced a technique that allowed the decoder to focus on the appropriate words (as encoded by the encoder) at each time step. e.g, at the time step where the decoder needs to output the word “fútbol”, it will focus its attention on the word “soccer”.\n",
    "  \n",
    "  Here is our model with an added attention mechanism:\n",
    "  \n",
    "  <center>\n",
    "    <img \n",
    "      src=\"../images/16/simple_nmt_with_attention.png\" \n",
    "      onerror=\"\n",
    "        this.onerror = null;\n",
    "        const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "        this.src = repo + this.src.split('..')[1];\n",
    "      \"\n",
    "    >\n",
    "  </center>\n",
    "    \n",
    "  We now also send all of the encoder’s outputs to the decoder. Since the decoder cannot deal with all these encoder outputs at once, they need to be aggregated: at each time step, the decoder’s memory cell computes a weighted sum of all the encoder outputs to determine which words to focus on. The weight $\\alpha_{(t,i)}$ is the weight of the $i^{\\text{th}}$ encoder output at the $t^{\\text{th}}$ decoder time step. Then it uses this weighted sum to compute the decoder’s current hidden state. The rest of the decoder works just like earlier.\n",
    "\n",
    "  These $\\alpha_{(t,i)}$ weights are generated by a small neural network called an *alignment model* (or an *attention layer*), which is trained jointly with the rest of the encoder–decoder model. It starts with a `Dense` layer composed of a single neuron to process each of the encoder’s outputs, along with the decoder’s previous hidden. This layer outputs a score (or energy) for each encoder output (e.g., $e_{(3,2)}$): this score measures how well each output is aligned with the decoder’s previous hidden state. e.g. in the figure above, the model has already output “me gusta el” (meaning “I like”), so it’s now expecting a noun: the word “soccer” is the one that best aligns with the current state, so it gets a high score. Finally, all the scores go through a softmax layer to get a final weight for each encoder output (e.g., $\\alpha_{(3,2)}$). Since it concatenates the encoder output with the decoder’s previous hidden state, it is sometimes called *concatenative attention* (or *additive attention*).\n",
    "  \n",
    "  **Note**: If the input sentence is $n$ words long, and assuming the output sentence is about as long, then this model will need to compute about $n^2$ weights. Fortunately, this quadratic computational complexity is still tractable because even long sentences don’t have thousands of words.\n",
    "\n",
    "*Luong attention*\n",
    "- Or *multiplicative attention*, was proposed shortly after, in a 2015 paper [“Effective Approaches to Attention-Based Neural Machine Translation”](https://homl.info/luongattention), by Minh-Thang Luong et al. Because the goal of the alignment model is to measure the similarity between one of the encoder’s outputs and the decoder’s previous hidden state, the authors proposed to simply compute the dot product of these two vectors, as this is often a fairly good similarity measure, and modern hardware can compute it very efficiently. For this, both vectors must have the same dimensionality. The dot product gives a score, and all the scores (at a given decoder time step) go through a softmax layer to give the final weights.\n",
    "\n",
    "  They also proposed to use the decoder’s hidden state at the current time step rather than at the previous time step (i.e., $\\mathbf{h}_{(t)}$ rather than $\\mathbf{h}_{(t-1)}$), then to use the output of the attention mechanism (noted $\\widetilde{\\mathbf{h}}_{(t)}$) directly to compute the decoder’s predictions, rather than using it to compute the decoder’s current hidden state.\n",
    "\n",
    "  They also proposed a variant of the dot product mechanism where the encoder outputs first go through a fully connected layer (without a bias term) before the dot products are computed. This is called the “general” dot product approach. The researchers compared both dot product approaches with the concatenative attention mechanism (adding a rescaling parameter vector $v$), and they observed that the dot product variants performed better than concatenative attention.\n",
    "\n",
    "**Equation 16-1** Attention mechanisms\n",
    "$$\n",
    "\\begin{split}\n",
    "&\\widetilde{\\mathbf{h}}_{(t)}=\\sum_i\\alpha_{(t,i)}\\mathbf{y}_{(i)}\n",
    "\\\\&\\text{with}\\;\\alpha_{(t,i)}\n",
    "=\\frac{\\exp(e_{(t,i)})}{\\sum_{i^\\prime}\\exp({e_{(t,i^\\prime)}})}\n",
    "\\\\&\\text{and}\\;e_{(t,i)}=\\begin{cases}\n",
    "{\\mathbf{h}_{(t)}}^\\top\\mathbf{y}_{(i)}&dot\n",
    "\\\\{\\mathbf{h}_{(t)}}^\\top\\mathbf{W}\\mathbf{y}_{(i)}&general\n",
    "\\\\\\mathbf{v}^\\top\\tanh(\\mathbf{W}[\\mathbf{h}_{(t)};\\mathbf{y}_{(i)}])&concat\n",
    "\\end{cases}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Keras provides a `keras.layers.Attention` layer for Luong attention, and an `AdditiveAttention` layer for Bahdanau attention.\n",
    "\n",
    "Let’s add it to our model. We need to feed all the encoder’s outputs to the `Attention` layer, so we must add `return_sequences=True` to the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures reproducibility on CPU\n",
    "keras.utils.set_random_seed(42)\n",
    "encoder = keras.layers.Bidirectional(\n",
    "    keras.layers.LSTM(256, return_sequences=True, return_state=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part of the model is exactly the same as earlier\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [\n",
    "    # Short-term (0 & 2)\n",
    "    tf.concat(encoder_state[::2], axis=-1),\n",
    "    # Long-term (1 & 3)\n",
    "    tf.concat(encoder_state[1::2], axis=-1),\n",
    "]\n",
    "decoder = keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create the attention layer and pass it the decoder’s states and the encoder’s outputs. However, to access the decoder’s states at each step we would need to write a custom memory cell. For simplicity, let’s use the decoder’s outputs instead of its states: in practice this works well too, and it’s much easier to code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = keras.layers.Attention()\n",
    "keras.layers.AdditiveAttention\n",
    "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
    "output_layer = keras.layers.Dense(vocab_size, activation='softmax')\n",
    "Y_proba = output_layer(attention_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: The following cell will take a while to run (possibly a couple hours if we are not using a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 597s 189ms/step - loss: 0.3074 - accuracy: 0.5469 - val_loss: 0.2106 - val_accuracy: 0.6487\n",
      "Epoch 2/10\n",
      "3125/3125 [==============================] - 585s 187ms/step - loss: 0.1902 - accuracy: 0.6789 - val_loss: 0.1865 - val_accuracy: 0.6830\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - 585s 187ms/step - loss: 0.1659 - accuracy: 0.7123 - val_loss: 0.1759 - val_accuracy: 0.7005\n",
      "Epoch 4/10\n",
      "3125/3125 [==============================] - 584s 187ms/step - loss: 0.1493 - accuracy: 0.7359 - val_loss: 0.1728 - val_accuracy: 0.7060\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - 582s 186ms/step - loss: 0.1358 - accuracy: 0.7548 - val_loss: 0.1724 - val_accuracy: 0.7084\n",
      "Epoch 6/10\n",
      "3125/3125 [==============================] - 583s 186ms/step - loss: 0.1245 - accuracy: 0.7712 - val_loss: 0.1738 - val_accuracy: 0.7103\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - 582s 186ms/step - loss: 0.1148 - accuracy: 0.7863 - val_loss: 0.1770 - val_accuracy: 0.7111\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 582s 186ms/step - loss: 0.1064 - accuracy: 0.7992 - val_loss: 0.1806 - val_accuracy: 0.7110\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 582s 186ms/step - loss: 0.0991 - accuracy: 0.8101 - val_loss: 0.1862 - val_accuracy: 0.7088\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 581s 186ms/step - loss: 0.0929 - accuracy: 0.8205 - val_loss: 0.1903 - val_accuracy: 0.7077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f87e5c8ad90>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    (X_train, X_train_dec),\n",
    "    Y_train,\n",
    "    epochs=10,\n",
    "    validation_data=((X_valid, X_valid_dec), Y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me gusta el fútbol y también ir a la playa'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('I like soccer and also going to the beach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top first words: [(-0.26210824, 'me'), (-2.553061, 'prefiero'), (-3.2005944, 'yo')]\n",
      "Top translations so far: [(-0.32478744, 'me gusta'), (-3.0608056, 'prefiero el'), (-3.1685317, 'me gustan')]\n",
      "Top translations so far: [(-0.7464272, 'me gusta el'), (-2.4712462, 'me gusta fútbol'), (-2.9149299, 'me gusta al')]\n",
      "Top translations so far: [(-1.0369574, 'me gusta el fútbol'), (-2.3301778, 'me gusta el el'), (-2.9658434, 'me gusta fútbol y')]\n",
      "Top translations so far: [(-1.0404125, 'me gusta el fútbol y'), (-2.5983238, 'me gusta el el fútbol'), (-2.9736564, 'me gusta fútbol y también')]\n",
      "Top translations so far: [(-1.0520902, 'me gusta el fútbol y también'), (-2.6003318, 'me gusta el el fútbol y'), (-3.128903, 'me gusta fútbol y también me')]\n",
      "Top translations so far: [(-1.9568634, 'me gusta el fútbol y también ir'), (-2.6169589, 'me gusta el el fútbol y también'), (-2.6949644, 'me gusta el fútbol y también fuera')]\n",
      "Top translations so far: [(-1.9676423, 'me gusta el fútbol y también ir a'), (-2.8482866, 'me gusta el fútbol y también fuera a'), (-3.7197533, 'me gusta el el fútbol y también ir')]\n",
      "Top translations so far: [(-1.9692448, 'me gusta el fútbol y también ir a la'), (-2.8501132, 'me gusta el fútbol y también fuera a la'), (-3.7309551, 'me gusta el el fútbol y también ir a')]\n",
      "Top translations so far: [(-1.9733216, 'me gusta el fútbol y también ir a la playa'), (-2.851697, 'me gusta el fútbol y también fuera a la playa'), (-3.7333717, 'me gusta el el fútbol y también ir a la')]\n",
      "Top translations so far: [(-1.9737166, 'me gusta el fútbol y también ir a la playa endofseq'), (-2.8547554, 'me gusta el fútbol y también fuera a la playa endofseq'), (-3.737218, 'me gusta el el fútbol y también ir a la playa')]\n",
      "Top translations so far: [(-1.9737166, 'me gusta el fútbol y también ir a la playa endofseq'), (-2.8547554, 'me gusta el fútbol y también fuera a la playa endofseq'), (-3.7375438, 'me gusta el el fútbol y también ir a la playa endofseq')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'me gusta el fútbol y también ir a la playa'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search(\n",
    "    'I like soccer and also going to the beach', beam_width=3, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s another way to think of this layer: it acts as a differentiable memory retrieval mechanism.\n",
    "\n",
    "e.g. let’s suppose the encoder analyzed the input sentence “I like soccer”, and it managed to understand that the word “I” is the subject and the word “like” is the verb, so it encoded this information in its outputs for these words. Now suppose the decoder has already translated the subject, and it thinks that it should translate the verb next. For this, it needs to fetch the verb from the input sentence. This is analogous to a dictionary lookup: it’s as if the encoder had created a dictionary {\"subject”: “They”, “verb”: “played”, ...} and the decoder wanted to look up the value that corresponds to the key “verb”.However, the model does not have discrete tokens to represent the keys (like “subject” or “verb”); instead, it has vectorized representations of these concepts that it learned during training, so the query it will use for the lookup will not perfectly match any key in the dictionary. The solution is to compute a similarity measure between the query and each key in the dictionary, and then use the softmax function to convert these similarity scores to weights that add up to 1. As we saw earlier, that’s exactly what the attention layer does. If the key that represents the verb is by far the most similar to the query, then that key’s weight will be close to 1. Next, the attention layer computes a weighted sum of the corresponding values: if the weight of the “verb” key is close to 1, then the weighted sum will be very close to the representation of the word “played”.\n",
    "\n",
    "This is why the Keras `Attention` and `AdditiveAttention` layers both expect a list as input, containing two or three items: the *queries*, the *keys*, and optionally the *values*. If we do not pass any values, then they are automatically equal to the *keys*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Is All You Need: The Original Transformer Architecture\n",
    "In a groundbreaking 2017 paper, a team of Google researchers suggested that [“Attention Is All You Need”](https://homl.info/transformer). They created an architecture called the *transformer*, which significantly improved the state-of-the-art in NMT. Because the model is not recurrent:\n",
    "- It doesn’t suffer as much from the vanishing or exploding gradients problems as RNNs.\n",
    "- It can be trained in fewer steps.\n",
    "- It’s easier to parallelize across multiple GPUs.\n",
    "- It can better capture long-range patterns than RNNs.\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/16/transformer.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "The left part of is the encoder, and the right part is the decoder. Each embedding layer outputs a 3D tensor of shape [*batch size*, *sequence length*, *embedding size*]. After that, the tensors are gradually transformed as they flow through the transformer, but their shape remains the same.\n",
    "\n",
    "The encoder’s role is to gradually transform the inputs (word representations of the English sentence) until each word’s representation perfectly captures the meaning of the word, in the context of the sentence.\n",
    "\n",
    "The decoder’s role is to gradually transform each word representation in the translated sentence into a word representation of the next word in the translation.\n",
    "\n",
    "After going through the decoder, each word representation goes through a final `Dense` layer with a softmax activation function, which will hopefully output a high probability for the correct next word and a low probability for all other words.\n",
    "\n",
    "Let’s go to details:\n",
    "- Both the encoder and the decoder contain modules that are stacked $N$ times. In the paper, $N=6$. The final outputs of the whole encoder stack are fed to the decoder at each of these $N$ levels.\n",
    "- There are two embedding layers; \n",
    "- Several skip connections, each of them followed by a layer normalization layer; \n",
    "- Several feedforward modules that are composed of two dense layers each (the first one using the ReLU activation function, the second with no activation function);\n",
    "- The output layer is a dense layer using the softmax activation function. \n",
    "- We can also use a bit of dropout after the attention layers and the feedforward modules.\n",
    "- Since all of these layers are time-distributed, each word is treated independently from all the others. But we can’t translate a sentence by looking at the words completely separately. That’s where the new components come in:\n",
    "  - The encoder’s *multi-head attention* layer updates each word representation by attending to (i.e., paying attention to) all other words in the same sentence.\n",
    "  - The decoder’s *masked multi-head attention* layer does the same thing, but when it processes a word, it doesn’t attend to words located after it: it’s a causal layer.\n",
    "  - The decoder’s upper *multi-head attention* layer is where the decoder pays attention to the words in the English sentence. This is called *cross*-attention, not *self*-attention in this case.\n",
    "  - The *positional encodings* are dense vectors (much like word embeddings) that represent the position of each word in the sentence. The $n^{\\text{th}}$ positional encoding is added to the word embedding of the $n^{\\text{th}}$ word in each sentence. This is needed because all layers in the transformer architecture ignore word positions: without positional encodings, we could shuffle the input sequences, and it would just shuffle the output sequences in the same way.\n",
    "\n",
    "**Note**: The first two arrows going into each multi-head attention layer represent the keys and values, and the third arrow represents the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional encodings\n",
    "The easiest way to implement this is to use an `Embedding` layer and make it encode all the positions from 0 to the maximum sequence length in the batch, then add the result to the word embeddings. The rules of broadcasting will ensure that the positional encodings get applied to every input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max length in the whole training set\n",
    "max_length = 50\n",
    "embed_size = 128\n",
    "# Ensures reproducibility on CPU\n",
    "keras.utils.set_random_seed(42)\n",
    "pos_embed_layer = keras.layers.Embedding(max_length, embed_size)\n",
    "batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
    "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
    "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
    "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use fixed, non-trainable positional encodings (however, when there is a large amount of pretraining data, trainable positional encodings are usually favored):\n",
    "\n",
    "**Equation 16-2** Sine/cosine positional encodings\n",
    "$$\n",
    "P_{p,i}=\\begin{cases}\n",
    "\\sin(p/10000^{i/d})&\\text{if $i$ is even}\n",
    "\\\\\\cos(p/10000^{(i-1)/d})&\\text{if $i$ is odd}\n",
    "\\end{cases}\n",
    "$$\n",
    "- $P_{p,i}$: The $i^{\\text{th}}$ component of the encoding for the word located at the $p^{\\text{th}}$ position in the sentence.\n",
    "- $d$: THe embedding size\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/16/positional_encoding.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_length: int,\n",
    "        embed_size: int,\n",
    "        dtype: tf.DType = tf.float32,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert embed_size % 2 == 0, 'embed_size must be even'\n",
    "        p, i = np.meshgrid(\n",
    "            np.arange(max_length), 2 * np.arange(embed_size // 2)\n",
    "        )\n",
    "        pos_emb = np.empty((1, max_length, embed_size))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        batch_max_length = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encodings[:, :batch_max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
    "encoder_in = pos_embed_layer(encoder_embeddings)\n",
    "decoder_in = pos_embed_layer(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head attention\n",
    "To understand how a multi-head attention layer works, we must first understand the *scaled dot-product attention* layer, which it is based on. It’s the same as Luong attention, except for a scaling factor.\n",
    "\n",
    "**Equation 16-3** Scaled dot-product attention\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\n",
    "=\\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_{keys}}})\\mathbf{V}\n",
    "$$\n",
    "- $\\mathbf{Q}$: A matrix containing one row per *query*. Its shape is [$n_{queries}$, $d_{keys}$], where $n_{queries}$ is the number of queries and $d_{keys}$ is the number of dimensions of each query and each key.\n",
    "- $\\mathbf{K}$: A matrix containing one row per *key*. Its shape is [$n_{keys}$, $d_{keys}$], where $n_{keys}$ is the number of keys and values.\n",
    "- $\\mathbf{V}$: A matrix containing one row per *value*. Its shape is [$n_{keys}$, $d_{values}$], where $d_{values}$ is the number of dimensions of each value.\n",
    "- The shape of $\\mathbf{Q}\\mathbf{K}^\\top$ is [$n_{queries}$, $n_{keys}$]: it contains one similarity score for each query/key pair. To prevent this matrix from being huge, the input sequences must not be too long. The final output has a shape of [$n_{queries}$, $d_{values}$]: there is one row per query, where each row represents the query result (a weighted sum of the values).\n",
    "- The scaling factor $1/(\\sqrt{d_{keys}})$ scales down the similarity scores to avoid saturating the softmax function, which would lead to tiny gradients.\n",
    "- It is possible to mask out some key/value pairs by adding a very large negative value to the corresponding similarity scores, just before computing the softmax. This is useful in the masked multi-head attention layer.\n",
    "\n",
    "**Note**: If we set `use_scale=True` when creating a `keras.layers.Attention` layer, then it will create an additional parameter that lets the layer learn how to properly downscale the similarity scores.\n",
    "\n",
    "**Note**: the Attention layer’s inputs are just like $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$, except with an extra batch dimension (the first dimension). Internally, the layer computes all the attention scores for all sentences in the batch with just one call to `tf.matmul(queries, keys)`:In TensorFlow, if `A` and `B` are tensors with more than two dimensions, say, of shape [2, 3, 4, 5] and [2, 3, 5, 6], respectively, then `tf.matmul(A, B)` will treat these tensors as 2 $\\times$ 3 arrays where each cell contains a matrix, and it will multiply the corresponding matrices: the matrix at the $i^{\\text{th}}$ row and $j^{\\text{th}}$ column in `A` will be multiplied by the matrix at the $i^{\\text{th}}$ row and $j^{\\text{th}}$ column in B. Since the product of a 4 $\\times$ 5 matrix with a 5 $\\times$ 6 matrix is a 4 $\\times$ 6 matrix, `tf.matmul(A, B)` will return an array of shape [2, 3, 4, 6].\n",
    "\n",
    "Now let’s look at the multi-head attention layer:\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/16/multi_head_attention_layer.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "It is just a bunch of scaled dot-product attention layers, each preceded by a linear transformation of the values, keys, and queries (i.e., a time-distributed dense layer with no activation function). All the outputs are simply concatenated, and they go through a final linear transformation (again, time-distributed).\n",
    "\n",
    "What is the intuition behind this architecture? Consider the word “like” in the sentence “I like soccer”. The encoder was smart enough to encode the fact that it is a verb. But the word representation also includes its position and many other features that are useful for its translation, such as the fact that it is in the present tense. If we just used a single scaled dot-product attention layer, we would only be able to query all of these characteristics in one shot. \n",
    "\n",
    "Applying *multiple* different linear transformations of the values, keys, and queries allows the model to apply many different projections of the word representation into different subspaces, each focusing on a subset of the word’s characteristics. Then the scaled dot-product attention layers implement the lookup phase, and finally we concatenate all the results and project them back to the original space.\n",
    "\n",
    "Let’s build the rest of the transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of 6\n",
    "N = 2\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "# For the first Dense layer in each Feed Forward block\n",
    "n_units = 128\n",
    "# [batch size, 1, max sequence length]\n",
    "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
    "Z = encoder_in\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate\n",
    "    )\n",
    "    Z = attn_layer(\n",
    "        Z,\n",
    "        value=Z,\n",
    "        # The layer now supports automatic masking\n",
    "        # attention_mask=encoder_pad_mask\n",
    "    )\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    Z = keras.layers.Dense(n_units, activation='relu')(Z)\n",
    "    Z = keras.layers.Dense(embed_size)(Z)\n",
    "    Z = keras.layers.Dropout(dropout_rate)(Z)\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A while a ago `MultiHeadAttention` layer did not support automatic masking. So we must handled it manually. Understanding manual masking is really helpful.\n",
    "\n",
    "The `MultiHeadAttention` layer accepts an `attention_mask` argument, which is a Boolean tensor of shape [*batch size*, *max query length*, *max value length*]: for every token in every query sequence, this mask indicates which tokens in the corresponding value sequence should be attended to. We want to tell the `MultiHeadAttention` layer to ignore all the padding tokens in the values. So, we first compute the padding mask using `tf.math.not_equal(encoder_input_ids, 0)`. This returns a Boolean tensor of shape [*batch size*, *max sequence length*]. We then insert a second axis using `[:, tf.newaxis]`, to get a mask of shape [*batch size*, *1*, *max sequence length*]. This allows us to use this mask as the `attention_mask` when calling the `MultiHeadAttention` layer: thanks to broadcasting, the same mask will be used for all tokens in each query. This way, the padding tokens in the values will be ignored correctly.\n",
    "\n",
    "**Note**: Currently `Z + skip` does not support automatic masking, which is why we had to write `keras.layers.Add()([Z, skip])` instead.\n",
    "\n",
    "Also in the decoder the first multi-head attention layer is a self-attention layer, like in the encoder, but it is a *masked* multi-head attention layer, meaning it is causal: it should ignore all tokens in the future. So, we need two masks: a padding mask and a causal mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
    "# Creates a lower triangular matrix\n",
    "causal_mask = tf.linalg.band_part(\n",
    "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.linalg.band_part()` function takes a tensor and returns a copy with all the values outside a diagonal band set to zero. With these arguments, we get a square matrix of size `batch_max_len_dec` (the max length of the input sequences in the batch), with 1s in the lower-left triangle and 0s in the upper right. If we use this mask as the attention mask, we will get exactly what we want. The causal mask only has two dimensions: it’s missing the batch dimension, but that’s okay since broadcasting ensures that it gets copied across all the instances in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s save the encoder’s final outputs\n",
    "encoder_outputs = Z\n",
    "# The decoder starts with its own inputs\n",
    "Z = decoder_in\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate\n",
    "    )\n",
    "    Z = attn_layer(\n",
    "        Z,\n",
    "        value=Z,\n",
    "        # The layer now supports automatic masking\n",
    "        use_causal_mask=True,\n",
    "        # attention_mask=causal_mask & decoder_pad_mask,\n",
    "    )\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    attn_layer = keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate\n",
    "    )\n",
    "    Z = attn_layer(\n",
    "        Z,\n",
    "        value=encoder_outputs,\n",
    "        # The layer now supports automatic masking\n",
    "        # attention_mask=encoder_pad_mask,\n",
    "    )\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    Z = keras.layers.Dense(n_units, activation='relu')(Z)\n",
    "    Z = keras.layers.Dense(embed_size)(Z)\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: The following cell will take a while to run (possibly 2 or 3 hours if we are not using a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 828s 263ms/step - loss: 0.2982 - accuracy: 0.5545 - val_loss: 0.2105 - val_accuracy: 0.6476\n",
      "Epoch 2/10\n",
      "3125/3125 [==============================] - 820s 262ms/step - loss: 0.2006 - accuracy: 0.6601 - val_loss: 0.1876 - val_accuracy: 0.6802\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - 820s 263ms/step - loss: 0.1842 - accuracy: 0.6816 - val_loss: 0.1766 - val_accuracy: 0.6975\n",
      "Epoch 4/10\n",
      "3125/3125 [==============================] - 820s 262ms/step - loss: 0.1748 - accuracy: 0.6942 - val_loss: 0.1704 - val_accuracy: 0.7055\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - 820s 262ms/step - loss: 0.1683 - accuracy: 0.7021 - val_loss: 0.1657 - val_accuracy: 0.7102\n",
      "Epoch 6/10\n",
      "3125/3125 [==============================] - 821s 263ms/step - loss: 0.1628 - accuracy: 0.7096 - val_loss: 0.1628 - val_accuracy: 0.7130\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - 826s 264ms/step - loss: 0.1588 - accuracy: 0.7154 - val_loss: 0.1595 - val_accuracy: 0.7205\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 822s 263ms/step - loss: 0.1550 - accuracy: 0.7205 - val_loss: 0.1590 - val_accuracy: 0.7199\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 821s 263ms/step - loss: 0.1518 - accuracy: 0.7249 - val_loss: 0.1547 - val_accuracy: 0.7258\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 821s 263ms/step - loss: 0.1492 - accuracy: 0.7279 - val_loss: 0.1538 - val_accuracy: 0.7281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8946cdf9a0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = keras.layers.Dense(vocab_size, activation='softmax')(Z)\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    (X_train, X_train_dec),\n",
    "    Y_train,\n",
    "    epochs=10,\n",
    "    validation_data=((X_valid, X_valid_dec), Y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me gusta el fútbol y yo también voy a la playa'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('I like soccer and also going to the beach')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: The Keras team has created a new [Keras NLP project](https://github.com/keras-team/keras-nlp), including an API to build a transformer more easily. We may also be interested in the new [Keras CV project](https://github.com/keras-team/keras-cv) for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Avalanche of Transformer Models\n",
    "The year 2018 has been called the “ImageNet moment for NLP”. Since then, progress has been astounding, with larger and larger transformer-based architectures trained on immense datasets.\n",
    "\n",
    "- First, the GPT paper [“Improving Language Understanding by Generative Pre-Training”](https://homl.info/gpt) by Alec Radford and other OpenAI researchers in 2018 once again demonstrated the effectiveness of unsupervised pretraining, like the ELMo and ULMFiT papers before it, but this time using a transformer-like architecture. The authors pretrained a large but fairly simple architecture composed of a stack of 12 transformer modules using only masked multi-head attention layers, like in the original transformer’s decoder. They trained it on a very large dataset, using the same autoregressive technique we used for our Shakespearean char-RNN: just predict the next token. This is a form of self-supervised learning. Then they fine-tuned it on various language tasks, using only minor adaptations for each task: text classification, *entailment* (whether sentence A imposes, involves, or implies sentence B as a necessary consequence), similarity (e.g., “Nice weather today” is very similar to “It is sunny”), and question answering (given a few paragraphs of text giving some context, the model must answer some multiple-choice questions).\n",
    "- Then in 2019 Google’s BERT paper [“BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”](https://homl.info/bert) by Jacob Devlin et al. came out: it also demonstrated the effectiveness of self-supervised pretraining on a large corpus, using a similar architecture to GPT but with nonmasked multi-head attention layers only, like in the original transformer’s encoder. This means that the model is naturally bidirectional; hence the B in BERT (*Bidirectional Encoder Representations from Transformers*). The authors proposed two pretraining tasks that explain most of the model’s strength:\n",
    "  \n",
    "  *Masked language model (MLM)*\n",
    "  - Each word in a sentence has a 15% probability of being masked, and the model is trained to predict the masked words. And then each selected word has an 80% chance of being masked, a 10% chance of being replaced by a random word (to reduce the discrepancy between pretraining and fine-tuning, since the model will not see \\<mask> tokens during fine-tuning), and a 10% chance of being left alone (to bias the model toward the correct answer).\n",
    "  \n",
    "  *Next sentence prediction (NSP)*\n",
    "  - The model is trained to predict whether two sentences are consecutive or not. \n",
    "  Later research showed that NSP was not as important as was initially thought, \n",
    "  so it was dropped in most later architectures.\n",
    "  \n",
    "  The model is trained on these two tasks simultaneously.\n",
    "  \n",
    "  <center>\n",
    "    <img \n",
    "      src=\"../images/16/bert.png\" \n",
    "      onerror=\"\n",
    "        this.onerror = null;\n",
    "        const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "        this.src = repo + this.src.split('..')[1];\n",
    "      \"\n",
    "    >\n",
    "  </center>\n",
    "  \n",
    "  For the NSP task, the authors inserted a class token (\\<CLS>) at the start of every input. The two input sentences are concatenated, separated only by a special separation token (\\<SEP>), and they are fed as input to the model. To help the model know which sentence each input token belongs to, a *segment embedding* is added on top of each token’s positional embeddings. The loss is only computed on the NSP prediction and the masked tokens, not on the unmasked ones.\n",
    "  \n",
    "  After this the model is then fine-tuned on many different tasks, changing very little for each task. e.g. for text classification such as sentiment analysis, all output tokens are ignored except for the first one, corresponding to the class token, and a new output layer replaces the previous one, which was just a binary classification layer for NSP.\n",
    "\n",
    "- In February 2019, Alec Radford, Jeffrey Wu, and other OpenAI researchers published the GPT-2 paper [“Language Models Are Unsupervised Multitask Learners”](https://homl.info/gpt2), which proposed a very similar architecture to GPT, but larger still (with over 1.5 billion parameters!). The researchers showed that the new and improved GPT model could perform *zero-shot learning* (ZSL), meaning it could achieve good performance on many tasks without any fine-tuning.\n",
    "- Google’s [“Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity”](https://homl.info/switch) introduced in January 2021 used 1 trillion parameters, and soon much larger models came out, such as the Wu Dao 2.0 model by the Beijing Academy of Artificial Intelligence (BAII), announced in June 2021.\n",
    "- Luckily, ingenious researchers are finding new ways to downsize transformers and make them more data-efficient. e.g. the [“DistilBERT, A Distilled Version of Bert: Smaller, Faster, Cheaper and Lighter”](https://homl.info/distilbert) model, introduced in October 2019 by Victor Sanh et al. from Hugging Face, is a small and fast transformer model based on BERT. It is available on Hugging Face’s excellent model hub.\n",
    "\n",
    "  DistilBERT was trained using *distillation* (hence the name): this means transferring knowledge from a teacher model to a student one, which is usually much smaller than the teacher model. This is typically done by using the teacher’s predicted probabilities for each training instance as targets for the student. Surprisingly, distillation often works better than training the student from scratch on the same dataset as the teacher! Indeed, the student benefits from the teacher’s more nuanced labels.\n",
    "\n",
    "- Many more transformer architectures came out after BERT, almost on a monthly basis, often improving on the state of the art across all NLP tasks: XLNet (June 2019), RoBERTa (July 2019), StructBERT (August 2019), ALBERT (September 2019), T5 (October 2019), ELECTRA (March 2020), GPT3 (May 2020), DeBERTa (June 2020), Switch Transformers (January 2021), Wu Dao 2.0 (June 2021), Gopher (December 2021), GPT-NeoX-20B (February 2022), Chinchilla (March 2022), OPT (May 2022), and the list goes on and on. Mariya Yao summarized many of these models in this post: https://homl.info/yaopost.\n",
    "- Let’s take a quick look at the T5 paper [“Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”](https://homl.info/t5) by Colin Raffel et al. at Google: it frames all NLP tasks as text-to-text, using an encoder–decoder transformer. e.g. to translate “I like soccer” to Spanish, we can just call the model with the input sentence “translate English to Spanish: I like soccer” and it outputs “me gusta el fútbol”. To summarize a paragraph, we just enter “summarize:” followed by the paragraph, and it outputs the summary. For classification, we only need to change the prefix to “classify:” and the model outputs the class name, as text. This simplifies using the model, and it also makes it possible to pretrain it on even more tasks.\n",
    "- In April 2022, Google researchers used a new large-scale training platform named *Pathways* to train a humongous language model named the *Pathways Language Model* (PaLM) described in [“PaLM: Scaling Language Modeling with Pathways”](https://homl.info/palm) by Aakanksha Chowdhery et al. with a whopping 540 billion parameters, using over 6,000 TPUs. This model is a standard transformer, using only masked multi-head attention layers, with just a few tweaks. This model achieved incredible performance on all sorts of NLP tasks, particularly in natural language understanding (NLU). It’s capable of impressive feats, such as explaining jokes, giving detailed step-by-step answers to questions, and even coding. \n",
    "- PaLM strength is in part due to the model’s size, but also thanks to a technique called *Chain of thought prompting*, which was introduced a couple months earlier by another team of Google researchers in [“Chain of Thought Prompting Elicits Reasoning in Large Language Models”](https://homl.info/ctp) by Jason Wei et al.\n",
    "\n",
    "  In question answering tasks, regular prompting typically includes a few examples of questions and answers, such as: “Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: 11.” The prompt then continues with the actual question, such as “Q: John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care of their business. How many hours a week does he spend taking care of dogs? A:”, and the model’s job is to append the answer: in this case, “35.”\n",
    "\n",
    "  But with chain of thought prompting, the example answers include all the reasoning steps that lead to the conclusion. e.g. instead of “A: 11”, the prompt contains “A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11.” This encourages the model to give a detailed answer to the actual question, such as “John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care of their business. So that is 10 $\\times$ .5 = 5 hours a day. 5 hours a day $\\times$ 7 days a week = 35 hours a week. The answer is 35 hours a week.” This is an actual example from the paper!\n",
    "\n",
    "  Not only does the model give the right answer much more frequently than using regular prompting, we’re encouraging the model to think things through, but it also provides all the reasoning steps, which can be useful to better understand the rationale behind a model’s answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformers\n",
    "- One of the first applications of attention mechanisms beyond NMT was in generating image captions using visual attention proposed in [“Show, Attend and Tell: Neural Image Caption Generation with Visual Attention”](https://homl.info/visualattention) by Kelvin Xu et al. in 2015: a convolutional neural network first processes the image and outputs some feature maps, then a decoder RNN equipped with an attention mechanism generates the caption, one word at a time. \n",
    "  \n",
    "  At each decoder time step (i.e., each word), the decoder uses the attention model to focus on just the right part of the image.\n",
    "  \n",
    "  <center>\n",
    "    <img \n",
    "      src=\"../images/16/visual_attention.png\" \n",
    "      onerror=\"\n",
    "        this.onerror = null;\n",
    "        const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "        this.src = repo + this.src.split('..')[1];\n",
    "      \"\n",
    "    >\n",
    "  </center>\n",
    "  \n",
    "  Here the model generated the caption “A woman is throwing a frisbee in a park”, and we can see what part of the input image the decoder focused its attention on when it was about to output the word “frisbee”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid;\">\n",
    "\n",
    "### Explainability\n",
    "One extra benefit of attention mechanisms is that they make it easier to understand what led the model to produce its output. This is called *explainability*. It can be especially useful when the model makes a mistake: e.g. if an image of a dog walking in the snow is labeled as “a wolf walking in the snow”, then we can go back and check what the model focused on when it output the word “wolf”. We may find that it was paying attention not only to the dog, but also to the snow, hinting at a possible explanation: perhaps the way the model learned to distinguish dogs from wolves is by checking whether or not there’s a lot of snow around. We can then fix this by training the model with more images of wolves without snow, and dogs with snow. This example comes from a great 2016 paper [“‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier”](https://homl.info/explainclass) by Marco Tulio Ribeiro et al. that uses a different approach to explainability: learning an interpretable model locally around a classifier’s prediction.\n",
    "\n",
    "In some applications, explainability is not just a tool to debug a model; it can be a legal requirement: think of a system deciding whether or not it should grant us a loan.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At first transformer used alongside CNNs, without replacing them. Instead, transformers were generally used to replace RNNs. Transformers became slightly more visual in a 2020 paper [“End-to-End Object Detection with Transformers”](https://homl.info/detr) by Nicolas Carion et al. in Facebook, which proposed a hybrid CNN–transformer architecture for object detection. The CNN first processes the input images and outputs a set of feature maps, then these feature maps are converted to sequences and fed to a transformer, which outputs bounding box predictions.\n",
    "- In October 2020, a team of Google researchers released a paper [“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale”](https://homl.info/vit) Alexey Dosovitskiy et al. that introduced a fully transformer-based vision model, called a *vision transformer* (ViT). They choped the image into little 16 $\\times$ 16 squares, and treat the sequence of squares as if it were a sequence of word representations. The squares are first flattened into 16 $\\times$ 16 $\\times$ 3 = 768-dimensional vectors, then these vectors go through a linear layer that transforms them but retains their dimensionality. The resulting sequence of vectors can then be treated just like a sequence of word embeddings: this means adding positional embeddings, and passing the result to the transformer. This model beat the state of the art on ImageNet image classification, but the authors had to use over 300 million additional images for training. This makes sense since transformers don’t have as many *inductive biases* as convolution neural nets, so they need extra data just to learn things that CNNs implicitly assume.\n",
    "\n",
    "**Note**: An inductive bias is an implicit assumption made by the model, due to its architecture. Linear models implicitly assume that the data is linear. CNNs implicitly assume that patterns learned in one location will likely be useful in other locations. RNNs implicitly assume that the inputs are ordered, and that recent tokens are more important than older ones. The more inductive biases a model has, assuming they are correct, the less training data the model will require.\n",
    "\n",
    "- Just two months later, a team of Facebook researchers led by Hugo Touvron released a paper [“Training Data-Efficient Image Transformers & Distillation Through Attention”](https://homl.info/deit) that introduced *data-efficient image transformers* (DeiTs). Their model achieved competitive results on ImageNet without requiring any additional data for training. The model’s architecture is virtually the same as the original ViT, but the authors used a distillation technique to transfer knowledge from state-of-the-art CNN models to their model.\n",
    "- In March 2021, DeepMind released an important paper [“Perceiver: General Perception with Iterative Attention”](https://homl.info/perceiver) by Andrew Jaegle et al., that introduced the *Perceiver* architecture.\n",
    "  - It is a *multimodal* transformer, meaning we can feed it text, images, audio, or virtually any other modality. Until then, transformers had been restricted to fairly short sequences because of the performance and RAM bottleneck in the attention layers. This excluded modalities such as audio or video, and it forced researchers to treat images as sequences of patches, rather than sequences of pixels. \n",
    "  - The bottleneck is due to self-attention, where every token must attend to every other token: if the input sequence has $M$ tokens, then the attention layer must compute an $M\\times M$ matrix, which can be huge if $M$ is very large.\n",
    "  - The Perceiver solves this problem by gradually improving a fairly short *latent representation* of the inputs, composed of $N$ tokens, typically just a few hundred. (The word *latent* means hidden, or internal.)\n",
    "  - The model uses cross-attention layers only, feeding them the latent representation as the queries, and the (possibly large) inputs as the values. This only requires computing an $M\\times N$ matrix, so the computational complexity is linear with regard to $M$, instead of quadratic. \n",
    "  - After going through several cross-attention layers, the latent representation ends up capturing everything that matters in the inputs. \n",
    "  - The authors also suggested sharing the weights between consecutive cross-attention layers: if we do that, then the Perceiver effectively becomes an RNN.\n",
    "    - The shared cross-attention layers can be seen as the same memory cell at different time steps\n",
    "    - And the latent representation corresponds to the cell’s context vector.\n",
    "    - The same inputs are repeatedly fed to the memory cell at every time step.\n",
    "    \n",
    "    It looks like RNNs are not dead after all!\n",
    "- Just a month later, Mathilde Caron et al. introduced DINO in [“Emerging Properties in Self-Supervised Vision Transformers”](https://homl.info/dino), an impressive vision transformer trained entirely without labels, using self-supervision, and capable of high-accuracy semantic segmentation. \n",
    "  - The model is duplicated during training, with one network acting as a teacher and the other acting as a student. \n",
    "  - Gradient descent only affects the student, while the teacher’s weights are just an exponential moving average of the student’s weights.\n",
    "  - The student is trained to match the teacher’s predictions: since they’re almost the same model, this is called *self-distillation*.\n",
    "  - At each training step, the input images are augmented in different ways for the teacher and the student, so they don’t see the exact same image, but their predictions must match. This forces them to come up with high-level representations.\n",
    "  - To prevent *mode collapse*, where both the student and the teacher would always output the same thing, completely ignoring the inputs, DINO keeps track of a moving average of the teacher’s outputs, and it tweaks the teacher’s predictions to ensure that they remain centered on zero, on average.\n",
    "  - DINO also forces the teacher to have high confidence in its predictions: this is called *sharpening*.\n",
    "  \n",
    "  Together, these techniques preserve diversity in the teacher’s outputs.\n",
    "- In a 2021 paper [“Scaling Vision Transformers”](https://homl.info/scalingvits) by Xiaohua Zhai et al., Google researchers showed how to scale ViTs up or down, depending on the amount of data. They managed to create a huge 2 billion parameter model that reached over 90.4% top-1 accuracy on ImageNet. Conversely, they also trained a scaled-down model that reached over 84.8% top-1 accuracy on ImageNet, using only 10,000 images: that’s just 10 images per class!\n",
    "- In March 2022, a paper [“Model Soups: Averaging Weights of Multiple Fine-tuned Models Improves Accuracy Without Increasing Inference Time”](https://homl.info/modelsoups) by Mitchell Wortsman et al. demonstrated that it’s possible to first train multiple transformers, then average their weights to create a new and improved model. This is similar to an ensemble, except there’s just one model in the end, which means there’s no inference time penalty.\n",
    "- OpenAI’s 2021 CLIP paper [“Learning Transferable Visual Models From Natural Language Supervision”](https://homl.info/clip) Alec Radford et al. proposed a large transformer model pretrained to match captions with images: this task allows it to learn excellent image representations, and the model can then be used directly for tasks such as image classification using simple text prompts such as “a photo of a cat”.\n",
    "- Soon after, OpenAI announced DALL·E paper [“Zero-Shot Text-to-Image Generation”](https://homl.info/dalle), capable of generating amazing images based on text prompts and then DALL·E 2 paper [“Hierarchical Text-Conditional Image Generation with CLIP Latents”](https://homl.info/dalle2) which generates even higher quality images using a diffusion model both by Aditya Ramesh et al.\n",
    "- In April 2022, DeepMind released the Flamingo paper [“Flamingo: a Visual Language Model for Few-Shot Learning”](https://homl.info/flamingo) by Jean-Baptiste Alayrac et al., which introduced a family of models pretrained on a wide variety of tasks across multiple modalities, including text, images, and videos. A single model can be used across very different tasks, such as question answering, image captioning, and more.\n",
    "- Soon after, in May 2022, DeepMind introduced GATO paper [“A Generalist Agent”](https://homl.info/gato) by Scott Reed et al., a multimodal model that can be used as a policy for a reinforcement learning agent. The same transformer can chat with us, caption images, play Atari games, control (simulated) robotic arms, and more, all with “only” 1.2 billion parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face’s Transformers Library\n",
    "Transformers library allows us to easily download a pretrained model, including its corresponding tokenizer, and then fine-tune it on our own dataset, if needed. Plus, the library supports TensorFlow, PyTorch, and JAX.\n",
    "\n",
    "The simplest way to use the Transformers library is to use the `transformers.pipeline()` function: we just specify which task we want, such as sentiment analysis, and it downloads a default pretrained model:\n",
    "\n",
    "Install the Transformers if we’re running on Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "    %pip install -q -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "result = classifier('The actors were very convincing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be very biased. e.g. it may like or dislike some countries depending on the data it was trained on, and how it is used, so use it with care:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9896161556243896},\n",
       " {'label': 'NEGATIVE', 'score': 0.9811071157455444}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(['I am from India.', 'I am from Iraq.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text classification tasks such as sentiment analysis, at the time of writing, it defaults to `distilbert-base-uncased-finetuned-sst-2-english`, a DistilBERT model with an uncased tokenizer, trained on English Wikipedia and a corpus of English books, and fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task. We could use a DistilBERT model fine-tuned on the Multi-Genre Natural Language Inference (MultiNLI) task, which classifies two sentences into three classes: contradiction, neutral, or entailment. Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at huggingface/distilbert-base-uncased-finetuned-mnli were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at huggingface/distilbert-base-uncased-finetuned-mnli and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'contradiction', 'score': 0.9790192246437073}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'huggingface/distilbert-base-uncased-finetuned-mnli'\n",
    "classifier_mnli = pipeline('text-classification', model=model_name)\n",
    "classifier_mnli('She loves me. [SEP] She loves me not.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: We can find the available models at https://huggingface.co/models, and the list of tasks at https://huggingface.co/tasks.\n",
    "\n",
    "Let’s load the same DistilBERT model, along with its corresponding tokenizer, using the `TFAutoModelForSequenceClassification` and `AutoTokenizer` classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at huggingface/distilbert-base-uncased-finetuned-mnli were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at huggingface/distilbert-base-uncased-finetuned-mnli and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n",
       "         102,    0,    0,    0],\n",
       "       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n",
       "        2003, 2214, 1012,  102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer(\n",
    "    [\n",
    "        'I like soccer. [SEP] We all love soccer!',\n",
    "        'Joe lived for a very long time. [SEP] Joe is old.',\n",
    "    ],\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    ")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a dictionary-like instance of the `BatchEncoding` class, which contains the sequences of token IDs, as well as a mask containing 0s for the padding tokens.\n",
    "\n",
    "**Tip**: Instead of passing `'Sentence 1 [SEP] Sentence 2'` to the tokenizer, you can equivalently pass it a tuple: `('Sentence 1', 'Sentence 2')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n",
       "         102,    0,    0,    0],\n",
       "       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n",
       "        2003, 2214, 1012,  102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer(\n",
    "    [\n",
    "        ('I like soccer.', 'We all love soccer!'),\n",
    "        ('Joe lived for a very long time.', 'Joe is old.'),\n",
    "    ],\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    ")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set `return_token_type_ids=True` when calling the tokenizer, we will also get an extra tensor that indicates which sentence each token belongs to. This is needed by some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[-2.1123817 ,  1.1786783 ,  1.4101017 ],\n",
       "       [-0.01478387,  1.0962474 , -0.9919954 ]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(token_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.01619702, 0.43523544, 0.5485676 ],\n",
       "       [0.22655967, 0.6881726 , 0.0852678 ]], dtype=float32)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_probas = keras.activations.softmax(outputs.logits)\n",
    "Y_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 1])>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = tf.argmax(Y_probas, axis=1)\n",
    "# 0 = Contradiction, 1 = entailment, 2 = Neutral\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fine-tune this model on our own dataset, and train the model as usual with Keras since it’s just a regular Keras model with a few extra methods. Because the model outputs logits instead of probabilities, we must use the `keras.losses.SparseCategoricalCrossentropy(from_logits=True)` loss instead of the usual `'sparse_categorical_crossentropy'` loss. Moreover, the model does not support `BatchEncoding` inputs during training, so we must use its `data` attribute to get a regular dictionary instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.1190 - accuracy: 0.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 491ms/step - loss: 0.6666 - accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "sentences = [('Sky is blue', 'Sky is red'), ('I love her', 'She loves me')]\n",
    "X_train = tokenizer(sentences, padding=True, return_tensors='tf').data\n",
    "# Contradiction, Neutral\n",
    "y_train = tf.constant([0, 2])\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer='nadam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face has also built a Datasets library that we can use to easily download a standard dataset (such as IMDb) or a custom one, and use it to fine-tune our model. It’s similar to TensorFlow Datasets, but it also provides tools to perform common preprocessing tasks on the fly, such as masking. The list of datasets is available at https://huggingface.co/datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. to 8.\n",
    "1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
    "> Stateless RNNs can only capture patterns whose length is less than, or equal to, the size of the windows the RNN is trained on. Conversely, stateful RNNs can capture longer-term patterns. However, implementing a stateful RNN is much harder⁠, especially preparing the dataset properly. Moreover, stateful RNNs do not always work better, in part because consecutive batches are not independent and identically distributed (IID). Gradient descent is not fond of non-IID datasets.\n",
    "2. Why do people use encoder–decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
    "> In general, if we translate a sentence one word at a time, the result will be terrible. e.g. the French sentence “Je vous en prie” means “You are welcome”, but if we translate it one word at a time, we get “I you in pray” Huh? It is much better to read the whole sentence first and then translate it. A plain sequence-to-sequence RNN would start translating a sentence immediately after reading the first word, while an encoder–decoder RNN will first read the whole sentence and then translate it. That said, one could imagine a plain sequence-to-sequence RNN that would output silence whenever it is unsure about what to say next (just like human translators do when they must translate a live broadcast).\n",
    "3. How can we deal with variable-length input sequences? What about variable-length output sequences?\n",
    "> Variable-length input sequences can be handled by padding the shorter sequences so that all sequences in a batch have the same length, and using masking to ensure the RNN ignores the padding token. For better performance, we may also want to create batches containing sequences of similar sizes. Ragged tensors can hold sequences of variable lengths, and Keras now supports them, which simplifies handling variable-length input sequences (at the time of this writing, it still does not handle ragged tensors as targets on the GPU, though). Regarding variable-length output sequences, if the length of the output sequence is known in advance (e.g., if we know that it is the same as the input sequence), then we just need to configure the loss function so that it ignores tokens that come after the end of the sequence. Similarly, the code that will use the model should ignore tokens beyond the end of the sequence. But generally the length of the output sequence is not known ahead of time, so the solution is to train the model so that it outputs an end-of-sequence token at the end of each sequence.\n",
    "4. What is beam search, and why would we use it? What tool can we use to implement it?\n",
    "> Beam search is a technique used to improve the performance of a trained encoder–decoder model, e.g. in a neural machine translation system. The algorithm keeps track of a short list of the *k* most promising output sentences (say, the top three), and at each decoder step it tries to extend them by one word; then it keeps only the *k* most likely sentences. The parameter *k* is called the *beam width*: the larger it is, the more CPU and RAM will be used, but also the more accurate the system will be. Instead of greedily choosing the most likely next word at each step to extend a single sentence, this technique allows the system to explore several promising sentences simultaneously. Moreover, this technique lends itself well to parallelization. we can implement beam search by writing a custom memory cell. Alternatively, TensorFlow Addons’s seq2seq API provides an implementation.\n",
    "5. What is an attention mechanism? How does it help?\n",
    "> An attention mechanism is a technique initially used in encoder–decoder models to give the decoder more direct access to the input sequence, allowing it to deal with longer input sequences. At each decoder time step, the current decoder’s state and the full output of the encoder are processed by an alignment model that outputs an alignment score for each input time step. This score indicates which part of the input is most relevant to the current decoder time step. The weighted sum of the encoder output (weighted by their alignment score) is then fed to the decoder, which produces the next decoder state and the output for this time step. The main benefit of using an attention mechanism is the fact that the encoder–decoder model can successfully process longer input sequences. Another benefit is that the alignment scores make the model easier to debug and interpret: e.g. if the model makes a mistake, we can look at which part of the input it was paying attention to, and this can help diagnose the issue. An attention mechanism is also at the core of the transformer architecture, in the multi-head attention layers. See the next answer.\n",
    "6. What is the most important layer in the transformer architecture? What is its purpose?\n",
    "> The multi-head attention layer (the original transformer architecture contains 18 of them, including 6 masked multi-head attention layers). It is at the core of language models such as BERT and GPT-2. Its purpose is to allow the model to identify which words are most aligned with each other, and then improve each word’s representation using these contextual clues.\n",
    "7. When would we need to use sampled softmax?\n",
    "> Sampled softmax is used when training a classification model when there are many classes (e.g., thousands). It computes an approximation of the cross-entropy loss based on the logit predicted by the model for the correct class, and the predicted logits for a sample of incorrect words. This speeds up training considerably compared to computing the softmax over all logits and then estimating the cross-entropy loss. After training, the model can be used normally, using the regular softmax function to compute all the class probabilities based on all the logits.\n",
    "8. *Embedded Reber grammars* were used by Hochreiter and Schmidhuber in [their paper](https://homl.info/93) about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE”. Check out Jenny Orr’s [nice introduction](https://homl.info/108) to this topic, then choose a particular embedded Reber grammar (such as the one represented on Jenny Orr’s page), then train an RNN to identify whether a string respects that grammar or not. We will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t.\n",
    "> First we need to build a function that generates strings based on a grammar. The grammar will be represented as a list of possible transitions for each state. A transition specifies the string to output (or a grammar to generate it) and the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_reber_grammar = [\n",
    "    # (state 0) =B=>(state 1)\n",
    "    [('B', 1)],\n",
    "    # (state 1) =T=>(state 2) or =P=>(state 3)\n",
    "    [('T', 2), ('P', 3)],\n",
    "    # (state 2) =S=>(state 2) or =X=>(state 4)\n",
    "    [('S', 2), ('X', 4)],\n",
    "    # and so on...\n",
    "    [('T', 3), ('V', 5)],\n",
    "    [('X', 3), ('S', 6)],\n",
    "    [('P', 4), ('V', 6)],\n",
    "    # (state 6) =E=>(terminal state)\n",
    "    [('E', None)],\n",
    "]\n",
    "\n",
    "embedded_reber_grammar = [\n",
    "    [('B', 1)],\n",
    "    [('T', 2), ('P', 3)],\n",
    "    [(default_reber_grammar, 4)],\n",
    "    [(default_reber_grammar, 5)],\n",
    "    [('T', 6)],\n",
    "    [('P', 6)],\n",
    "    [('E', None)],\n",
    "]\n",
    "\n",
    "Grammer = list[list[tuple[str | 'Grammer', Optional[int]]]]\n",
    "\n",
    "\n",
    "def generate_string(grammar: Grammer) -> str:\n",
    "    state = 0\n",
    "    output = []\n",
    "    while state is not None:\n",
    "        index = np.random.randint(len(grammar[state]))\n",
    "        production, state = grammar[state][index]\n",
    "        if isinstance(production, list):\n",
    "            production = generate_string(grammar=production)\n",
    "        output.append(production)\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s generate a few strings based on the default Reber grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTXXTTVPXTVPXTTVPSE BPVPSE BTXSE BPVVE BPVVE BTSXSE BPTVPXTTTVVE BPVVE BTXSE BTXXVPSE BPTTTTTTTTVVE BTXSE BPVPSE BTXSE BPTVPSE BTXXTVPSE BPVVE BPVVE BPVVE BPTTVVE BPVVE BPVVE BTXXVVE BTXXVVE BTXXVPXVVE "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_string(default_reber_grammar), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looks good. Now let’s generate a few strings based on the embedded Reber grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBPTTTVPXTVPXTTVPSETE BPBPTVPSEPE BPBPVVEPE BPBPVPXVVEPE BPBTXXTTTTVVEPE BPBPVPSEPE BPBTXXVPSEPE BPBTSSSSSSSXSEPE BTBPVVETE BPBTXXVVEPE BPBTXXVPSEPE BTBTXXVVETE BPBPVVEPE BPBPVVEPE BPBTSXSEPE BPBPVVEPE BPBPTVPSEPE BPBTXXVVEPE BTBPTVPXVVETE BTBPVVETE BTBTSSSSSSSXXVVETE BPBTSSSXXTTTTVPSEPE BTBPTTVVETE BPBTXXTVVEPE BTBTXSETE "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_string(embedded_reber_grammar), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Okay, now we need a function to generate strings that do not respect the grammar. We could generate a random string, but the task would be a bit too easy, so instead we will generate a string that respects the grammar, and we will corrupt it by changing just one character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLE_CHARS = 'BEPSTVX'\n",
    "\n",
    "\n",
    "def generate_corrupted_string(\n",
    "    grammar: Grammer, chars: str = POSSIBLE_CHARS\n",
    ") -> str:\n",
    "    good_string = generate_string(grammar)\n",
    "    index = np.random.randint(len(good_string))\n",
    "    good_char = good_string[index]\n",
    "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
    "    return good_string[:index] + bad_char + good_string[index + 1 :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s look at a few corrupted strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBPTTTPPXTVPXTTVPSETE BPBTXEEPE BPBPTVVVEPE BPBTSSSSXSETE BPTTXSEPE BTBPVPXTTTTTTEVETE BPBTXXSVEPE BSBPTTVPSETE BPBXVVEPE BEBTXSETE BPBPVPSXPE BTBPVVVETE BPBTSXSETE BPBPTTTPTTTTTVPSEPE BTBTXXTTSTVPSETE BBBTXSETE BPBTPXSEPE BPBPVPXTTTTVPXTVPXVPXTTTVVEVE BTBXXXTVPSETE BEBTSSSSSXXVPXTVVETE BTBXTTVVETE BPBTXSTPE BTBTXXTTTVPSBTE BTBTXSETX BTBTSXSSTE "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_corrupted_string(embedded_reber_grammar), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We cannot feed strings directly to an RNN, so we need to encode them somehow. One option would be to one-hot encode each character. Another option is to use embeddings. Let’s go for the second option (but since there are just a handful of characters, one-hot encoding would probably be a good option as well). For embeddings to work, we need to convert each string into a sequence of character IDs. Let’s write a function for that, using each character’s index in the string of possible characters `'BEPSTVX'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_ids(s: str, chars: str = POSSIBLE_CHARS) -> list[int]:\n",
    "    return [chars.index(c) for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 4, 4, 6, 6, 5, 5, 1, 4, 1]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_ids('BTTTXXVVETE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can now generate the dataset, with 50% good strings, and 50% bad strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(size: int) -> tuple[tf.RaggedTensor, np.ndarray]:\n",
    "    good_strings = [\n",
    "        string_to_ids(generate_string(embedded_reber_grammar))\n",
    "        for _ in range(size // 2)\n",
    "    ]\n",
    "    bad_strings = [\n",
    "        string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n",
    "        for _ in range(size - size // 2)\n",
    "    ]\n",
    "    all_strings = good_strings + bad_strings\n",
    "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
    "    y = np.array(\n",
    "        [[1.0] for _ in range(len(good_strings))]\n",
    "        + [[0.0] for _ in range(len(bad_strings))]\n",
    "    )\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, y_train = generate_dataset(10000)\n",
    "X_valid, y_valid = generate_dataset(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s take a look at the first training sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(22,), dtype=int32, numpy=\n",
       "array([0, 4, 0, 2, 4, 4, 4, 5, 2, 6, 4, 5, 2, 6, 4, 4, 5, 2, 3, 1, 4, 1],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What class does it belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Perfect! We are ready to create the RNN to identify good strings. We build a simple sequence binary classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 4s 8ms/step - loss: 0.6910 - accuracy: 0.5095 - val_loss: 0.6825 - val_accuracy: 0.5645\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6678 - accuracy: 0.5659 - val_loss: 0.6635 - val_accuracy: 0.6105\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6504 - accuracy: 0.5766 - val_loss: 0.6521 - val_accuracy: 0.6110\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.6347 - accuracy: 0.5980 - val_loss: 0.6224 - val_accuracy: 0.6445\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6054 - accuracy: 0.6361 - val_loss: 0.5779 - val_accuracy: 0.6980\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5414 - accuracy: 0.7093 - val_loss: 0.4695 - val_accuracy: 0.7795\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.3756 - accuracy: 0.8418 - val_loss: 0.2685 - val_accuracy: 0.9115\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2601 - accuracy: 0.9044 - val_loss: 0.1534 - val_accuracy: 0.9615\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1774 - accuracy: 0.9427 - val_loss: 0.1063 - val_accuracy: 0.9735\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0624 - accuracy: 0.9826 - val_loss: 0.0219 - val_accuracy: 0.9975\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0371 - accuracy: 0.9914 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 8.7265e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 6.7552e-04 - accuracy: 1.0000 - val_loss: 4.9408e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 4.4514e-04 - accuracy: 1.0000 - val_loss: 3.6322e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 3.3943e-04 - accuracy: 1.0000 - val_loss: 2.8524e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.7723e-04 - accuracy: 1.0000 - val_loss: 2.3880e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.3477e-04 - accuracy: 1.0000 - val_loss: 2.0363e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.0382e-04 - accuracy: 1.0000 - val_loss: 1.7760e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.8077e-04 - accuracy: 1.0000 - val_loss: 1.5916e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.6246e-04 - accuracy: 1.0000 - val_loss: 1.4362e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "embedding_size = 5\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(\n",
    "            input_shape=[None], dtype=tf.int32, ragged=True\n",
    "        ),\n",
    "        keras.layers.Embedding(\n",
    "            input_dim=len(POSSIBLE_CHARS), output_dim=embedding_size\n",
    "        ),\n",
    "        keras.layers.GRU(30),\n",
    "        keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ]\n",
    ")\n",
    "optimizer = keras.optimizers.SGD(\n",
    "    learning_rate=0.02, momentum=0.95, nesterov=True\n",
    ")\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, y_train, epochs=20, validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let’s test our RNN on two tricky strings: the first one is bad while the second one is good. They only differ by the second to last character. If the RNN gets this right, it shows that it managed to notice the pattern that the second letter should always be equal to the second to last letter. That requires a fairly long short-term memory (which is the reason why we used a GRU cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated probability that these are Reber strings:\n",
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE: 0.02%\n",
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE: 99.99%\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\n",
    "    'BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE',\n",
    "    'BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE',\n",
    "]\n",
    "X_test = tf.ragged.constant(\n",
    "    [string_to_ids(s) for s in test_strings], ragged_rank=1\n",
    ")\n",
    "\n",
    "y_proba = model.predict(X_test)\n",
    "print()\n",
    "print('Estimated probability that these are Reber strings:')\n",
    "for index, string in enumerate(test_strings):\n",
    "    print(f'{string}: {100 * y_proba[index][0]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta-da! It worked fine. The RNN found the correct answers with very high confidence. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "Train an encoder–decoder model that can convert a date string from one format to another (e.g., from “April 22, 2019” to “2019-04-22”).\n",
    "> Let’s start by creating the dataset. We will use random days between 1000-01-01 and 9999-12-31:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# Cannot use strftime()’s %B format since it depends on the locale\n",
    "MONTHS = [\n",
    "    'January',\n",
    "    'February',\n",
    "    'March',\n",
    "    'April',\n",
    "    'May',\n",
    "    'June',\n",
    "    'July',\n",
    "    'August',\n",
    "    'September',\n",
    "    'October',\n",
    "    'November',\n",
    "    'December',\n",
    "]\n",
    "\n",
    "\n",
    "def random_dates(n_dates: int) -> tuple[list[str], ...]:\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "\n",
    "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "\n",
    "    x = [MONTHS[dt.month - 1] + ' ' + dt.strftime('%d, %Y') for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here are a few random dates, displayed in both the input format and the target format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "September 20, 7075       7075-09-20               \n",
      "May 15, 8579             8579-05-15               \n",
      "January 11, 7103         7103-01-11               \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print('{:25s}{:25s}'.format('Input', 'Target'))\n",
    "print('-' * 50)\n",
    "for idx in range(n_dates):\n",
    "    print('{:25s}{:25s}'.format(x_example[idx], y_example[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s get the list of all possible characters in the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ,0123456789ADFJMNOSabceghilmnoprstuvy'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_CHARS = ''.join(sorted(set(''.join(MONTHS) + '0123456789, ')))\n",
    "INPUT_CHARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And here’s the list of possible characters in the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHARS = '0123456789-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s write a function to convert a string to a list of character IDs, as we did in the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_str_to_ids(date_str: str, chars: str = INPUT_CHARS) -> list[int]:\n",
    "    return [chars.index(c) for c in date_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 23, 31, 34, 23, 28, 21, 23, 32, 0, 4, 2, 1, 0, 9, 2, 9, 7]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(x_example[0], INPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(\n",
    "    date_strs: list[str], chars: str = INPUT_CHARS\n",
    ") -> tf.Tensor:\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    # Using 0 as the padding token ID\n",
    "    return (X + 1).to_tensor()\n",
    "\n",
    "\n",
    "def create_dataset(n_dates: int) -> tuple[tf.Tensor, ...]:\n",
    "    x, y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(\n",
    "        y, OUTPUT_CHARS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, Y_train = create_dataset(10000)\n",
    "X_valid, Y_valid = create_dataset(2000)\n",
    "X_test, Y_test = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1], dtype=int32)>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### First version: a very basic seq2seq model\n",
    "> Let’s first try the simplest possible model: we feed in the input sequence, which first goes through the encoder (an embedding layer followed by a single LSTM layer), which outputs a vector, then it goes through a decoder (a single LSTM layer, followed by a dense output layer), which outputs a sequence of vectors, each representing the estimated probabilities for all possible output character.\n",
    ">\n",
    "> Since the decoder expects a sequence as input, we repeat the vector (which is output by the encoder) as many times as the longest possible output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 10s 23ms/step - loss: 1.8150 - accuracy: 0.3489 - val_loss: 1.3726 - val_accuracy: 0.4939\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 1.2447 - accuracy: 0.5510 - val_loss: 1.0725 - val_accuracy: 0.6115\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 1.0937 - accuracy: 0.6125 - val_loss: 1.0548 - val_accuracy: 0.6130\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 1.0032 - accuracy: 0.6413 - val_loss: 3.8747 - val_accuracy: 0.1788\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.8159 - accuracy: 0.7023 - val_loss: 0.6623 - val_accuracy: 0.7474\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.5645 - accuracy: 0.7795 - val_loss: 0.5005 - val_accuracy: 0.8032\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.5037 - accuracy: 0.8103 - val_loss: 0.3798 - val_accuracy: 0.8500\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.3131 - accuracy: 0.8795 - val_loss: 0.2582 - val_accuracy: 0.9043\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.2141 - accuracy: 0.9280 - val_loss: 0.1637 - val_accuracy: 0.9498\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.1282 - accuracy: 0.9650 - val_loss: 0.0918 - val_accuracy: 0.9774\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0669 - accuracy: 0.9871 - val_loss: 0.3368 - val_accuracy: 0.8871\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.1551 - accuracy: 0.9662 - val_loss: 0.0398 - val_accuracy: 0.9949\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0291 - accuracy: 0.9969 - val_loss: 0.0240 - val_accuracy: 0.9984\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 9s 30ms/step - loss: 0.0182 - accuracy: 0.9986 - val_loss: 0.0161 - val_accuracy: 0.9993\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 9s 30ms/step - loss: 0.0119 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9997\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0082 - accuracy: 0.9998 - val_loss: 0.0083 - val_accuracy: 0.9999\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.0059 - accuracy: 0.9999 - val_loss: 0.0058 - val_accuracy: 0.9999\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9999\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 0.9999\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 12s 40ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = Y_train.shape[1]\n",
    "\n",
    "np.random.seed(42)\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "encoder = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Embedding(\n",
    "            input_dim=len(INPUT_CHARS) + 1,\n",
    "            output_dim=embedding_size,\n",
    "            input_shape=[None],\n",
    "        ),\n",
    "        keras.layers.LSTM(128),\n",
    "    ]\n",
    ")\n",
    "\n",
    "decoder = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.LSTM(128, return_sequences=True),\n",
    "        keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation='softmax'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [encoder, keras.layers.RepeatVector(max_output_length), decoder]\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looks great, we reach 100% validation accuracy! Let’s use the model to make some predictions. We will need to be able to convert a sequence of character IDs to a readable string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_date_strs(ids: tf.Tensor, chars: str = OUTPUT_CHARS) -> list[str]:\n",
    "    return [\n",
    "        ''.join([('?' + chars)[index] for index in sequence])\n",
    "        for sequence in ids\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we can use the model to convert some dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs(['September 17, 2009', 'July 14, 1789'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-09-17\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict(X_new).argmax(axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Perfect! :)\n",
    ">\n",
    "> However, since the model was only trained on input strings of length 18 (which is the length of the longest date), it does not perform well if we try to use it to make predictions on shorter sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs(['May 02, 2020', 'July 14, 1789'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-02\n",
      "1789-01-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict(X_new).argmax(axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Oops! We need to ensure that we always pass sequences of the same length as during training, using padding if necessary. Let’s write a little helper function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = X_train.shape[1]\n",
    "\n",
    "\n",
    "def prepare_date_strs_padded(date_strs: list[str]) -> tf.Tensor:\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < max_input_length:\n",
    "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
    "    return X\n",
    "\n",
    "\n",
    "def convert_date_strs(date_strs: list[str]) -> list[str]:\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    ids = model.predict(X).argmax(axis=-1)\n",
    "    return ids_to_date_strs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-05-02', '1789-07-14']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date_strs(['May 02, 2020', 'July 14, 1789'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cool! Granted, there are certainly much easier ways to write a date conversion tool (e.g., using regular expressions or even basic string manipulation), but we have to admit that using neural networks is way cooler. ;-)\n",
    ">\n",
    "> However, real-life sequence-to-sequence problems will usually be harder, so for the sake of completeness, let’s build a more powerful model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Second version: feeding the shifted targets to the decoder (teacher forcing)\n",
    "> Instead of feeding the decoder a simple repetition of the encoder’s output vector, we can feed it the target sequence, shifted by one time step to the right. This way, at each time step the decoder will know what the previous target character was. This should help to tackle more complex sequence-to-sequence problems.\n",
    ">\n",
    "> Since the first output character of each target sequence has no previous character, we will need a new token to represent the start-of-sequence (sos).\n",
    ">\n",
    "> During inference, we won’t know the target, so what will we feed the decoder? We can just predict one character at a time, starting with an sos token, then feeding the decoder all the characters that were predicted so far (we will look at this in more details later in this notebook).\n",
    ">\n",
    "> But if the decoder’s LSTM expects to get the previous target as input at each step, how shall we pass it the vector output by the encoder? Well, one option is to ignore the output vector, and instead use the encoder’s LSTM state as the initial state of the decoder’s LSTM (which requires that encoder’s LSTM must have the same number of units as the decoder’s LSTM).\n",
    ">\n",
    "> Now let’s create the decoder’s inputs (for training, validation and testing). The sos token will be represented using the last possible output character’s ID + 1.\n",
    ">\n",
    "> **Note**: The length of decoder’s inputs and outputs must be equal. So since we are not adding a eos token to the outputs, we cut the last token from the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "\n",
    "def shifted_output_sequences(Y: tf.Tensor) -> tf.Tensor:\n",
    "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
    "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
    "\n",
    "\n",
    "X_train_decoder = shifted_output_sequences(Y_train)\n",
    "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
    "X_test_decoder = shifted_output_sequences(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s take a look at the decoder’s training inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n",
       "array([[12,  8,  1, ..., 10, 11,  3],\n",
       "       [12,  9,  6, ...,  6, 11,  2],\n",
       "       [12,  8,  2, ...,  2, 11,  2],\n",
       "       ...,\n",
       "       [12, 10,  8, ...,  2, 11,  4],\n",
       "       [12,  2,  2, ...,  3, 11,  3],\n",
       "       [12,  8,  9, ...,  8, 11,  3]], dtype=int32)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let’s build the model. It’s not a simple sequential model anymore, so \n",
    "> let’s use the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 11s 27ms/step - loss: 1.6824 - accuracy: 0.3734 - val_loss: 1.4054 - val_accuracy: 0.4681\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 1.1935 - accuracy: 0.5550 - val_loss: 0.8868 - val_accuracy: 0.6750\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.6403 - accuracy: 0.7700 - val_loss: 0.3493 - val_accuracy: 0.8978\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.2292 - accuracy: 0.9423 - val_loss: 0.1254 - val_accuracy: 0.9782\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.0694 - accuracy: 0.9932 - val_loss: 0.0441 - val_accuracy: 0.9982\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0576 - accuracy: 0.9923 - val_loss: 0.0280 - val_accuracy: 0.9988\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.0179 - accuracy: 0.9998 - val_loss: 0.0143 - val_accuracy: 0.9999\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0107 - accuracy: 0.9999 - val_loss: 0.0092 - val_accuracy: 0.9999\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 0.9999\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "lstm_units = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(INPUT_CHARS) + 1, output_dim=encoder_embedding_size\n",
    ")(encoder_input)\n",
    "_, *encoder_state = keras.layers.LSTM(lstm_units, return_state=True)(\n",
    "    encoder_embedding\n",
    ")\n",
    "\n",
    "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(OUTPUT_CHARS) + 2, output_dim=decoder_embedding_size\n",
    ")(decoder_input)\n",
    "decoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences=True)(\n",
    "    decoder_embedding, initial_state=encoder_state\n",
    ")\n",
    "decoder_output = keras.layers.Dense(\n",
    "    len(OUTPUT_CHARS) + 1, activation='softmax'\n",
    ")(decoder_lstm_output)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[encoder_input, decoder_input], outputs=[decoder_output]\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "history = model.fit(\n",
    "    [X_train, X_train_decoder],\n",
    "    Y_train,\n",
    "    epochs=10,\n",
    "    validation_data=([X_valid, X_valid_decoder], Y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This model also reaches 100% validation accuracy, but it does so even faster. Let’s once again use the model to make some predictions. This time we need to predict characters one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_date_strs(date_strs: list[str]) -> list[str]:\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - Y_pred.shape[1]\n",
    "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
    "        Y_probas_next = model.predict([X, X_decoder])[:, index : index + 1]\n",
    "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
    "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
    "    return ids_to_date_strs(Y_pred[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs(['July 14, 1789', 'May 01, 2020'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   > Works fine! Next, feel free to write a transformer version. :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. to 11\n",
    "10. Go through the example on the Keras website for [“Natural language image search with a Dual Encoder”](https://homl.info/dualtuto). We will learn how to build a model capable of representing both images and text within the same embedding space. This makes it possible to search for images using a text prompt, like in the [CLIP model](https://openai.com/blog/clip/) by OpenAI.\n",
    "> Just click the link and follow the instructions.\n",
    "11. Use the Hugging Face Transformers library to download a pretrained language model capable of generating text (e.g., GPT), and try generating more convincing Shakespearean text. We will need to use the model’s `generate()` method. See Hugging Face’s documentation for more details.\n",
    "> First, let’s load a pretrained model. In this example, we will use OpenAI’s GPT model, with an additional language model on top (just a linear layer with weights tied to the input embeddings). Let’s import it and load the pretrained weights (this will download about 445MB of data to *~/.cache/torch/transformers*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFOpenAIGPTLMHeadModel.\n",
      "\n",
      "All the layers of TFOpenAIGPTLMHeadModel were initialized from the model checkpoint at openai-gpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFOpenAIGPTLMHeadModel\n",
    "\n",
    "model = TFOpenAIGPTLMHeadModel.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next we will need a specialized tokenizer for this model. This one will try to use the [spaCy](https://spacy.io/) and [ftfy](https://pypi.org/project/ftfy/) libraries if they are installed, or else it will fall back to BERT’s `BasicTokenizer` followed by byte-pair encoding (which should be fine for most use cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let’s use the tokenizer to tokenize and encode the prompt text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3570, 1473], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('hello everyone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187]], dtype=int32)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = 'This royal throne of kings, this sceptred isle'\n",
    "encoded_prompt = tokenizer.encode(\n",
    "    prompt_text, add_special_tokens=False, return_tensors='tf'\n",
    ")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Easy! Next, let’s use the model to generate text after the prompt. We will generate 5 different sentences, each starting with the prompt text, followed by 40 additional tokens. For an explanation of what all the hyperparameters do, make sure to check out this great [blog post](https://huggingface.co/blog/how-to-generate) by Patrick von Platen (from Hugging Face). We can play around with the hyperparameters to try to obtain better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 50), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   498,   481,   550, 12974,   554, 20275,   544,   481,\n",
       "          808,  1082,   525,   759, 13717,   507,   617,   616,  1294,\n",
       "         1276,   239, 40477,   249,  1048,  2210,   525,   249,   880,\n",
       "          694,   817,   485,   788,   507,   240,   244,   481,   762,\n",
       "         4049,  3983,  6474,  1387,   485],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   509,  1163,   485,  1272,  8660,  3380, 14760,   240,\n",
       "         1389,   557,   481,  7232,     8,   789,  3408,   239,   754,\n",
       "        10253,   558,   694,  2556,   488,  2093,   485,  2185,   917,\n",
       "           11,  5272,  6372,   562,  1272, 11413,   239, 40477,   481,\n",
       "         1583,   618,   558,   524,  1074],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   544,   597,   622,  1163,   488,   481,  1594,   498,\n",
       "          622, 11547,   267,   256,   616,   509,   885,   481,  7789,\n",
       "          498,   481,   588,  1917,   240,   984,   544,   491,   618,\n",
       "         4647,   681,   535,  4244,   239, 40477,   616,   509,   481,\n",
       "        12194,  1734,   481,   588,  1917],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   980,   246,  3128,  4321,   525,   759,   595,   580,\n",
       "        12563,   522, 15668,   239,   507,   812, 16841,  1073,   655,\n",
       "          544,   664,  3409,   500,   622,  6903,   522,   481,  1092,\n",
       "          812,  7629,   617,   481,  1988,   240,   488,   481,  4814,\n",
       "          812,   580,  7752,   498,   987],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   812,   580,   704,  3360,  4034,   485,   618,  6099,\n",
       "        33974,   239, 40477,   870,  3754,   240,   547,  3089,   239,\n",
       "        40477,   269,   269,   269, 40477,   246,  1092,  1882,   504,\n",
       "          513,  1188,  3761, 27661,   485, 10525,   239,   244,   848,\n",
       "          504,   239,   249,   825,   512]], dtype=int32)>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "generated_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let’s decode the generated sequences and print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle of the necronomicon is the only place that can unlock it from this dark world. \n",
      " i am surprised that i've been able to see it, \" the man named dallon says to\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle was home to many beloved possessors, such as the mighty astaroth. their wives had been husband and wife to lord teixiara for many generations. \n",
      " the high king had his own\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle is now our home and the land of our fathers!'this was made the standard of the coates, which is at king celebrant's command. \n",
      " this was the longest story the coates\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle has a powerful spirit that can not be severed or erased. it will reign until there is no army in our realm or the light will fade from the sky, and the lands will be stripped of its\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle will be your final gift to king dragomir. \n",
      " good luck, my guards. \n",
      " * * * \n",
      " a light touch on her arm caused aleria to jolt. \" come on. i think you\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can try more recent (and larger) models, such as GPT-2, CTRL, Transformer-XL or XLNet, which are all available as pretrained models in the transformers library, including variants with language models on top. The preprocessing steps vary slightly between models. Hope we enjoyed this chapter! :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
