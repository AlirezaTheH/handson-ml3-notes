{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 – Ensemble Learning and Random Forests\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alirezatheh/handson-ml3-notes/blob/main/notebooks/07_ensemble_learning_and_random_forests.ipynb)\n",
    "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/alirezatheh/handson-ml3-notes/blob/main/notebooks/07_ensemble_learning_and_random_forests.ipynb)\n",
    "\n",
    "Suppose we pose a complex question to thousands of random people, then aggregate their answers. In many cases we will find that this aggregated answer is better than an expert’s answer. This is called the *wisdom of the crowd*. Similarly, if we aggregate the predictions of a group of predictors, we will often get better predictions than with the best individual predictor. A group of predictors is called an *ensemble*; thus, this technique is called *ensemble learning*, and an ensemble learning algorithm is called an *ensemble method*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Voting Classifiers\n",
    "The class that gets the most votes is the ensemble’s prediction. This majority-vote classifier is called a *hard voting classifier*.\n",
    "\n",
    "How ensemble often achieves a higher accuracy than the best classifier? Suppose we have a slightly biased coin that has a 51% chance of coming up heads and 49% chance of coming up tails. If we do the math, we will find that the probability of obtaining a majority of heads after 1,000 tosses is close to 75%. The more we toss the coin, the higher the probability (e.g., with 10,000 tosses, the probability climbs over 97%). This is due to the *law of large numbers*: as we keep tossing the coin, the ratio of heads gets closer and\n",
    "closer to the probability of heads (51%).\n",
    "\n",
    "Similarly, if we build an ensemble containing 1,000 classifiers that are individually correct only 51% of the time (barely better than random guessing) and are perfectly independent, making uncorrelated errors, we can hope for up to 75% accuracy! But this is clearly not the case because they are trained on the same data.\n",
    "\n",
    "**Tip**: Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy.\n",
    "\n",
    "Let’s try it on the moons dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(random_state=42))])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit a `VotingClassifier`, it clones every estimator and fits the clones. The original estimators are available via the `estimators` attribute, while the fitted clones are available via the `estimators_` attribute. To get dict instead use `named_estimators` or `named_estimators_` instead. Let’s look at each fitted classifier’s accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.864\n",
      "rf = 0.896\n",
      "svc = 0.896\n"
     ]
    }
   ],
   "source": [
    "for name, clf in voting_clf.named_estimators_.items():\n",
    "    print(name, '=', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs hard voting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1]), array([1]), array([0])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clf.predict(X_test[:1]) for clf in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the performance of the voting classifier on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all classifiers have a `predict_proba()` method, then we can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called *soft voting*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.voting = 'soft'\n",
    "voting_clf.named_estimators['svc'].probability = True\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "Another approach is to use the same training algorithm for every predictor but train them on different random subsets of the training set. When sampling is performed *with* replacement, this method is called *bagging* (short for bootstrap aggregating, in statistics, resampling with replacement is called *bootstrapping*.) proposed in a 1996 paper [“Bagging Predictors”](https://homl.info/20) by Leo Breiman. When sampling is performed without replacement, it is called *pasting* proposed in a 1999 paper [“Pasting Small Votes for Classification in Large Databases and On-Line”](https://homl.info/21) by Leo Breiman. \n",
    "\n",
    "The aggregation function is typically the *statistical mode* for classification (hard voting), or the average for regression. Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.\n",
    "\n",
    "Training and predictions can be made in parallel. This is one of the reasons bagging and pasting are popular: they scale very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    max_samples=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Tip**: `max_samples` can also be a float between 0.0 and 1.0, in which case the max number of sampled instances is equal to the size of the training set times `max_samples`.\n",
    "\n",
    "**Tip**: If we want to use pasting instead, just set `bootstrap=False`\n",
    "\n",
    "**Note**: `BaggingClassifier` performs soft voting if the base classifier has a `predict_proba()` method.\n",
    "\n",
    "Let's compare the decision boundary of a single decision tree with the decision boundary of a bagging ensemble of 500 trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "\n",
    "def plot_decision_boundary(\n",
    "    clf: Pipeline, X: ArrayLike, y: ArrayLike, alpha: float = 1.0\n",
    ") -> None:\n",
    "    axes = [-1.5, 2.4, -1, 1.5]\n",
    "    x1, x2 = np.meshgrid(\n",
    "        np.linspace(axes[0], axes[1], 100), np.linspace(axes[2], axes[3], 100)\n",
    "    )\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3 * alpha, cmap='Wistia')\n",
    "    plt.contour(x1, x2, y_pred, cmap='Greys', alpha=0.8 * alpha)\n",
    "    colors = ['#78785c', '#c47b27']\n",
    "    markers = ('o', '^')\n",
    "    for idx in (0, 1):\n",
    "        plt.plot(\n",
    "            X[:, 0][y == idx],\n",
    "            X[:, 1][y == idx],\n",
    "            color=colors[idx],\n",
    "            marker=markers[idx],\n",
    "            linestyle='none',\n",
    "        )\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$', rotation=0)\n",
    "\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf, X_train, y_train)\n",
    "plt.title('Decision Tree')\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(bag_clf, X_train, y_train)\n",
    "plt.title('Decision Trees with Bagging')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging ends up with a slightly higher bias than pasting because it has it bit more diversity in the subsets that each predictor is trained on; but the extra diversity also means that the predictors end up being less correlated, so the ensemble’s variance is reduced. Overall, bagging often results in better models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag evaluation\n",
    "In bagging with `BaggingClassifier` by default it samples $m$ training instances with replacement (`bootstrap=True`). It can be shown mathematically that only about 63% of the training instances are sampled on average for each predictor (As $m$ grows, this ratio approaches $1–\\exp(–1)\\approx63\\%$). The remaining 37% of the training instances that are not sampled are called *out-of-bag* (OOB) instances. Note that they are not the same 37% for all predictors.\n",
    "\n",
    "A bagging ensemble can be evaluated using OOB instances, without the need for a separate validation set: if there are enough estimators, then each instance in the training set will likely be an OOB instance of several estimators, so these estimators can be used to make a fair ensemble prediction for that instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32352941, 0.67647059],\n",
       "       [0.3375    , 0.6625    ],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probas for the first 3 instances\n",
    "bag_clf.oob_decision_function_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OOB evaluation was a bit too pessimistic.\n",
    "\n",
    "If we randomly draw one instance from a dataset of size $m$, each instance in the dataset obviously has probability $1/m$ of getting picked, and therefore it has a probability $1–1/m$ of *not* getting picked. If we draw $m$ instances with replacement, all draws are independent and therefore each instance has a probability ${(1–1/m)}^m$ of *not* getting picked. Now let's use the fact that $\\exp(x)=\\lim_{m\\to\\infty}(1+x/m)^m$. So if $m$ is large, the ratio of out-of-bag instances will be about $\\exp(-1)\\approx0.37$. So roughly 63% (1 – 0.37) will be sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6323045752290363\n",
      "0.6321205588285577\n"
     ]
    }
   ],
   "source": [
    "print(1 - (1 - 1 / 1000) ** 1000)\n",
    "print(1 - np.exp(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Random Patches and Random Subspaces\n",
    "With `BaggingClassifier` we can also sample features as well by two hyperparameters: `max_features` and `bootstrap_features`. They work the same way as `max_samples` and `bootstrap`. This technique is particularly useful when we are dealing with high-dimensional inputs (such as images) and results in even more predictor diversity, trading a bit more bias for a lower variance.\n",
    "\n",
    "Sampling both training instances and features is called the *random patches* method proposed in 2012 paper [“Ensembles on Random Patches”](https://homl.info/22) by Gilles Louppe and Pierre Geurts. Keeping all training instances (by setting `bootstrap=False` and `max_samples=1.0`) but sampling features (by setting `bootstrap_features=True` and/or `max_features` to a value smaller than 1.0) is called the *random subspaces* method proposed in a 1998 paper [“The Random Subspace Method for Constructing Decision Forests”](https://homl.info/23) by Tin Kam Ho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "A random forest as proposed in a paper [“Random Decision Forests”](https://homl.info/24) by Tin Kam Ho in 1995, is an ensemble of decision trees, generally trained via the bagging method (or sometimes pasting), typically with `max_samples` set to $m$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(\n",
    "    n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42\n",
    ")\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest algorithm instead of searching for the very best feature when splitting a node, searches for the best feature among a random subset of features. By default, it samples $\\sqrt{n}$ features. The algorithm results in greater tree diversity, trades a higher bias for a lower variance.\n",
    "\n",
    "A Random Forest is equivalent to a bagging of decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_features='sqrt', max_leaf_nodes=16),\n",
    "    n_estimators=500,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred_bag = bag_clf.predict(X_test)\n",
    "np.all(y_pred_bag == y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra-Trees\n",
    "It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds. For this, simply set `splitter='random'` when creating a `DecisionTreeClassifier`.\n",
    "\n",
    "A forest of such extremely random trees is called an *extremely randomized trees* (or *extra-trees* for short) ensemble, proposed by Pierre Geurts et al. in a 2006 paper [“Extremely Randomized Trees”](https://homl.info/25). This trades more bias for a lower variance and is faster to train than regular random forests.\n",
    "\n",
    "Scikit-Learn provides `ExtraTreesClassifier` and `ExtraTreesRegressor` similar to `RandomForestClassifier` and `RandomForestRegressor`, except `bootstrap` defaults to `False`.\n",
    "\n",
    "**Tip**: It is hard to know which between a random forest and extra-trees performs better. The only way to know is to try both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Another great quality of random forests is that they easily measure the relative importance of each feature. Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average, across all trees in the forest. More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11 sepal length (cm)\n",
      "0.02 sepal width (cm)\n",
      "0.44 petal length (cm)\n",
      "0.42 petal width (cm)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rnd_clf.fit(iris.data, iris.target)\n",
    "for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n",
    "    print(round(score, 2), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a random forest classifier on the MNIST dataset and plot each pixel’s importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X_mnist, y_mnist = fetch_openml('mnist_784', return_X_y=True, as_frame=False)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rnd_clf.fit(X_mnist, y_mnist)\n",
    "\n",
    "heatmap_image = rnd_clf.feature_importances_.reshape(28, 28)\n",
    "plt.imshow(heatmap_image, cmap='hot')\n",
    "cbar = plt.colorbar(\n",
    "    ticks=[\n",
    "        rnd_clf.feature_importances_.min(),\n",
    "        rnd_clf.feature_importances_.max(),\n",
    "    ],\n",
    ")\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'], fontsize=14)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "*Boosting* (originally called *hypothesis boosting*) refers to any ensemble method that can combine several *weak* learners into a *strong* learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfit. This results in new predictors focusing more and more on the hard cases. This is the technique used by *AdaBoost* (short for *adaptive boosting*) proposed by Yoav Freund and Robert E. Schapire in 1997 paper [“A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting”](https://homl.info/26).\n",
    "\n",
    "e.g., when training an AdaBoost classifier, the algorithm first trains a base classifier (such as a decision tree) and uses it to make predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, and again makes predictions on the training set, updates the instance weights, and so on.\n",
    "\n",
    "**Warning**: With this technique training cannot be parallelized since each predictor can only be trained after the previous predictor has been trained and evaluated. As a result, it does not scale as well as bagging or pasting.\n",
    "\n",
    "Let’s take a look at the AdaBoost algorithm. Each instance weight $w^{(i)}$ is initially set to $1/m$.\n",
    "\n",
    "**Equation 7-1** Weighted error rate of the $j^{\\text{th}}$ predictor\n",
    "$$\n",
    "r_j\n",
    "=\\sum_{\\begin{gathered}i=1\\\\\\widehat{y}_j^{(i)}\\neq y^{(i)}\\end{gathered}}^m\n",
    "w^{(i)}\n",
    "$$\n",
    "- $\\widehat{y}_j^{(i)}$: The $j^{\\text{th}}$ predictor’s prediction for the $i^{\\text{th}}$ instance\n",
    "\n",
    "**Equation 7-2** Predictor weight\n",
    "$$\n",
    "\\alpha_j=\\eta\\log\\frac{1-r_j}{r_j}\n",
    "$$\n",
    "- $\\eta$: Defaults to 1.\n",
    "\n",
    "**Equation 7-3** Weight update rule\n",
    "$$\n",
    "w^{(i)}\\gets\n",
    "\\begin{cases}\n",
    "w^{(i)}&\\text{if}\\;\\widehat{y}_j^{(i)}=y^{(i)}\n",
    "\\\\w^{(i)}\\exp(\\alpha_j)&\\text{if}\\;\\widehat{y}_j^{(i)}\\neq y^{(i)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then all the instance weights are normalized (i.e., divided by $\\sum_{i=1}^mw^{(i)}$)\n",
    "\n",
    "The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found. The predicted class is the one that receives the majority of weighted votes.\n",
    "\n",
    "**Equation 7-4** AdaBoost predictions\n",
    "$$\n",
    "\\widehat{y}(x)\n",
    "=\\underset{k}{\\text{argmax}}\\;\n",
    "\\sum_{\\begin{gathered}j=1\\\\\\widehat{y}_j(x)=k\\end{gathered}}^N\\alpha_j\n",
    "$$\n",
    "- $N$: The number of predictors\n",
    "\n",
    "Scikit-Learn uses a multiclass version of AdaBoost called *SAMME* (which stands for *Stagewise Additive Modeling using a Multiclass Exponential loss function*) proposed by Ji Zhu et al. in 2009 paper [“Multi-Class AdaBoost”](https://homl.info/27). If the predictors have a `predict_proba()`, Scikit-Learn can use a variant of SAMME called *SAMME.R* (the *R* stands for “Real”), which relies on class probabilities rather than predictions and generally performs better.\n",
    "\n",
    "**Note**: A *decision stump* is a decision tree with `max_depth=1` (one root two leaf). This is the default base estimator for the `AdaBoostClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=30, random_state=42)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=30,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42,\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: If our AdaBoost ensemble is overfitting, we can try reducing the number of estimators or regularizing the base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "*Gradient boosting* was first introduced in Leo Breiman’s 1997 paper [“Arcing the Edge”](https://homl.info/arcing) and was further developed in the 1999 paper [“Greedy Function Approximation: A Gradient Boosting Machine”](https://homl.info/gradboost) by Jerome H. Friedman. It is like AdaBoost, but instead of tweaking the instance weights at every iteration, it tries to fit the new predictor to the *residual errors* made by the previous predictor.\n",
    "\n",
    "Let’s go through a simple regression example, using decision trees as the base predictors; this is called *gradient tree boosting*, or *gradient boosted regression trees* (GBRT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "# y = 3x² + Gaussian noise\n",
    "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train another decision tree regressor on the residual errors made by the previous predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=43)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=44)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make predictions on a new instance simply by adding up the predictions of all the trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49484029, 0.04021166, 0.75026781])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[-0.4], [0.0], [0.5]])\n",
    "sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try Scikit-Learn’s `GradientBoostingRegressor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(\n",
    "    max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42\n",
    ")\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The `learning_rate` hyperparameter scales the contribution of each tree. If we set it to a low value, such as 0.05, we will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called *shrinkage*.\n",
    "\n",
    "We can find the optimal number of trees by early stopping. Just set the `n_iter_no_change` hyperparameter to an integer value, say 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.05, max_depth=2, n_estimators=500,\n",
       "                          n_iter_no_change=10, random_state=42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_best = GradientBoostingRegressor(\n",
    "    max_depth=2,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=500,\n",
    "    n_iter_no_change=10,\n",
    "    random_state=42,\n",
    ")\n",
    "gbrt_best.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_best.n_estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `n_iter_no_change` is set, we can also tweak `validation_fraction` (0.1 by default) and `tol` (0.0001 by default).\n",
    "\n",
    "The `GradientBoostingRegressor` class also supports a `subsample` hyperparameter, which specifies the fraction of training instances to be used for training each tree. e.g. if `subsample=0.25`, then each tree is trained on 25% of the training instances, selected randomly. This technique trades a higher bias for a lower variance. It also speeds up training considerably. This is called *stochastic gradient boosting*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Histogram-Based Gradient Boosting\n",
    "*Histogram-based gradient boosting* (HGB) is an optimized version GBRT for large datasets. It works by binning the input features, replacing them with integers. The number of bins is controlled by the `max_bins` hyperparameter, which defaults to 255 and cannot be set any higher than this. Binning can reduce the number of possible thresholds that the training algorithm needs to evaluate. Working with integers makes it possible to use faster and more memory-efficient data structures. And bins removes the need for sorting the features when training each tree.\n",
    "\n",
    "Computational complexity: $O(b\\times m)$ instead of $O(n\\times m\\times\\log(m))$\n",
    "\n",
    "`HistGradientBoostingRegressor` and `HistGradientBoostingClassifier` are similar to `GradientBoostingRegressor` and `GradientBoostingClassifier` except:\n",
    "- Early stopping is automatically activated if the $m$ > 10,000. We can turn it always on or always off by setting the `early_stopping` to `True` or `False`.\n",
    "- Subsampling is not supported.\n",
    "- `n_estimators` is renamed to `max_iter`.\n",
    "- The only decision tree hyperparameters that can be tweaked are `max_leaf_nodes`, `min_samples_leaf`, and `max_depth`.\n",
    "\n",
    "The HGB classes support both categorical features and missing values. Let's build and train a complete pipeline for the California housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_housing_data() -> pd.DataFrame:\n",
    "    tarball_path = Path('datasets/housing.tgz')\n",
    "    if not tarball_path.is_file():\n",
    "        Path('datasets').mkdir(parents=True, exist_ok=True)\n",
    "        url = 'https://github.com/ageron/data/raw/main/housing.tgz'\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path='datasets')\n",
    "    return pd.read_csv(Path('datasets/housing/housing.csv'))\n",
    "\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "housing_labels = train_set['median_house_value']\n",
    "housing = train_set.drop('median_house_value', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('ordinalencoder',\n",
       "                                                  OrdinalEncoder(),\n",
       "                                                  ['ocean_proximity'])])),\n",
       "                ('histgradientboostingregressor',\n",
       "                 HistGradientBoostingRegressor(categorical_features=[0],\n",
       "                                               random_state=42))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "hgb_reg = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (OrdinalEncoder(), ['ocean_proximity']),\n",
    "        remainder='passthrough',\n",
    "    ),\n",
    "    HistGradientBoostingRegressor(categorical_features=[0], random_state=42),\n",
    ")\n",
    "hgb_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       10.000000\n",
       "mean     47613.307194\n",
       "std       1295.422509\n",
       "min      44963.213061\n",
       "25%      47001.233485\n",
       "50%      48000.963564\n",
       "75%      48488.093243\n",
       "max      49176.368465\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "hgb_rmses = -cross_val_score(\n",
    "    hgb_reg,\n",
    "    housing,\n",
    "    housing_labels,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=10,\n",
    ")\n",
    "pd.Series(hgb_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Tip**: Other optimized implementations of gradient boosting are available: [XGBoost](https://github.com/dmlc/xgboost), [Cat‐Boost](https://catboost.ai), [LightGBM](https://lightgbm.readthedocs.io), [TensorFlow Random Forests library](https://tensorflow.org/decision_forests)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "*Stacking* (short for *stacked generalization*) proposed by David H. Wolpert in a 1992 paper [“Stacked Generalization”](https://homl.info/29), is based on a simple idea: instead of using trivial functions to aggregate the predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation? To train the final predictor (called *blender*, or *meta learner*), we first need to build the blending training set. Use `cross_val_predict()` on every predictor in the ensemble to get out-of-sample predictions for each instance in the original training set and use these predictions as input features to train the blender; and the targets can simply be copied from the original training set. Once the blender is trained, the base predictors are retrained one last time on the full original training set. We can also add layer of blenders, and then add another blender on top of that.\n",
    "\n",
    "Let's try it on the moons dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(cv=5,\n",
       "                   estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                               ('rf', RandomForestClassifier(random_state=42)),\n",
       "                               ('svc', SVC(probability=True, random_state=42))],\n",
       "                   final_estimator=RandomForestClassifier(random_state=43))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42)),\n",
    "    ],\n",
    "    final_estimator=RandomForestClassifier(random_state=43),\n",
    "    cv=5,\n",
    ")\n",
    "stacking_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.928"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "It's a bit better than the voting classifier using soft voting (92%).\n",
    "\n",
    "If we don’t provide a final estimator, `StackingClassifier` will use `LogisticRegression` and `StackingRegressor` will use `RidgeCV`.\n",
    "\n",
    "In conclusion:\n",
    "- Random forests, AdaBoost, and GBRT should be among the first models we test for most ML tasks.\n",
    "- Ensemble methods like voting classifiers and stacking classifiers can help push our system’s performance to its limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. to 8.\n",
    "1. If we have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that we can combine these models to get better results? If so, how? If not, why?\n",
    "> We can try combining them into a voting ensemble, which will often give us even better results. It works better if the models are very different (e.g., an SVM classifier, a Decision Tree classifier, a Logistic Regression classifier, and so on). It is even better if they are trained on different training instances (that's the whole point of bagging and pasting ensembles), but if not this will still be effective as long as the models are very different.\n",
    "2. What is the difference between hard and soft voting classifiers?\n",
    "> A hard voting classifier just counts the votes of each classifier in the ensemble and picks the class that gets the most votes. A soft voting classifier computes the average estimated class probability for each class and picks the class with the highest probability. This gives high-confidence votes more weight and often performs better, but it works only if every classifier is able to estimate class probabilities (e.g., for the SVM classifiers in Scikit-Learn we must set `probability=True`).\n",
    "3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\n",
    "> Yes, since each predictor in the ensemble is independent of the others. The same goes for pasting ensembles and Random Forests, for the same reason. However, each predictor in a boosting ensemble is built based on the previous predictor, so training is necessarily sequential, and we will not gain anything by distributing training across multiple servers. Regarding stacking ensembles, all the predictors in a given layer are independent of each other, so they can be trained in parallel on multiple servers. However, the predictors in one layer can only be trained after the predictors in the previous layer have all been trained.\n",
    "4. What is the benefit of out-of-bag evaluation?\n",
    "> With out-of-bag evaluation, each predictor in a bagging ensemble is evaluated using instances that it was not trained on (they were held out). This makes it possible to have a fairly unbiased evaluation of the ensemble without the need for an additional validation set. Thus, we have more instances available for training, and our ensemble we perform slightly better.\n",
    "5. What makes extra-trees ensembles more random than regular random forests? How can this extra randomness help? Are extra-trees classifiers slower or faster than regular random forests?\n",
    "> When we are growing a tree in a Random Forest, only a random subset of the features is considered for splitting at each node. This is true as well for extra-trees, but they go one step further: rather than searching for the best possible thresholds, like regular decision trees do, they use random thresholds for each feature. This extra randomness acts like a form of regularization: if a random forest overfits the training data, extra-trees might perform better. Moreover, since extra-trees don't search for the best possible thresholds, they are much faster to train than random forests. However, they are neither faster nor slower than random forests when making predictions.\n",
    "6. If our AdaBoost ensemble underfits the training data, which hyperparameters should we tweak, and how?\n",
    "> We can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. We may also try slightly increasing the learning rate.\n",
    "7. If our gradient boosting ensemble overfits the training set, should we increase or decrease the learning rate?\n",
    "> We should try decreasing the learning rate. This means that each estimator gets less weight, so the model is less likely to overfit. We could also use early stopping to find the right number of predictors (we probably have too many).\n",
    "8. Load the MNIST dataset and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing).\n",
    "> The MNIST dataset was loaded earlier. The dataset is already split into a training set (the first 60,000 instances) and a test set (the last 10,000 instances), and the training set is already shuffled. So all we need to do is to take the first 50,000 instances for the new training set, the next 10,000 for the validation set, and the last 10,000 for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_mnist[:50_000], y_mnist[:50_000]\n",
    "X_valid, y_valid = X_mnist[50_000:60_000], y_mnist[50_000:60_000]\n",
    "X_test, y_test = X_mnist[60_000:], y_mnist[60_000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then train various classifiers, such as a random forest classifier, an \n",
    "extra-trees classifier, and an SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = LinearSVC(max_iter=100, tol=20, random_state=42)\n",
    "mlp_clf = MLPClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RandomForestClassifier(random_state=42)\n",
      "Training the ExtraTreesClassifier(random_state=42)\n",
      "Training the LinearSVC(max_iter=100, random_state=42, tol=20)\n",
      "Training the MLPClassifier(random_state=42)\n"
     ]
    }
   ],
   "source": [
    "estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\n",
    "for estimator in estimators:\n",
    "    print('Training the', estimator)\n",
    "    estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9736, 0.9743, 0.8662, 0.966]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.score(X_valid, y_valid) for estimator in estimators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The linear SVM is far outperformed by the other classifiers. However, let's keep it for now since it may improve the voting classifier's performance.\n",
    "\n",
    "Next, try to combine the classifers into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_estimators = [\n",
    "    ('random_forest_clf', random_forest_clf),\n",
    "    ('extra_trees_clf', extra_trees_clf),\n",
    "    ('svm_clf', svm_clf),\n",
    "    ('mlp_clf', mlp_clf),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(named_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('random_forest_clf',\n",
       "                              RandomForestClassifier(random_state=42)),\n",
       "                             ('extra_trees_clf',\n",
       "                              ExtraTreesClassifier(random_state=42)),\n",
       "                             ('svm_clf',\n",
       "                              LinearSVC(max_iter=100, random_state=42, tol=20)),\n",
       "                             ('mlp_clf', MLPClassifier(random_state=42))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9758"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `VotingClassifier` made a clone of each classifier, and it trained the clones using class indices as the labels, not the original class names. Therefore, to evaluate these clones we need to provide class indices as well. To convert the classes to class indices, we can use a `LabelEncoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_valid_encoded = encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However, in the case of MNIST, it's simpler to just convert the class names to integers, since the digits match the class ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_encoded = y_valid.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let's evaluate the classifier clones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9736, 0.9743, 0.8662, 0.966]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    estimator.score(X_valid, y_valid_encoded)\n",
    "    for estimator in voting_clf.estimators_\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's remove the SVM to see if performance improves. It is possible to remove an estimator by setting it to `'drop'` using `set_params()` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('random_forest_clf',\n",
       "                              RandomForestClassifier(random_state=42)),\n",
       "                             ('extra_trees_clf',\n",
       "                              ExtraTreesClassifier(random_state=42)),\n",
       "                             ('svm_clf', 'drop'),\n",
       "                             ('mlp_clf', MLPClassifier(random_state=42))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.set_params(svm_clf='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This updated the list of estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('random_forest_clf', RandomForestClassifier(random_state=42)),\n",
       " ('extra_trees_clf', ExtraTreesClassifier(random_state=42)),\n",
       " ('svm_clf', 'drop'),\n",
       " ('mlp_clf', MLPClassifier(random_state=42))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However, it did not update the list of *trained* estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(random_state=42),\n",
       " ExtraTreesClassifier(random_state=42),\n",
       " LinearSVC(max_iter=100, random_state=42, tol=20),\n",
       " MLPClassifier(random_state=42)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_forest_clf': RandomForestClassifier(random_state=42),\n",
       " 'extra_trees_clf': ExtraTreesClassifier(random_state=42),\n",
       " 'svm_clf': LinearSVC(max_iter=100, random_state=42, tol=20),\n",
       " 'mlp_clf': MLPClassifier(random_state=42)}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.named_estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So we can either fit the `VotingClassifier` again, or just remove the SVM from the list of trained estimators, both in `estimators_` and `named_estimators_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_trained = voting_clf.named_estimators_.pop('svm_clf')\n",
    "voting_clf.estimators_.remove(svm_clf_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let's evaluate the `VotingClassifier` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9769"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A bit better! The SVM was hurting performance. Now let's try using a soft voting classifier. We do not actually need to retrain the classifier, we can just set `voting` to `'soft'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.voting = 'soft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9724"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nope, hard voting wins in this case.\n",
    "\n",
    "Once we have found an ensemble that performs better than the individual predictors, try it on the test set. How much better does it perform compared to the individual classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9727"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.voting = 'hard'\n",
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.968, 0.9703, 0.965]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    estimator.score(X_test, y_test.astype(np.int64))\n",
    "    for estimator in voting_clf.estimators_\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The voting classifier reduced the error rate of the best model from about 3% to 2.7%, which means 10% less errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. \n",
    "Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image's class. Train a classifier on this new training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_predictions = np.empty((len(X_valid), len(estimators)), dtype=object)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_valid_predictions[:, index] = estimator.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['3', '3', '3', '3'],\n",
       "       ['8', '8', '8', '8'],\n",
       "       ['6', '6', '6', '6'],\n",
       "       ...,\n",
       "       ['5', '5', '5', '5'],\n",
       "       ['6', '6', '6', '6'],\n",
       "       ['8', '8', '8', '8']], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender = RandomForestClassifier(\n",
    "    n_estimators=200, oob_score=True, random_state=42\n",
    ")\n",
    "rnd_forest_blender.fit(X_valid_predictions, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We could fine-tune this blender or try other types of blenders (e.g., an `MLPClassifier`), then select the best one using cross-validation, as always.\n",
    "\n",
    "Congratulations, we have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let's evaluate the ensemble on the test set. For each image in the test set, make predictions with all our classifiers, then feed the predictions to the blender to get the ensemble's predictions. How does it compare to the voting classifier we trained earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=object)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_test_predictions[:, index] = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_forest_blender.predict(X_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9705"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This stacking ensemble does not perform as well as the voting classifier we trained earlier.\n",
    "\n",
    "Now try again using a `StackingClassifier` instead: do we get better performance? If so, why?\n",
    "> Since `StackingClassifier` uses K-Fold cross-validation, we don't need a separate validation set, so let's join the training set and the validation set into a bigger training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, y_train_full = X_mnist[:60_000], y_mnist[:60_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let's create and train the stacking classifier on the full training set:\n",
    "\n",
    "> **Warning**: the following cell will take quite a while to run (15-30 minutes depending on our hardware), as it uses K-Fold validation with 5 folds by default. It will train the 4 classifiers 5 times each on 80% of the full training set to make the predictions, plus one last time each on the full training set, and lastly it will train the final model on the predictions. That's a total of 25 models to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('random_forest_clf',\n",
       "                                RandomForestClassifier(random_state=42)),\n",
       "                               ('extra_trees_clf',\n",
       "                                ExtraTreesClassifier(random_state=42)),\n",
       "                               ('svm_clf',\n",
       "                                LinearSVC(max_iter=100, random_state=42,\n",
       "                                          tol=20)),\n",
       "                               ('mlp_clf', MLPClassifier(random_state=42))],\n",
       "                   final_estimator=RandomForestClassifier(n_estimators=200,\n",
       "                                                          oob_score=True,\n",
       "                                                          random_state=42))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_clf = StackingClassifier(\n",
    "    named_estimators, final_estimator=rnd_forest_blender\n",
    ")\n",
    "stack_clf.fit(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9784"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `StackingClassifier` significantly outperforms the custom stacking implementation we tried earlier! This is for mainly two reasons:\n",
    "> - Since we could reclaim the validation set, the `StackingClassifier` was trained on a larger dataset.\n",
    "> - It used `predict_proba()` if available, or else `decision_function()` if available, or else `predict()`. This gave the blender much more nuanced inputs to work with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "nav_menu": {
   "height": "252px",
   "width": "333px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
