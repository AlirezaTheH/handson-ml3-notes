{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 - The Machine Learning landscape\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alirezatheh/handson-ml3-notes/blob/main/notebooks/01_the_machine_learning_landscape.ipynb)\n",
    "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/alirezatheh/handson-ml3-notes/blob/main/notebooks/01_the_machine_learning_landscape.ipynb)\n",
    "\n",
    "Machine learning has been around for decades in some specialized applications, such as optical character recognition (OCR). The first machine learning application that really became mainstream in the 1990s was the *spam filter*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning\n",
    "Machine learning is the science (and art) of programming computers so they can *learn from data*.\n",
    "\n",
    "A more general definition:\n",
    "\n",
    "    [Machine learning is the] field of study that gives computers the ability to learn without being explicitly programmed.\n",
    "    —Arthur Samuel, 1959\n",
    "    \n",
    "a more engineering-oriented one:\n",
    "\n",
    "    A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n",
    "    —Tom Mitchell, 1997"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use Machine Learning\n",
    "To summarize, machine learning is great for:\n",
    "- Problems for which existing solutions require a lot of fine-tuning or long lists of rules, e.g. spam filter\n",
    "- Complex problems for which using a traditional approach yields no good solution, e.g. speech recognition\n",
    "- Fluctuating environments (a machine learning system can easily be retrained on new data, always keeping it up to date), e.g. when spammers notice\n",
    "- Getting insights about complex problems and large amounts of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Applications\n",
    "- *Analyzing images of products on a production line to automatically classify them*\n",
    "- *Detecting tumors in brain scans*\n",
    "- *Automatically classifying news articles*\n",
    "- *Automatically flagging offensive comments on discussion forums*\n",
    "- *Summarizing long documents automatically*\n",
    "- *Creating a chatbot or a personal assistant*\n",
    "- *Forecasting our company’s revenue next year, based on many performance metrics*\n",
    "- *Making our app react to voice commands*\n",
    "- *Detecting credit card fraud*\n",
    "- *Segmenting clients based on their purchases so that we can design a different marketing strategy for each segment*\n",
    "- *Representing a complex, high-dimensional dataset in a clear and insightful diagram*\n",
    "- *Recommending a product that a client may be interested in, based on past purchases*\n",
    "- *Building an intelligent bot for a game*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Machine Learning Systems\n",
    "It is useful to classify ML systems based on the following criteria:\n",
    "- How they are supervised during training\n",
    "- Whether or not they can learn incrementally on the fly\n",
    "- Whether they work by comparing new data points to known data points, or instead by detecting patterns in the training data and building a predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Training Supervision\n",
    "#### Supervised learning\n",
    "The training set we feed to the algorithm includes the desired solutions, called *labels*. e.g. classification and regression.\n",
    "\n",
    "#### Unsupervised learning\n",
    "The training data is unlabeled. e.g. clustering and association rule learning, in which the goal is to dig into large amounts of data and discover interesting relations between attributes.\n",
    "\n",
    "#### Semi-supervised learning\n",
    "Since labeling data is usually time-consuming and costly, we will often have partially labeled data, which some algorithms can deal with. Most semi-supervised learning algorithms are combinations of unsupervised and supervised algorithms.\n",
    "\n",
    "#### Self-supervised learning\n",
    "Another approach is generating a fully labeled dataset from a fully unlabeled one. e.g. randomly mask an small part of each image and train a model to recover original images.\n",
    "\n",
    "#### Reinforcement learning\n",
    "The learning system, called an *agent* in this context, can observe the environment, select and perform actions, and get *rewards* in return (or *penalties* in the form of negative rewards). It must then learn by itself what is the best strategy, called a *policy*, to get the most reward over time. A policy defines what action the agent should choose when it is in a given situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Versus Online Learning\n",
    "#### Batch learning\n",
    "The system is incapable of learning incrementally: it must be trained using all the available data. It is typically done offline and called *offline learning*. Unfortunately, a model’s performance tends to decay slowly over time, because the world continues to evolve while the model remains unchanged. This phenomenon is often called *model rot* or *data drift*.\n",
    "\n",
    "#### Online learning\n",
    "We train the system incrementally by feeding it data instances sequentially, either individually or in small groups called *mini-batches*. Online learning algorithms can be used to train models on huge datasets that cannot fit in one machine’s main memory (this is called *out-of-core learning*). Out-of-core learning is usually done offline, so *online learning* can be a confusing name. Think of it as incremental learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance-Based Versus Model-Based Learning\n",
    "One more way to categorize machine learning systems is by how they *generalize*. There are two main approaches to generalization:\n",
    "\n",
    "#### Instance-based learning\n",
    "The system learns the examples by heart, then generalizes to new cases by using a *similarity measure* to compare them to the learned examples (or a subset of them).\n",
    "\n",
    "#### Model-based learning and a typical machine learning workflow\n",
    "Another way to generalize from a set of examples is to build a model of these examples and then use that model to make *predictions*.\n",
    "\n",
    "e.g. suppose we want to know if money makes people happy, so we download the Better Life Index data from the [OECD's website](http://stats.oecd.org/index.aspx?DataSetCode=BLI) (to get the Life Satisfaction for each country) and World Bank stats about gross domestic product (GDP) per capita from [OurWorldInData.org](https://ourworldindata.org/grapher/gdp-per-capita-worldbank). Then we join the tables and sort by GDP per capita:\n",
    "\n",
    "| Country       | GDP per capita \\(USD\\) | Life satisfaction |\n",
    "|:--------------|:-----------------------|:------------------|\n",
    "| Turkey        | 28384.987785           | 5.5               |\n",
    "| Hungary       | 31007.768407           | 5.6               |\n",
    "| France        | 42025.617373           | 6.5               |\n",
    "| New Zealand   | 42404.393738           | 7.3               |\n",
    "| Australia     | 48697.837028           | 7.3               |\n",
    "| Denmark       | 55938.212809           | 7.6               |\n",
    "| United States | 60235.728492           | 6.9               |\n",
    "\n",
    "Let’s plot the data for these countries:\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/01/money_happy.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "Although the data is noisy, it looks like life satisfaction goes up more or less linearly as the country’s GDP per capita increases. So we decide to model life satisfaction as a linear function of GDP per capita. This step is called *model selection*.\n",
    "\n",
    "**Equation 1-1** A simple linear model\n",
    "$$\n",
    "\\text{life$\\_$satisfaction}=\\theta_0+\\theta_1\\times\\text{GDP$\\_$per$\\_$capita}\n",
    "$$\n",
    "\n",
    "We need to define the model's parameter values $\\theta_0$ and $\\theta_1$ and in order to know which values will make our model perform best, we need to specify a performance measure. Either a *utility function* (or *fitness function*) that measures how *good* the model is, or a *cost function* that measures how *bad* it is. For linear regression problems, people typically use a cost function.\n",
    "\n",
    "**Example 1-1** Training and running a linear model using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.30165767]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Make this notebook’s output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Download and prepare the data\n",
    "data_root = 'https://github.com/ageron/data/raw/main/'\n",
    "lifesat = pd.read_csv(data_root + 'lifesat/lifesat.csv')\n",
    "X = lifesat[['GDP per capita (USD)']].values\n",
    "y = lifesat[['Life satisfaction']].values\n",
    "\n",
    "# The next 5 lines define the default font sizes\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "# Visualize the data\n",
    "lifesat.plot(\n",
    "    kind='scatter', grid=True, x='GDP per capita (USD)', y='Life satisfaction'\n",
    ")\n",
    "plt.axis([23_500, 62_500, 4, 9])\n",
    "plt.show()\n",
    "\n",
    "# Select a linear model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make a prediction for Cyprus\n",
    "# Cyprus' GDP per capita in 2020\n",
    "X_new = [[37_655.2]]\n",
    "# outputs [[6.30165767]]\n",
    "print(model.predict(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Replacing the Linear Regression model with *k*-Nearest Neighbors (in this example, $k=3$) regression in the previous code is as simple as replacing these two lines:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "```\n",
    "\n",
    "with these two:\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.33333333]]\n"
     ]
    }
   ],
   "source": [
    "# Select a 3-Nearest Neighbors regression model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make a prediction for Cyprus\n",
    "# outputs [[6.33333333]]\n",
    "print(model.predict(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Challenges of Machine Learning\n",
    "The two things that can go wrong are “bad model” and “bad data”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insufficient Quantity of Training Data\n",
    "It takes a lot of data for most ML algorithms to work properly. Even for simple \n",
    "problems we need thousands of examples, and for complex problems such as image \n",
    "or speech recognition we may need millions of examples.\n",
    "\n",
    "<div style=\"border: 1px solid;\">\n",
    "\n",
    "#### The Unreasonable Effectiveness of Data\n",
    "In a famous paper published in 2001 [“Scaling to Very Very Large Corpora for Natural Language Disambiguation”](https://homl.info/6), Microsoft researchers Michele Banko and Eric Brill showed that very different machine learning algorithms, including fairly simple ones, performed almost identically well on a complex problem of natural language disambiguation (e.g. knowing whether to write “to”, “two”, or “too”, depending on the context) once they were given enough data.\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/01/the_unreasonable_effectiveness_of_data.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "The idea that data matters more than algorithms for complex problems was further popularized by Peter Norvig et al. in a paper titled [“The Unreasonable Effectiveness of Data”](https://homl.info/7), published in 2009.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonrepresentative Training Data\n",
    "In order to generalize well, it is crucial that our training data be representative of the new cases we want to generalize to. If the sample is too small, we will have *sampling noise* (i.e., nonrepresentative data as a result of chance), but even very large samples can be nonrepresentative if the sampling method is flawed. This is called *sampling bias*.\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/01/representative_training_data.png\"\n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poor-Quality Data\n",
    "If our training data is full of errors, outliers, and noise (e.g., due to poor-quality measurements), it will make it harder for the system to detect the underlying patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irrelevant Features\n",
    "As the saying goes: garbage in, garbage out. A critical part of the success of a machine learning project is coming up with a good set of features to train on. This process, called *feature engineering*, involves the following steps:\n",
    "- Feature selection (selecting the most useful features to train on among existing features)\n",
    "- Feature extraction (combining existing features to produce a more useful one)\n",
    "- Creating new features by gathering new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting the Training Data\n",
    "Means that the model performs well on the training data, but it does not generalize well. Overfitting happens when the model is too complex relative to the amount and noisiness of the training data. Here are possible solutions:\n",
    "- Simplify the model by selecting one with fewer parameters, by reducing the number of attributes in the training data, or by constraining the model.\n",
    "- Gather more training data.\n",
    "- Reduce the noise in the training data (e.g., fix data errors and remove outliers).\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/01/overfitting_model.png\"\n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "Constraining a model to make it simpler and reduce the risk of overfitting is called *regularization*. The amount of regularization to apply during learning can be controlled by a *hyperparameter*. A hyperparameter is a parameter of a learning algorithm (not of the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting the Training Data\n",
    "It occurs when our model is too simple to learn the underlying structure of the data.\n",
    "\n",
    "Solutions:\n",
    "- Select a more powerful model, with more parameters.\n",
    "- Feed better features to the learning algorithm (feature engineering).\n",
    "- Reduce the constraints on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Validating\n",
    "Split our data into two sets: the *training set* and the *test set*. Train the model using the training set, and test it using the test set. The error rate on new cases is called the *generalization error* (or *out-of-sample error*), and by evaluating our model on the test set, we get an estimate of this error. If the training error is low but the generalization error is high, it means that our model is overfitting the training data.\n",
    "\n",
    "**Tip**: It is common to use 80% of the data for training and *hold out* 20% for testing. But if size of the dataset is large (> 1 million) 1% for test is more than enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning and Model Selection\n",
    "If we measure generalization error multiple times on the test set, and adapt the model and hyperparameters to produce the best model for that *particular set*, the model is unlikely to perform as well on new data.\n",
    "\n",
    "A common solution to this problem is called *holdout validation*: simply hold out part of the training set to evaluate several candidate models and select the best one. The new held-out set is called the *validation set* (or the *development set*, or *dev set*).\n",
    "- Train multiple models with various hyperparameters on the reduced training set (i.e., the full training set minus the validation set).\n",
    "- Select the model that performs best on the validation set.\n",
    "- Retrain the best model on the full training set (including the validation set).\n",
    "- Estimate the generalization error on test set.\n",
    "\n",
    "If the validation set is too small, then the model evaluations will be imprecise. If the validation set is too large, then the remaining training set will be much smaller than the full training set. It is like selecting the fastest sprinter to participate in a marathon.\n",
    "\n",
    "The solution is to perform repeated *cross-validation*, using many small validation sets. Each model is evaluated once per validation set after it is trained on the rest of the data. By averaging out all the evaluations of a model, we get a much more accurate measure of its performance.\n",
    "\n",
    "There is a drawback: the training time is multiplied by the number of validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mismatch\n",
    "Sometimes we have a large amount of data downloaded from the web and nonrepresentative of the data that will be used in production. In this case it is important to keep both validation and test sets as representative as possible.\n",
    "\n",
    "But if we observe that the validation error is bad, we will not know whether this is because the model is overfiting, or it is just due to the mismatch between the validation and test sets.\n",
    "\n",
    "The solution is to hold out some of the training data in yet another set that Andrew Ng dubbed the *train-dev set*.\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/01/train_dev_set.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "After the model is trained, we can evaluate it on the train-dev set. If it performs poorly, then it must have overfit the training set. But if it performs well on the train-dev set, then we evaluate the model on the dev set. If it performs poorly, then the problem must be coming from the data mismatch. We can try to tackle this problem by preprocessing the downloaded data to make them look more like the production data, and then retraining the model.\n",
    "\n",
    "<div style=\"border: 1px solid;\">\n",
    "\n",
    "#### No Free Lunch Theorem\n",
    "In a famous 1996 paper [“The Lack of A Priori Distinctions Between Learning Algorithms”](https://homl.info/8), David Wolpert demonstrated that if we make absolutely no assumption about the data, then there is no reason to prefer one model over any other. The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice we make some reasonable assumptions about the data and evaluate only a few reasonable models.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. How would we define machine learning?\n",
    "> Machine learning is about building systems that can learn from data. Learning means getting better at some task, given some performance measure.\n",
    "2. Can we name four types of applications where it shines?\n",
    "> Machine learning is great for complex problems for which we have no  algorithmic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining).\n",
    "3. What is a labeled training set?\n",
    "> A training set that contains the desired solution (a.k.a. a label) for each instance.\n",
    "4. What are the two most common supervised tasks?\n",
    "> The two most common supervised tasks are regression and classification.\n",
    "5. Can we name four common unsupervised tasks?\n",
    "> Clustering, visualization, dimensionality reduction, and association rule learning.\n",
    "6. What type of algorithm would we use to allow a robot to walk in various unknown terrains?\n",
    "> Reinforcement learning, since this is typically the type of problem that reinforcement learning tackles. It might be possible to express the problem as a supervised or semi-supervised learning problem, but it would be less natural.\n",
    "7. What type of algorithm would we use to segment our customers into multiple groups?\n",
    "> If we don't know how to define the groups, then we can use a clustering algorithm (unsupervised learning) to segment our customers into clusters of similar customers. However, if we know what groups we would like to have, then we can feed many examples of each group to a classification algorithm (supervised learning), and it will classify all our customers into these groups.\n",
    "8. Would we frame the problem of spam detection as a supervised learning problem or an unsupervised learning problem?\n",
    "> Supervised learning: the algorithm is fed many emails along with their labels (spam or not spam).\n",
    "9. What is an online learning system?\n",
    "> An online learning system can learn incrementally, as opposed to a batch learning system. This makes it capable of adapting rapidly to both changing data and autonomous systems, and of training on very large quantities of data.\n",
    "10. What is out-of-core learning?\n",
    "> Out-of-core algorithms can handle vast quantities of data that cannot fit in a computer's main memory. An out-of-core learning algorithm chops the data into mini-batches and uses online learning techniques to learn from these mini-batches.\n",
    "11. What type of algorithm relies on a similarity measure to make predictions?\n",
    "> An instance-based learning system learns the training data by heart; then, when given a new instance, it uses a similarity measure to find the most similar learned instances and uses them to make predictions.\n",
    "12. What is the difference between a model parameter and a model hyperparameter?\n",
    "> A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply).\n",
    "13. What do model-based algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?\n",
    "> Model-based learning algorithms search for an optimal value for the model parameters such that the model will generalize well to new instances. We usually train such systems by minimizing a cost function that measures how bad the system is at making predictions on the training data, plus a penalty for model complexity if the model is regularized. To make predictions, we feed the new instance's features into the model's prediction function, using the parameter values found by the learning algorithm.\n",
    "14. Can we name four of the main challenges in machine learning?\n",
    "> The lack of data, poor data quality, nonrepresentative data, uninformative features, excessively simple models that underfit the training data, and excessively complex models that overfit the data.\n",
    "15. If our model performs great on the training data but generalizes poorly to new instances, what is happening? Can we name three possible solutions?\n",
    "> The model is likely overfitting the training data (or we got extremely lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data.\n",
    "16. What is a test set, and why would we want to use it?\n",
    "> A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production.\n",
    "17. What is the purpose of a validation set?\n",
    "> A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters.\n",
    "18. What is the train-dev set, when do we need it, and how do we use it?\n",
    "> The train-dev set is used when there is a risk of mismatch between the training data and the data used in the validation and test datasets (which should always be as close as possible to the data used once the model is in production). The train-dev set is a part of the training set that's held out (the model is not trained on it). The model is trained on the rest of the training set, and evaluated on both the train-dev set and the validation set. If the model performs well on the training set but not on the train-dev set, then the model is likely overfitting the training set If it performs well on both the training set and the train-dev set, but not on the validation set, then there is probably a significant data mismatch between the training data and the validation + test data, and we should try to improve the training data to make it look more like the validation + test data.\n",
    "19. What can go wrong if we tune hyperparameters using the test set?\n",
    "> If we tune hyperparameters using the test set, we risk overfitting the test set, and the generalization error we measure will be optimistic (we may launch a model that performs worse than we expect)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "22b0ec00cd9e253c751e6d2619fc0bb2d18ed12980de3246690d5be49479dd65"
   }
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "616px",
   "left": "0px",
   "right": "20px",
   "top": "106px",
   "width": "213px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
