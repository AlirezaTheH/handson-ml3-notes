{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19 – Training and Deploying TensorFlow Models at Scale\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alirezatheh/handson-ml3-notes/blob/main/notebooks/19_training_and_deploying_at_scale.ipynb)\n",
    "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/alirezatheh/handson-ml3-notes/blob/main/notebooks/19_training_and_deploying_at_scale.ipynb)\n",
    "\n",
    "**Note**: An *A/B experiment* consists in testing two different versions of our product on different subsets of users in order to check which version works best and get other insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving a TensorFlow Model\n",
    "As our infrastructure grows, there comes a point where it is preferable to wrap our model in a small service whose sole role is to make predictions and have the rest of the infrastructure query it (e.g., via a REST or gRPC API).\n",
    "\n",
    "This decouples our model from the rest of the infrastructure, making it possible to easily switch model versions or scale the service up as needed (independently from the rest of our infrastructure), perform A/B experiments, and ensure that all our software components rely on the same model versions. It also simplifies testing and development, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TensorFlow Serving\n",
    "Let’s deploy a trained MNIST model using Keras to TF Serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting SavedModels\n",
    "To version the model, we just need to create a subdirectory for each model version.\n",
    "\n",
    "**Note**: If running on Colab or Kaggle, we need to install the Google AI Platform client library, which will be used later in this notebook. We can ignore the warnings about version incompatibilities.\n",
    "\n",
    "**Warning**: On Colab, we must restart the Runtime after the installation, and continue with the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules or 'kaggle_secrets' in sys.modules:\n",
    "    %pip install -q -U google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTsawKlapKzy"
   },
   "source": [
    "**Warning**: This chapter discusses how to run or train a model on one or more \n",
    "GPUs, so let’s make sure there’s at least one, or else issue a warning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ekxzo6pOpKzy"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print('No GPU was detected. Neural nets can be very slow without a GPU.')\n",
    "    if 'google.colab' in sys.modules:\n",
    "        print(\n",
    "            'Go to Runtime > Change runtime and select a GPU hardware '\n",
    "            'accelerator.'\n",
    "        )\n",
    "    if 'kaggle_secrets' in sys.modules:\n",
    "        print('Go to Settings > Accelerator and select GPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s load the MNIST dataset, scale it, and split it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.7012 - accuracy: 0.8241 - val_loss: 0.3715 - val_accuracy: 0.9024\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 943us/step - loss: 0.3536 - accuracy: 0.9020 - val_loss: 0.2990 - val_accuracy: 0.9144\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 933us/step - loss: 0.3036 - accuracy: 0.9145 - val_loss: 0.2651 - val_accuracy: 0.9272\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 965us/step - loss: 0.2736 - accuracy: 0.9231 - val_loss: 0.2436 - val_accuracy: 0.9334\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 946us/step - loss: 0.2509 - accuracy: 0.9296 - val_loss: 0.2257 - val_accuracy: 0.9364\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 974us/step - loss: 0.2322 - accuracy: 0.9350 - val_loss: 0.2121 - val_accuracy: 0.9396\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 959us/step - loss: 0.2161 - accuracy: 0.9400 - val_loss: 0.1970 - val_accuracy: 0.9452\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 944us/step - loss: 0.2021 - accuracy: 0.9432 - val_loss: 0.1880 - val_accuracy: 0.9476\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 945us/step - loss: 0.1898 - accuracy: 0.9470 - val_loss: 0.1778 - val_accuracy: 0.9524\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 940us/step - loss: 0.1793 - accuracy: 0.9494 - val_loss: 0.1685 - val_accuracy: 0.9544\n",
      "INFO:tensorflow:Assets written to: my_mnist_model/0001/assets\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import keras\n",
    "\n",
    "# Load and split the MNIST dataset\n",
    "mnist = keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Build and train an MNIST model (also handles image preprocessing)\n",
    "keras.utils.set_random_seed(42)\n",
    "keras.backend.clear_session()\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n",
    "        keras.layers.Rescaling(scale=1 / 255),\n",
    "        keras.layers.Dense(100, activation='relu'),\n",
    "        keras.layers.Dense(10, activation='softmax'),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "\n",
    "model_name = 'my_mnist_model'\n",
    "model_version = '0001'\n",
    "model_path = Path(model_name) / model_version\n",
    "model.save(model_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: Since a SavedModel saves the computation graph, it can only be used with models that are based exclusively on TensorFlow operations, excluding the `tf.py_function()` operation, which wraps arbitrary Python code.\n",
    "\n",
    "Let’s take a look at the file tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_mnist_model/0001',\n",
       " 'my_mnist_model/0001/assets',\n",
       " 'my_mnist_model/0001/keras_metadata.pb',\n",
       " 'my_mnist_model/0001/saved_model.pb',\n",
       " 'my_mnist_model/0001/variables',\n",
       " 'my_mnist_model/0001/variables/variables.data-00000-of-00001',\n",
       " 'my_mnist_model/0001/variables/variables.index']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([str(path) for path in model_path.parent.glob('**/*')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow comes with a small `saved_model_cli` command-line interface to inspect SavedModels. Let’s inspect the SavedModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel contains the following tag-sets:\n",
      "'serve'\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir '{model_path}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SavedModel contains one or more *metagraphs*. A metagraph is a computation graph plus some function signature definitions, including their input and output names, types, and shapes. Each metagraph is identified by a set of tags. e.g. we may want to have a metagraph containing the full computation graph, including the training operations: we would typically tag this one as `'train'`. And we might have another metagraph containing a pruned computation graph with only the prediction operations, including some GPU-specific operations: this one might be tagged as `'serve'`, `'gpu'`. This can be done using TensorFlow’s low-level [SavedModel API](https://homl.info/savedmodel). However, when we save a Keras model using its `save()` method, it saves a single metagraph tagged as `'serve'`. Let’s inspect this `'serve'` tag set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"__saved_model_init_op\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir '{model_path}' --tag_set serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metagraph contains two signature definitions:\n",
    "- An initialization function called `'__saved_model_init_op'`, which we do not need to worry about.\n",
    "- A default serving function called `'serving_default'`. When saving a Keras model, the default serving function is the model’s `call()` method. Let’s get more details about this serving function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['flatten_input'] tensor_info:\n",
      "      dtype: DT_UINT8\n",
      "      shape: (-1, 28, 28)\n",
      "      name: serving_default_flatten_input:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['dense_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 10)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show \\\n",
    "    --dir '{model_path}' \\\n",
    "    --tag_set serve \\\n",
    "    --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These correspond to the Keras model’s input and output layer names. For even more details, we can run the following command:\n",
    "```ipython\n",
    "!saved_model_cli show --dir '{model_path}' --all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing and Starting TensorFlow Serving\n",
    "There are many ways to install TF Serving:\n",
    "- Using the system’s package manager\n",
    "- Using a Docker image\n",
    "- Installing from source\n",
    "- And more\n",
    "\n",
    "**Note**: Docker allows us to easily download a set of applications packaged in a *Docker image* (including all their dependencies and usually some good default configuration) and then run them on our system using a *Docker engine*. When we run an image, the engine creates a *Docker container* that keeps the applications well isolated from our own system, but we can give it some limited access if we want. It is similar to a virtual machine, but much faster and lighter, as the container relies directly on the host’s kernel. This means that the image does not need to include or run its own kernel.\n",
    "\n",
    "Since Colab runs on Ubuntu, we can use Ubuntu’s apt package manager like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules or 'kaggle_secrets' in sys.modules:\n",
    "    url = 'https://storage.googleapis.com/tensorflow-serving-apt'\n",
    "    src = 'stable tensorflow-model-server tensorflow-model-server-universal'\n",
    "    !echo 'deb {url} {src}' > /etc/apt/sources.list.d/tensorflow-serving.list\n",
    "    !curl '{url}/tensorflow-serving.release.pub.gpg' | apt-key add -\n",
    "    !apt update -q && apt-get install -y tensorflow-model-server\n",
    "    %pip install -q -U tensorflow-serving-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code:\n",
    "- Starts by adding TensorFlow’s package repository to Ubuntu’s list of package sources. \n",
    "- Then it downloads TensorFlow’s public GPG key and adds it to the package manager’s key list so it can verify TensorFlow’s package \n",
    "signatures.\n",
    "- Next, it uses apt to install the `tensorflow-model-server` package. \n",
    "- Lastly, it installs the `tensorflow-serving-api` library, which we will need to communicate with the server.\n",
    "\n",
    "The following 2 cells will start the server. If our OS is Windows, we may need to run the `tensorflow_model_server` command in a terminal, and replace `${MODEL_DIR}` with the full path to the `my_mnist_model` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MODEL_DIR'] = str(model_path.parent.absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "tensorflow_model_server \\\n",
    "    --port=8500 \\\n",
    "    --rest_api_port=8501 \\\n",
    "    --model_name=my_mnist_model \\\n",
    "    --model_base_path='${MODEL_DIR}' >my_server.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Jupyter or Colab, the `%%bash --bg` magic command executes the cell as a bash script, running it in the background.\n",
    "- The `>my_server.log 2>&1` part redirects the standard output and standard error to the my_server.log file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid;\">\n",
    "\n",
    "##### Running TF Serving in a Docker Container \n",
    "If we are running this notebook on our own machine, and we prefer to install TF Serving using Docker, first make sure [Docker](https://docs.docker.com/install/) is installed, then run the following commands in a terminal.\n",
    "```bash\n",
    "# Downloads the latest TF Serving image\n",
    "docker pull tensorflow/serving\n",
    "\n",
    "docker run -it --rm -v '/path/to/my_mnist_model:/models/my_mnist_model' \\\n",
    "    -p 8500:8500 -p 8501:8501 -e MODEL_NAME=my_mnist_model tensorflow/serving\n",
    "```\n",
    "Here is what all these command-line options mean:\n",
    "- `-it`: Makes the container interactive (so we can press Ctrl-C to stop it) and displays the server’s output\n",
    "- `--rm`: Deletes the container when we stop it: no need to clutter our machine with interrupted containers. However, it does not delete the image.\n",
    "- `-v '/path/to/my_mnist_model:/models/my_mnist_model'`: Makes the host’s *my_mnist_model* directory available to the container at the path */models/mnist_model*. We must replace */path/to/my_mnist_model* with the absolute path of this directory. On Windows, remember to use \\ instead of / in the host path, but not in the container path (since the container runs on Linux).\n",
    "- `-p 8500:8500`: Makes the Docker engine forward the host’s TCP port 8500 to the container’s TCP port 8500. By default, TF Serving uses this port to serve the gRPC API.\n",
    "- `-p 8501:8501`: Forwards the host’s TCP port 8501 to the container’s TCP port 8501. The Docker image is configured to use this port by default to serve the REST API.\n",
    "- `-e MODEL_NAME=my_mnist_model`: Sets the container’s `MODEL_NAME` environment variable, so TF Serving knows which model to serve. By default, it will look for models in the */models* directory, and it will automatically serve the latest version it finds.\n",
    "- `tensorflow/serving`: This is the name of the image to run.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying TF Serving through the REST API\n",
    "Let’s start by creating the query. It must contain the name of the function signature we want to call, and of course the input data. Since the request must use the JSON format, we have to convert the input images from a NumPy array to a Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Pretend we have 3 new digit images to classify\n",
    "X_new = X_test[:3]\n",
    "request_json = json.dumps(\n",
    "    {\n",
    "        'signature_name': 'serving_default',\n",
    "        'instances': X_new.tolist(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"signature_name\": \"serving_default\", \"instances\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0..., 0, 0]]]}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_json[:100] + '...' + request_json[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s use TensorFlow Serving’s REST API to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "server_url = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
    "response = requests.post(server_url, data=request_json)\n",
    "# Raise an exception in case of error\n",
    "response.raise_for_status()\n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_proba = np.array(response['predictions'])\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: When transferring large amounts of data, or when latency is important, it is much better to use the gRPC API, if the client supports it, as it uses a compact binary format and an efficient communication protocol based on HTTP/2 framing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying TF Serving through the gRPC API\n",
    "The gRPC API expects a serialized `PredictRequest` protocol buffer as input, and it outputs a serialized `PredictResponse` protocol buffer. These protobufs are part of the `tensorflow-serving-api` library. First, let’s create the request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
    "\n",
    "request = PredictRequest()\n",
    "request.model_spec.name = model_name\n",
    "request.model_spec.signature_name = 'serving_default'\n",
    "# == 'flatten_input'\n",
    "input_name = model.input_names[0]\n",
    "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "# No encryption, no authentication\n",
    "channel = grpc.insecure_channel('localhost:8500')\n",
    "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "response = predict_service.Predict(request, timeout=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the response to a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# == 'dense_1'\n",
    "output_name = model.output_names[0]\n",
    "outputs_proto = response.outputs[output_name]\n",
    "y_proba = tf.make_ndarray(outputs_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our client does not include the TensorFlow library, we can convert the response to a NumPy array like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows how to avoid using tf.make_ndarray()\n",
    "output_name = model.output_names[0]\n",
    "outputs_proto = response.outputs[output_name]\n",
    "shape = [dim.size for dim in outputs_proto.tensor_shape.dim]\n",
    "y_proba = np.array(outputs_proto.float_val).reshape(shape)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying a new model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 931us/step - loss: 0.7039 - accuracy: 0.8056 - val_loss: 0.3418 - val_accuracy: 0.9042\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 1s 855us/step - loss: 0.3204 - accuracy: 0.9082 - val_loss: 0.2674 - val_accuracy: 0.9242\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 883us/step - loss: 0.2650 - accuracy: 0.9235 - val_loss: 0.2227 - val_accuracy: 0.9368\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 1s 869us/step - loss: 0.2319 - accuracy: 0.9329 - val_loss: 0.2032 - val_accuracy: 0.9432\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 1s 870us/step - loss: 0.2089 - accuracy: 0.9399 - val_loss: 0.1833 - val_accuracy: 0.9482\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 1s 871us/step - loss: 0.1908 - accuracy: 0.9446 - val_loss: 0.1740 - val_accuracy: 0.9498\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 873us/step - loss: 0.1756 - accuracy: 0.9490 - val_loss: 0.1605 - val_accuracy: 0.9540\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 877us/step - loss: 0.1631 - accuracy: 0.9524 - val_loss: 0.1543 - val_accuracy: 0.9558\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 879us/step - loss: 0.1517 - accuracy: 0.9567 - val_loss: 0.1460 - val_accuracy: 0.9570\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 1s 872us/step - loss: 0.1429 - accuracy: 0.9584 - val_loss: 0.1358 - val_accuracy: 0.9618\n"
     ]
    }
   ],
   "source": [
    "# Build and train a new MNIST model version\n",
    "keras.utils.set_random_seed(42)\n",
    "keras.utils.set_random_seed(42)\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n",
    "        keras.layers.Rescaling(scale=1 / 255),\n",
    "        keras.layers.Dense(50, activation='relu'),\n",
    "        keras.layers.Dense(50, activation='relu'),\n",
    "        keras.layers.Dense(10, activation='softmax'),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0002/assets\n"
     ]
    }
   ],
   "source": [
    "model_version = '0002'\n",
    "model_path = Path(model_name) / model_version\n",
    "model.save(model_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the file tree again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_mnist_model/0001',\n",
       " 'my_mnist_model/0001/assets',\n",
       " 'my_mnist_model/0001/keras_metadata.pb',\n",
       " 'my_mnist_model/0001/saved_model.pb',\n",
       " 'my_mnist_model/0001/variables',\n",
       " 'my_mnist_model/0001/variables/variables.data-00000-of-00001',\n",
       " 'my_mnist_model/0001/variables/variables.index',\n",
       " 'my_mnist_model/0002',\n",
       " 'my_mnist_model/0002/assets',\n",
       " 'my_mnist_model/0002/keras_metadata.pb',\n",
       " 'my_mnist_model/0002/saved_model.pb',\n",
       " 'my_mnist_model/0002/variables',\n",
       " 'my_mnist_model/0002/variables/variables.data-00000-of-00001',\n",
       " 'my_mnist_model/0002/variables/variables.index']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([str(path) for path in model_path.parent.glob('**/*')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At regular intervals (the delay is configurable), TF Serving checks the model directory for new model versions. If it finds one, it automatically handles the transition gracefully: by default, it answers pending requests (if any) with the previous model version, while handling new requests with the new version. As soon as every pending request has been answered, the previous model version is unloaded. We can see this at work in the TF Serving logs (in *my_server.log*):\n",
    "```text\n",
    "[...]\n",
    "Reading SavedModel from: /models/my_mnist_model/0002\n",
    "Reading meta graph with tags { serve }\n",
    "[...]\n",
    "Successfully loaded servable version {name: my_mnist_model version: 2}\n",
    "Quiescing servable version {name: my_mnist_model version: 1}\n",
    "Done quiescing servable version {name: my_mnist_model version: 1}\n",
    "Unloading servable version {name: my_mnist_model version: 1}\n",
    "```\n",
    "\n",
    "**Warning**: We may need to wait a minute before the new model is loaded by TensorFlow Serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_url = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
    "\n",
    "response = requests.post(server_url, data=request_json)\n",
    "response.raise_for_status()\n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['predictions'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = np.array(response['predictions'])\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: If the SavedModel contains some example instances in the *assets/extra* directory, we can configure TF Serving to run the new model on these instances before starting to use it to serve requests. This is called *model warmup*: it will ensure that everything is properly loaded, avoiding long response times for the first requests.\n",
    "\n",
    "This approach offers a smooth transition, but it may use too much RAM. We can configure TF Serving so that it handles all pending requests with the previous model version and unloads it before loading and using the new model version.\n",
    "\n",
    "If we discover that version 2 does not work as well as we expected, then rolling back to version 1 is as simple as removing the *my_mnist_model/0002* directory.\n",
    "\n",
    "**Tip**: We can activate automatic batching using the `--enable_batching` option upon startup. When TF Serving receives multiple requests within a short period of time (the delay is configurable), it will automatically batch them together before using the model. This offers a significant performance boost by leveraging the power of the GPU. Once the model returns the predictions, TF Serving dispatches each prediction to the right client. We can trade a bit of latency for a greater throughput by increasing the batching delay (see the `--batching_parameters_file` option).\n",
    "\n",
    "If we expect to get many queries per second, we will want to deploy TF Serving on multiple servers and load-balance the queries. This will require deploying and managing many TF Serving containers across these servers. One way to handle that is to use a tool such as [Kubernetes](https://kubernetes.io), which is an open source system for simplifying container orchestration across many servers. If we do not want to purchase, maintain, and upgrade all the hardware infrastructure, we will want to use virtual machines on a cloud platform such as Amazon AWS, Microsoft Azure, Google Cloud Platform, IBM Cloud, Alibaba Cloud, Oracle Cloud, or some other platform as a service (PaaS) offering. \n",
    "\n",
    "All of this can be a full-time job, but some service providers can take care of all this for us. In this chapter we will use Vertex AI: it’s the only platform with TPUs today; it supports TensorFlow 2, Scikit-Learn, and XGBoost; and it offers a nice suite of AI services. There are other providers such as Amazon AWS SageMaker and Microsoft AI Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Prediction Service on Vertex AI\n",
    "Vertex AI is a platform within Google Cloud Platform (GCP) that offers a wide range of AI-related tools and services:\n",
    "- We can upload datasets, get humans to label them, store commonly used features in a feature store and use them for training or in production, and train models across many GPU or TPU servers with automatic hyperparameter tuning or model architecture search (AutoML).\n",
    "- We can also manage our trained models, use them to make batch predictions on large amounts of data, schedule multiple jobs for our data workflows, serve our models via REST or gRPC at scale, and experiment with our data and models within a hosted Jupyter environment called the *Workbench*.\n",
    "- There’s even a *Matching Engine* service that lets us compare vectors very efficiently (i.e., approximate nearest neighbors).\n",
    "\n",
    "Before we start, there’s a little bit of setup to take care of:\n",
    "1. Log in to our Google account, and then go to the [Google Cloud Platform \n",
    "console](https://console.cloud.google.com).\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/19/gcp_console.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "2. If it’s our first time using GCP, we’ll have to read and accept the terms and conditions. New users are offered a free trial, including $300 worth of GCP credit that we can use over the course of 90 days.\n",
    "3. If we have used GCP before and our free trial has expired, then the services we will use in this chapter will cost us some money. To check, open the ☰ navigation menu at the top left and click Billing, then make sure we have set up a payment method and that the billing account is active.\n",
    "4. Every resource in GCP belongs to a *project*. This includes all the VMs we may use, the files we store, and the training jobs we run. \n",
    "   - When we create an account, GCP automatically creates a project for us, called “My First Project”. If we want, we can change its display name by going to the project settings: in the ☰ navigation menu, select “IAM and admin → Settings”, change the project’s display name, and click SAVE. \n",
    "   - Note that the project also has a unique ID and number. We can choose the project ID when we create a project, but we cannot change it later. The project number is automatically generated and cannot be changed.\n",
    "   - If we want to create a new project, click the project name at the top of the page, then click NEW PROJECT and enter the project name. We can also click EDIT to set the project ID. Make sure billing is active for this new project so that service fees can be billed (to our free credits, if any).\n",
    "5. Now we must activate the APIs we need. In the ☰ navigation menu, select “APIs and services”, and make sure the Cloud Storage API is enabled. If needed, click + ENABLE APIS AND SERVICES, find Cloud Storage, and enable it. Also enable the Vertex AI API.\n",
    "\n",
    "**Warning**: We should always set an alarm to remind ourselves to turn services off when we know we will only need them for a few hours, or else we might leave them running for days or months, incurring potentially significant costs.\n",
    "\n",
    "We could continue to do everything via the GCP console, but I recommend using Python instead: this way we can write scripts to automate just about anything we want with GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid;\">\n",
    "\n",
    "#### Google Cloud CLI and Shell\n",
    "Google Cloud’s command-line interface (CLI) includes the `gcloud` command, which lets us control almost everything in GCP, and `gsutil`, which lets us interact with Google Cloud Storage. This CLI is preinstalled in Colab: all we need to do is authenticate using `google.auth.authenticate_user()`. e.g. `!gcloud config list` will display the configuration.\n",
    "\n",
    "GCP also offers a preconfigured shell environment called the Google Cloud Shell, which we can use directly in our web browser; it runs on a free Linux VM (Debian) with the Google Cloud SDK already preinstalled and configured for us, so there’s no need to authenticate.\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/19/gc_shell.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "We can also [install the CLI on our machine](https://homl.info/gcloud).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you need to do before we can use any GCP service is to authenticate. The simplest solution when using Colab is to execute the following code:\n",
    "```python\n",
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "```\n",
    "The authentication process is based on [OAuth 2.0](https://oauth.net): a pop-up window will ask us to confirm that we want the Colab notebook to access our Google credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid;\">\n",
    "\n",
    "#### Authentication and Authorization on GCP\n",
    "In general, using OAuth 2.0 authentication is only recommended when an application must access the user’s personal data or resources from another application, on the user’s behalf.\n",
    "\n",
    "When an application needs to access a service on GCP on its own behalf, then it should generally use a *service account*. To create a service account:\n",
    "- Select “IAM and admin → Service accounts” in the GCP console’s ☰ navigation menu (or use the search box), then click + CREATE SERVICE ACCOUNT, fill in the first page of the form (service account name, ID, description), and click CREATE AND CONTINUE.\n",
    "- Next, we must give this account some access rights. Select the “Vertex AI user” role: this will allow the service account to make predictions and use other Vertex AI services, but nothing else. Click CONTINUE. \n",
    "- We can now optionally grant some users access to the service account: this is useful when our GCP user account is part of an organization and we wish to authorize other users in the organization to:\n",
    "  - Deploy applications that will be based on this service account;\n",
    "  - Or to manage the service account itself. \n",
    "  \n",
    "  Next, click DONE.\n",
    "\n",
    "Once we have created a service account, our application must authenticate as that service account. There are several ways to do that:\n",
    "- If our application is hosted on GCP, then the simplest and safest solution is to attach the service account to the GCP resource that hosts our website, such as a VM instance or a Google App Engine service. This can be done when creating the GCP resource, by selecting the service account in the “Identity and API access” section. Some resources, such as VM instances, also let us attach the service account after the VM instance is created: we must stop it and edit its settings. In any case, once a service account is attached to a VM instance, or any other GCP resource running our code, GCP’s client libraries will automatically authenticate as the chosen service account, with no extra step needed.\n",
    "- If our application is hosted using Kubernetes, then we should use Google’s Work‐load Identity service to map the right service account to each Kubernetes service account.\n",
    "- If our application is not hosted on GCP, then we can either use the Workload Identity Federation service (that’s the safest but hardest option), or just generate an access key for our service account, save it to a JSON file, and point the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to it so our client application can access it. We can manage access keys by clicking the service account we just created, and then opening the KEYS tab. \n",
    "\n",
    "For more details on setting up authentication and authorization so our application can access GCP services, check out the [documentation](https://homl.info/gcpauth).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are not running this notebook in Colab, after following the instructions explained we must create a service account and generate a key for it, download it to this notebook’s directory, and name it `my_service_account_key.json` (or make sure the `GOOGLE_APPLICATION_CREDENTIALS` environment variable points to our key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS TO OUR PROJECT ID\n",
    "project_id = 'my_project'\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()\n",
    "elif 'kaggle_secrets' in sys.modules:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "    UserSecretsClient().set_gcloud_credentials(project=project_id)\n",
    "else:\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = (\n",
    "        'my_service_account_key.json'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s create a Google Cloud Storage bucket to store our SavedModels (a GCS *bucket* is a container for our data). We first create a `Client` object, which will serve as the interface with GCS, then we use it to create the bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# CHANGE THIS TO A UNIQUE BUCKET NAME\n",
    "bucket_name = 'my_bucket'\n",
    "location = 'us-central1'\n",
    "\n",
    "storage_client = storage.Client(project=project_id)\n",
    "bucket = storage_client.create_bucket(bucket_name, location=location)\n",
    "# To reuse a bucket instead\n",
    "# bucket = storage_client.bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GCS uses a single worldwide namespace for buckets, so simple names like “machine-learning” will most likely not be available. Make sure the bucket name conforms to DNS naming conventions, as it may be used in DNS records. Moreover, bucket names are public, so do not put anything private in the name. It is common to use our domain name, our company name, or our project ID as a prefix to ensure uniqueness, or simply use a random number as part of the name.\n",
    "- We can change the region if we want, see [Google Cloud’s list of regions](https://homl.info/regions) and [Vertex AI’s documentation on locations](https://homl.info/locations) for more details.\n",
    "\n",
    "Next, let’s upload the *my_mnist_model* directory to the new bucket. Files in GCS are called *blobs* (or *objects*), and under the hood they are all just placed in the bucket without any directory structure. Blob names can be arbitrary Unicode strings, and they can even contain forward slashes (/). The GCP console and other tools use these slashes to give the illusion that there are directories. So, when we upload the *my_mnist_model* directory, we only care about the files, not the directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_directory(bucket: storage.Bucket, dirpath: str) -> None:\n",
    "    dirpath = Path(dirpath)\n",
    "    for filepath in dirpath.glob('**/*'):\n",
    "        if filepath.is_file():\n",
    "            blob = bucket.blob(filepath.relative_to(dirpath.parent).as_posix())\n",
    "            blob.upload_from_filename(filepath)\n",
    "\n",
    "\n",
    "upload_directory(bucket, 'my_mnist_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function works fine now, but it would be very slow if there were many files to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A much faster multithreaded implementation of upload_directory() which\n",
    "# also accepts a prefix for the target path, and prints stuff\n",
    "\n",
    "from concurrent import futures\n",
    "\n",
    "\n",
    "def upload_file(\n",
    "    bucket: storage.Bucket, filepath: Path, blob_path: str\n",
    ") -> None:\n",
    "    blob = bucket.blob(blob_path)\n",
    "    blob.upload_from_filename(filepath)\n",
    "\n",
    "\n",
    "def upload_directory(\n",
    "    bucket: storage.Bucket,\n",
    "    dirpath: str,\n",
    "    prefix: str = None,\n",
    "    max_workers: int = 50,\n",
    ") -> None:\n",
    "    dirpath = Path(dirpath)\n",
    "    prefix = prefix or dirpath.name\n",
    "    with futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_filepath = {\n",
    "            executor.submit(\n",
    "                upload_file,\n",
    "                bucket,\n",
    "                filepath,\n",
    "                f'{prefix}/{filepath.relative_to(dirpath).as_posix()}',\n",
    "            ): filepath\n",
    "            for filepath in sorted(dirpath.glob('**/*'))\n",
    "            if filepath.is_file()\n",
    "        }\n",
    "        for future in futures.as_completed(future_to_filepath):\n",
    "            filepath = future_to_filepath[future]\n",
    "            try:\n",
    "                _ = future.result()\n",
    "            except Exception as ex:\n",
    "                # f!s is str(f)\n",
    "                print(f'Error uploading {filepath!s:60}: {ex}')\n",
    "            else:\n",
    "                print(f'Uploaded {filepath!s:60}', end='\\r')\n",
    "\n",
    "    print(f'Uploaded {dirpath!s:60}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we installed Google Cloud CLI (it’s preinstalled on Colab), then we can use the following `gsutil` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp -r my_mnist_model gs://{bucket_name}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s tell Vertex AI about our MNIST model. \n",
    "- First we initialize the library, just to specify some default values for the project ID and the location.\n",
    "- Then we can create a new Vertex AI model: we specify a display name, the GCS path to our model (in this case the version 0001), and the URL of the Docker container we want Vertex AI to use to run this model. If we visit that URL and navigate up one level, we will find other containers we can use. This one supports TensorFlow 2.8 with a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/522977795627/locations/us-central1/models/4798114811986575360/operations/53403898236370944\n",
      "Model created. Resource name: projects/522977795627/locations/us-central1/models/4798114811986575360\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/522977795627/locations/us-central1/models/4798114811986575360')\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "server_image = 'gcr.io/cloud-aiplatform/prediction/tf2-gpu.2-8:latest'\n",
    "\n",
    "aiplatform.init(project=project_id, location=location)\n",
    "mnist_model = aiplatform.Model.upload(\n",
    "    display_name='mnist',\n",
    "    artifact_uri=f'gs://{bucket_name}/my_mnist_model/0001',\n",
    "    serving_container_image_uri=server_image,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s deploy this model so we can query it via a gRPC or REST API to make predictions:\n",
    "- For this we first need to create an *endpoint*. This is what client applications connect to when they want to access a service.\n",
    "- Then we need to deploy our model to this endpoint:\n",
    "\n",
    "**Warning**: This cell may take several minutes to run, as it waits for Vertex AI to provision the compute nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/522977795627/locations/us-central1/endpoints/5133373499481522176/operations/4135354010494304256\n",
      "Endpoint created. Resource name: projects/522977795627/locations/us-central1/endpoints/5133373499481522176\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/522977795627/locations/us-central1/endpoints/5133373499481522176')\n",
      "Deploying Model projects/522977795627/locations/us-central1/models/4798114811986575360 to Endpoint : projects/522977795627/locations/us-central1/endpoints/5133373499481522176\n",
      "Deploy Endpoint model backing LRO: projects/522977795627/locations/us-central1/endpoints/5133373499481522176/operations/388359120522051584\n",
      "Endpoint model deployed. Resource name: projects/522977795627/locations/us-central1/endpoints/5133373499481522176\n"
     ]
    }
   ],
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name='mnist-endpoint')\n",
    "\n",
    "endpoint.deploy(\n",
    "    mnist_model,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=5,\n",
    "    machine_type='n1-standard-4',\n",
    "    accelerator_type='NVIDIA_TESLA_K80',\n",
    "    accelerator_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://homl.info/machinetypes for other machine types and https://homl.info/accelerators for other GPU types.\n",
    "\n",
    "**Note**: GCP enforces various GPU quotas, both world‐wide and per region: we cannot create thousands of GPU nodes without prior authorization from Google. To check our quotas, open “IAM and admin → Quotas” in the GCP console. If some quotas are too low (e.g., if we need more GPUs in a particular region), we can ask for them to be increased; it often takes about 48 hours.\n",
    "\n",
    "Vertex AI will initially spawn the minimum number of compute nodes (just one in this case), and whenever the number of queries per second becomes too high, it will spawn more nodes (up to the maximum number we defined, five in this case) and will load-balance the queries between them. If the QPS rate goes down for a while, Vertex AI will stop the extra compute nodes automatically. The cost is therefore directly linked to the load, as well as the machine and accelerator types we selected and the amount of data we store on GCS. This pricing model is great for occasional users and for services with important usage spikes. It’s also ideal for startups: the price remains low until the startup actually starts up.\n",
    "\n",
    "Now let’s query this prediction service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = endpoint.predict(instances=X_new.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.round(response.predictions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/522977795627/locations/us-central1/endpoints/5133373499481522176\n",
      "Undeploy Endpoint model backing LRO: projects/522977795627/locations/us-central1/endpoints/5133373499481522176/operations/3579722406467469312\n",
      "Endpoint model undeployed. Resource name: projects/522977795627/locations/us-central1/endpoints/5133373499481522176\n",
      "Deleting Endpoint : projects/522977795627/locations/us-central1/endpoints/5133373499481522176\n",
      "Delete Endpoint  backing LRO: projects/522977795627/locations/us-central1/operations/4738836360561950720\n",
      "Endpoint deleted. . Resource name: projects/522977795627/locations/us-central1/endpoints/5133373499481522176\n"
     ]
    }
   ],
   "source": [
    "# Undeploy all models from the endpoint\n",
    "endpoint.undeploy_all()\n",
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Batch Prediction Jobs on Vertex AI\n",
    "If we have a large number of predictions to make, then instead of calling our prediction service repeatedly, we can ask Vertex AI to run a prediction job for us. This does not require an endpoint, only a model.\n",
    "\n",
    "One way to do this is to create a file containing one instance per line, each formatted as a JSON value, this format is called *JSON Lines*, then pass this file to Vertex AI. So let’s create a JSON Lines file in a new directory, then upload this directory to GCS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded my_mnist_batch                                              \n"
     ]
    }
   ],
   "source": [
    "batch_path = Path('my_mnist_batch')\n",
    "batch_path.mkdir(exist_ok=True)\n",
    "with open(batch_path / 'my_mnist_batch.jsonl', 'w') as jsonl_file:\n",
    "    for image in X_test[:100].tolist():\n",
    "        jsonl_file.write(json.dumps(image))\n",
    "        jsonl_file.write('\\n')\n",
    "\n",
    "upload_directory(bucket, batch_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s launch the prediction job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BatchPredictionJob\n",
      "BatchPredictionJob created. Resource name: projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544\n",
      "To use this BatchPredictionJob in another session:\n",
      "bpj = aiplatform.BatchPredictionJob('projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/4346926367237996544?project=522977795627\n",
      "BatchPredictionJob projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "BatchPredictionJob projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "BatchPredictionJob run completed. Resource name: projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544\n"
     ]
    }
   ],
   "source": [
    "batch_prediction_job = mnist_model.batch_predict(\n",
    "    job_display_name='my_batch_prediction_job',\n",
    "    machine_type='n1-standard-4',\n",
    "    starting_replica_count=1,\n",
    "    max_replica_count=5,\n",
    "    accelerator_type='NVIDIA_TESLA_K80',\n",
    "    accelerator_count=1,\n",
    "    gcs_source=[f'gs://{bucket_name}/{batch_path.name}/my_mnist_batch.jsonl'],\n",
    "    gcs_destination_prefix=f'gs://{bucket_name}/my_mnist_predictions/',\n",
    "    # Set to False if we don’t want to wait for completion\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: For large batches, we can split the inputs into multiple JSON Lines files and list them all via the `gcs_source` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gcs_output_directory: \"gs://my_bucket/my_mnist_predictions/prediction-mnist-2022_04_12T21_30_08_071Z\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the output directory\n",
    "batch_prediction_job.output_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions will be available in a set of files named something like *prediction.results-00001-of-00002*. These files use the JSON Lines format by default, and each value is a dictionary containing an instance and its corresponding prediction (i.e., 10 probabilities). The instances are listed in the same order as the inputs. The job also outputs *prediction-errors\\** files, which can be useful for debugging if something goes wrong. We can iterate through all these output files using `batch_prediction_job.iter_outputs()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_mnist_predictions/prediction-mnist-2022_04_12T21_30_08_071Z/prediction.errors_stats-00000-of-00001\n",
      "my_mnist_predictions/prediction-mnist-2022_04_12T21_30_08_071Z/prediction.results-00000-of-00002\n",
      "my_mnist_predictions/prediction-mnist-2022_04_12T21_30_08_071Z/prediction.results-00001-of-00002\n"
     ]
    }
   ],
   "source": [
    "y_probas = []\n",
    "for blob in batch_prediction_job.iter_outputs():\n",
    "    print(blob.name)\n",
    "    if 'prediction.results' in blob.name:\n",
    "        for line in blob.download_as_text().splitlines():\n",
    "            y_proba = json.loads(line)['prediction']\n",
    "            y_probas.append(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_probas, axis=1)\n",
    "accuracy = np.sum(y_pred == y_test[:100]) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Model : projects/522977795627/locations/us-central1/models/4798114811986575360\n",
      "Delete Model  backing LRO: projects/522977795627/locations/us-central1/operations/598902403101622272\n",
      "Model deleted. . Resource name: projects/522977795627/locations/us-central1/models/4798114811986575360\n"
     ]
    }
   ],
   "source": [
    "mnist_model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON Lines format is the default, but when dealing with large instances such as images, it is too verbose. The `batch_predict()` method accepts an `instances_format` argument that lets us choose another format if we want. It defaults to `'jsonl'`, but we can change it to `'csv'`, `'tf-record'`, `'tf-record-gzip'`, `'bigquery'`, or `'file-list'`. If we set it to `'file-list'`, then the `gcs_source` argument should point to a text file containing one input filepath per line; for instance, pointing to PNG image files. Vertex AI will read these files as binary, encode them using Base64, and pass the resulting byte strings to the model. This means that we must add a preprocessing layer in our model to parse the Base64 strings, using `tf.io.decode_base64()`. If the files are images, we must then parse the result using a function like `tf.io.decode_image()` or `tf.io.decode_png()`.\n",
    "\n",
    "Let’s delete all the directories we created on GCS (i.e., all the blobs with these prefixes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting BatchPredictionJob : projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544\n",
      "Delete BatchPredictionJob  backing LRO: projects/522977795627/locations/us-central1/operations/6699028098374959104\n",
      "BatchPredictionJob deleted. . Resource name: projects/522977795627/locations/us-central1/batchPredictionJobs/4346926367237996544\n"
     ]
    }
   ],
   "source": [
    "for prefix in ['my_mnist_model/', 'my_mnist_batch/', 'my_mnist_predictions/']:\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        blob.delete()\n",
    "\n",
    "# Uncomment and run if the bucket is empty and we want to delete the\n",
    "# bucket itself\n",
    "# bucket.delete()\n",
    "batch_prediction_job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a Model to a Mobile or Embedded Device\n",
    "ML models can run closer to the source of data (this is called *edge computing*), e.g. in the user’s mobile device or in an embedded device. \n",
    "\n",
    "Pros:\n",
    "- It allows the device to be smart even when it’s not connected to the internet.\n",
    "- It reduces latency by not having to send data to a remote server and reduces the load on the servers.\n",
    "- It may improve privacy, since the user’s data can stay on the device.\n",
    "\n",
    "Cons:\n",
    "- The device’s computing resources are generally tiny compared to a beefy multi-GPU server.\n",
    "- A large model may not fit in the device, it may use too much RAM and CPU, and it may take too long to download. As a result, the application may become unresponsive, and the device may heat up and quickly run out of battery.\n",
    "\n",
    "The TFLite library provides several tools to help us deploy our models to the edge, with three main objectives:\n",
    "- Reduce the model size, to shorten download time and reduce RAM usage.\n",
    "- Reduce the amount of computations needed for each prediction, to reduce latency, battery usage, and heating.\n",
    "- Adapt the model to device-specific constraints.\n",
    "\n",
    "**Note**: Also check TensorFlow’s [Graph Transform Tool](https://homl.info/tfgtt) for modifying and optimizing computational graphs.\n",
    "\n",
    "TFLite’s model converter can take a SavedModel and compress it to a much lighter format based on [FlatBuffers](https://google.github.io/flatbuffers). This is an efficient cross-platform serialization library initially created by Google for gaming. It is designed so we can load FlatBuffers straight to RAM without any preprocessing: this reduces the loading time and memory footprint. Once the model is loaded into a mobile or embedded device, the TFLite interpreter will execute it to make predictions. Here is how we can convert a SavedModel to a FlatBuffer and save it to a *.tflite* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 09:03:52.237094: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-04-10 09:03:52.237108: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
      "2022-04-10 09:03:52.237830: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: my_mnist_model/0001\n",
      "2022-04-10 09:03:52.238869: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-04-10 09:03:52.238881: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: my_mnist_model/0001\n",
      "2022-04-10 09:03:52.242108: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-04-10 09:03:52.263868: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: my_mnist_model/0001\n",
      "2022-04-10 09:03:52.271298: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 33470 microseconds.\n",
      "2022-04-10 09:03:52.281694: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(str(model_path))\n",
    "tflite_model = converter.convert()\n",
    "with open('my_converted_savedmodel.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: We can also save a Keras model directly to a FlatBuffer using `tf.lite.TFLiteConverter.from_keras_model(model)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows how to convert a Keras model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The converter also optimizes the model, both to shrink it and to reduce its latency. It prunes all the operations that are not needed to make predictions (such as training operations), and it optimizes computations whenever possible (e.g. $3\\times a+4\\times a+5\\times a\\to 12\\times a$). Also it tries to fuse operations whenever possible (e.g. batch normalization layers end up folded into the previous layer’s addition and multiplication operations). \n",
    "\n",
    "Another way we can reduce the model size is by using smaller bit-widths: TFLite does this by quantizing the model weights down to fixed-point, 8-bit integers! This leads to a fourfold size reduction compared to using 32-bit floats. The simplest approach is called *post-training quantization*: it just quantizes the weights after training, using a fairly basic but efficient symmetrical quantization technique. It finds the maximum absolute weight value, $m$, then it maps the floating-point range $-m$ to $+m$ to the fixed-point (integer) range –127 to +127. e.g. if the weights range from –1.5 to +0.8, then the bytes –127, 0, and +127 will correspond to the floats –1.5, 0.0, and +1.5, respectively. Note that 0.0 always maps to 0 when using symmetrical quantization. Also note that the byte values +68 to +127 will not be used in this example, since they map to floats greater than +0.8.\n",
    "\n",
    "To perform this post-training quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/wy/h39t6kb11pnbb0pzhksd_fqh0000gq/T/tmp6ffbc1qs/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/wy/h39t6kb11pnbb0pzhksd_fqh0000gq/T/tmp6ffbc1qs/assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
      "2022-04-10 09:26:30.319286: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-04-10 09:26:30.319301: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2022-04-10 09:26:30.319417: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/wy/h39t6kb11pnbb0pzhksd_fqh0000gq/T/tmp6ffbc1qs\n",
      "2022-04-10 09:26:30.320420: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-04-10 09:26:30.320431: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /var/folders/wy/h39t6kb11pnbb0pzhksd_fqh0000gq/T/tmp6ffbc1qs\n",
      "2022-04-10 09:26:30.323773: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-04-10 09:26:30.345416: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/wy/h39t6kb11pnbb0pzhksd_fqh0000gq/T/tmp6ffbc1qs\n",
      "2022-04-10 09:26:30.354270: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 34852 microseconds.\n",
      "2022-04-10 09:26:30.392352: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor sequential/dense_1/MatMul because it has fewer than 1024 elements (1000).\n"
     ]
    }
   ],
   "source": [
    "tflite_model = converter.convert()\n",
    "with open('my_converted_keras_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At runtime the quantized weights get converted back to floats before they are used. To avoid recomputing the float values all the time, which would severely slow down the model, TFLite caches them: unfortunately, this means that this technique does not reduce RAM usage, and it doesn’t speed up the model either.\n",
    "\n",
    "The most effective way to reduce latency and power consumption is to also quantize the activations so that the computations can be done entirely with integers. Even when using the same bit-width, integer computations use less CPU cycles, consume less energy, and produce less heat. And if we also reduce the bit-width, we can get huge speedups.\n",
    "\n",
    "The main problem with quantization is that it loses a bit of accuracy: it is similar to adding noise to the weights and activations. If the accuracy drop is too severe, then we may need to use *quantization-aware training*. This means adding fake quantization operations to the model so it can learn to ignore the quantization noise during training; the final weights will then be more robust to quantization. Moreover, the calibration step can be taken care of automatically during training, which simplifies the whole process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Model in a Web Page\n",
    "This can be useful in many scenarios, such as:\n",
    "- When our web application is often used in situations where the user’s connectivity is intermittent or slow (e.g., a website for hikers).\n",
    "- When we need the model’s responses to be as fast as possible (e.g., for an online game).\n",
    "- When our web service makes predictions based on some private user data\n",
    "\n",
    "For all these scenarios, we can use the [TensorFlow.js (TFJS) JavaScript library](https://tensorflow.org/js). This library can load a TFLite model and make predictions directly in the user’s browser.\n",
    "\n",
    "The following JavaScript module imports the TFJS library, downloads a pretrained MobileNet model, and uses this model to classify an image and log the predictions:\n",
    "```js\n",
    "import \"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest\";\n",
    "import \"https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0\";\n",
    "const image = document.getElementById(\"image\");\n",
    "\n",
    "mobilenet.load().then(model => {\n",
    "  model.classify(image).then(predictions => {\n",
    "    for (var i = 0; i < predictions.length; i++) {\n",
    "      let className = predictions[i].className\n",
    "      let proba = (predictions[i].probability * 100).toFixed(1)\n",
    "      console.log(className + \" : \" + proba + \"%\");\n",
    "    } \n",
    "  });\n",
    "});\n",
    "```\n",
    "\n",
    "Code examples for this section are hosted on Glitch.com, a website that lets us create Web apps for free\n",
    "- https://homl.info/tfjscode a simple TFJS Web app that loads a pretrained model and classifies an image.\n",
    "- https://homl.info/tfjswpa the same Web app setup as a *progressive web app* PWA. Try opening this link on various platforms, including mobile devices. See https://homl.info/wpacode for this PWA’s source code.\n",
    "\n",
    "**Note**: Check out many more demos of machine learning models running in our browser at https://tensorflow.org/js/demos.\n",
    "\n",
    "TFJS also supports training a model directly in the web browser! If our computer has a GPU card, then TFJS can generally use it, even if it’s not an Nvidia card. A model can be trained centrally, and then fine-tuned locally, in the browser, based on that user’s data. Check out [federated learning](https://tensorflow.org/federated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPUs to Speed Up Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Our Own GPU\n",
    "We will need to consider the amount of RAM we will need for our tasks (e.g., typically at least 10 GB for image processing or NLP), the bandwidth (i.e., how fast we can send data into and out of the GPU), the number of cores, the cooling system, etc. Tim Dettmers wrote an excellent [blog post](https://homl.info/66) to help us choose. TensorFlow only supports [Nvidia cards with CUDA Compute Capability 3.5+](https://homl.info/cudagpus) (as well as Google’s TPUs), but it may extend its support to other manufacturers, so make sure to check [TensorFlow’s documentation](https://tensorflow.org/install) to see what devices are supported today.\n",
    "\n",
    "If we go for an Nvidia GPU card, we will need to install the appropriate Nvidia drivers and several Nvidia libraries. These include the *Compute Unified Device Architecture* library (CUDA) Toolkit, which allows developers to use CUDA-enabled GPUs for all sorts of computations, and the *CUDA Deep Neural Network* library (cuDNN), a GPU-accelerated library of common DNN computations such as activation layers, normalization, forward and backward convolutions, and pooling. cuDNN is part of Nvidia’s Deep Learning SDK. TensorFlow uses CUDA and cuDNN to control the GPU cards and accelerate computations. We can use the `nvidia-smi` command to check that everything is properly installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s check that TensorFlow can see the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "physical_gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the GPU RAM\n",
    "By default TensorFlow automatically grabs almost all the RAM in all available GPUs the first time we run a computation. It does this to limit GPU RAM fragmentation. If we need to run multiple programs for some reason (e.g., to train two different models in parallel on the same machine), we will need to split the GPU RAM between these processes more evenly.\n",
    "\n",
    "If we have multiple GPU cards on our machine, a simple solution is to assign each of them to a single process. To do this, we can set the `CUDA_VISIBLE_DEVICES` environment variable so that each process only sees the appropriate GPU card(s). Also set the `CUDA_DEVICE_ORDER` environment variable to `PCI_BUS_ID` to ensure that each ID always refers to the same GPU card. e.g. for four GPU cards:\n",
    "```bash\n",
    "CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,1 python3 program_1.py\n",
    "# And in another terminal\n",
    "CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3,2 python3 program_2.py\n",
    "```\n",
    "\n",
    "Program 1 will then only see GPU cards 0 and 1, named `'/gpu:0'` and `'/gpu:1'`, in TensorFlow, and program 2 will only see GPU cards 2 and 3, named `'/gpu:1'` and `'/gpu:0'`.\n",
    "\n",
    "Another option is to tell TensorFlow to grab only a specific amount of GPU RAM. This must be done immediately after importing TensorFlow. We must create a *logical GPU device* (sometimes called a *virtual GPU device*) for each physical GPU device and set its memory limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gpu in physical_gpus:\n",
    "#     tf.config.set_logical_device_configuration(\n",
    "#         gpu, [tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make TensorFlow grab memory as it needs it (only releasing it when the \n",
    "process shuts down):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gpu in physical_gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do this is to set the `TF_FORCE_GPU_ALLOW_GROWTH` environment \n",
    "variable to `true`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split a physical GPU into two logical GPUs (this is useful if we only have \n",
    "one physical GPU but we want to test a multi-GPU algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.set_logical_device_configuration(\n",
    "#     physical_gpus[0],\n",
    "#     [\n",
    "#         tf.config.LogicalDeviceConfiguration(memory_limit=2048),\n",
    "#         tf.config.LogicalDeviceConfiguration(memory_limit=2048),\n",
    "#     ],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "logical_gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing Operations and Variables on Devices\n",
    "We can place operations and variables manually on each device, if we want more control:\n",
    "- We generally want to place the data preprocessing operations on the CPU, and place the neural network operations on the GPUs.\n",
    "- GPUs usually have a fairly limited communication bandwidth, so it is important to avoid unnecessary data transfers into and out of the GPUs.\n",
    "- Adding more CPU RAM to a machine is simple and fairly cheap, so there’s usually plenty of it, whereas the GPU RAM is baked into the GPU: it is an expensive and thus limited resource, so if a variable is not needed in the next few training steps, it should probably be placed on the CPU (e.g., datasets generally belong on the CPU).\n",
    "\n",
    "By default, all variables and all operations will be placed on the first GPU (the one named `'/gpu:0'`), except for variables and operations that don’t have a GPU kernel: these are placed on the CPU (always named `'/cpu:0'`). A tensor or variable’s `device` attribute tells us which device it was placed on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To log every variable and operation placement (this must be run just\n",
    "# after importing TensorFlow):\n",
    "\n",
    "# Log level is INFO by default\n",
    "# tf.get_logger().setLevel('DEBUG')\n",
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:GPU:0'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# float32 variable goes to the GPU\n",
    "a = tf.Variable([1.0, 2.0, 3.0])\n",
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# int32 variable goes to the CPU\n",
    "b = tf.Variable([1, 2, 3])\n",
    "b.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can place variables and operations manually on the desired device using a `tf.device()` context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    c = tf.Variable([1.0, 2.0, 3.0])\n",
    "\n",
    "c.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we specify a device that does not exist, or for which there is no kernel, TensorFlow will silently fallback to the default placement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'/job:localhost/replica:0/task:0/device:GPU:0'\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/gpu:1234'):\n",
    "    d = tf.Variable([1.0, 2.0, 3.0])\n",
    "\n",
    "d.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want TensorFlow to throw an exception when we try to use a device that does not exist, instead of falling back to the default device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:1000'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].\n"
     ]
    }
   ],
   "source": [
    "tf.config.set_soft_device_placement(False)\n",
    "\n",
    "try:\n",
    "    with tf.device('/gpu:1000'):\n",
    "        d = tf.Variable([1.0, 2.0, 3.0])\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)\n",
    "\n",
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Execution Across Multiple Devices\n",
    "One of the benefits of using TF functions is parallelism. When TensorFlow runs a TF function, it starts by analyzing its graph to find the list of operations that need to be evaluated, and it counts how many dependencies each of them has. TensorFlow then adds each operation with zero dependencies (i.e., each source operation) to the evaluation queue of this operation’s device. Once an operation has been evaluated, the dependency counter of each operation that depends on it is decremented. Once an operation’s dependency counter reaches zero, it is pushed to the evaluation queue of its device. And once all the outputs have been computed, they are returned.\n",
    "\n",
    "Operations in the CPU’s evaluation queue are dispatched to a thread pool called the *inter-op thread pool*. If the CPU has multiple cores, then these operations will effectively be evaluated in parallel. Some operations have multithreaded CPU kernels: these kernels split their tasks into multiple suboperations, which are placed in another evaluation queue and dispatched to a second thread pool called the *intra-op thread pool* (shared by all multithreaded CPU kernels). In short, multiple operations and suboperations may be evaluated in parallel on different CPU cores.\n",
    "\n",
    "Operations in a GPU’s evaluation queue are evaluated sequentially. However, most operations have multithreaded GPU kernels, typically implemented by libraries that TensorFlow depends on, such as CUDA and cuDNN. These implementations have their own thread pools, and they typically exploit as many GPU threads as they can (which is the reason why there is no need for an inter-op thread pool in GPUs: each operation already floods most GPU threads).\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/19/parallel_tf_graph_execution.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "In the figure, operations A, B, and C are source ops, so they can immediately be evaluated. Operations A and B are placed on the CPU, so they are sent to the CPU’s evaluation queue, then they are dispatched to the inter-op thread pool and immediately evaluated in parallel. Operation A happens to have a multi‐threaded kernel; its computations are split into three parts, which are executed in parallel by the intra-op thread pool. Operation C goes to GPU #0’s evaluation queue, and in this example its GPU kernel happens to use cuDNN, which manages its own intra-op thread pool and runs the operation across many GPU threads in parallel. Suppose C finishes first. The dependency counters of D and E are decremented and they reach 0, so both operations are pushed to GPU #0’s evaluation queue, and they are executed sequentially. Note that C only gets evaluated once, even though both D and E depend on it. Suppose B finishes next. Then F’s dependency counter is decremented from 4 to 3, and since that’s not 0, it does not run yet. Once A, D, and E are finished, then F’s dependency counter reaches 0, and it is pushed to the CPU’s evaluation queue and evaluated. Finally, TensorFlow returns the requested outputs. \n",
    "\n",
    "When the TF function modifies a stateful resource, such as a variable, TensorFlow ensures that the order of execution matches the order in the code, even if there is no explicit dependency between the statements. e.g. if our TF function contains `v.assign_add(1)` followed by `v.assign(v * 2)`, TensorFlow will ensure that these operations are executed in that order.\n",
    "\n",
    "**Tip**: We can control the number of threads in the inter-op thread pool by calling `tf.config.threading.set_inter_op_parallelism_threads()`. To set the number of intra-op threads, use `tf.config.threading.set_intra_op_parallelism_threads()`. This is useful if we do not want TensorFlow to use all the CPU cores or if we want it to be single-threaded. This can be useful if we want to guarantee perfect reproducibility, Check out [this video](https://homl.info/repro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.threading.set_inter_op_parallelism_threads(10)\n",
    "# tf.config.threading.set_intra_op_parallelism_threads(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models Across Multiple Devices\n",
    "There are two main approaches to training a single model across multiple devices: *model parallelism*, where the model is split across the devices, and *data parallelism*, where the model is replicated across every device, and each replica is trained on a different subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parallelism\n",
    "Unfortunately, model parallelism turns out to be pretty tricky, and its effectiveness really depends on the architecture of our neural network.\n",
    "\n",
    "For fully connected networks, there is generally not much to be gained from this approach (cross-device communication are represented by the dashed arrows):\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/19/split_fcn.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "Some neural network architectures, such as convolutional neural networks, contain layers that are only partially connected to the lower layers, so it is much easier to distribute chunks across devices in an efficient way:\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/19/split_pcn.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "Deep recurrent neural networks can be split a bit more efficiently across multiple GPUs. If we split the network horizontally by placing each layer on a different device, and feed the network with an input sequence to process, then at the first time step only one device will be active (working on the sequence’s first value), at the second step two will be active (the second layer will be handling the output of the first layer for the first value, while the first layer will be handling the second value), and by the time the signal propagates to the output layer, all devices will be active simultaneously. There is still a lot of cross-device communication going on, but since each cell may be fairly complex, the benefit of running multiple cells in parallel may (in theory) outweigh the communication penalty. However, in practice a regular stack of `LSTM` layers running on a single GPU actually runs much faster.\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/19/split_rnn.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Parallelism\n",
    "Another way is to replicate neural network on every device and run each training step simultaneously on all replicas, using a different mini-batch for each. The gradients computed by each replica are then averaged, and the result is used to update the model parameters. This is called *data parallelism*, or sometimes *single program, multiple data* (SPMD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data parallelism using the mirrored strategy\n",
    "The simplest approach is to completely mirror all the model parameters across all the GPUs and always apply the exact same parameter updates on every GPU. This way, all replicas always remain perfectly identical. This is called the *mirrored strategy*, and it’s quite efficient, especially when using a single machine.\n",
    "\n",
    "The tricky part when using this approach is to efficiently compute the mean of all the gradients from all the GPUs and distribute the result across all the GPUs. This can be done using an *AllReduce* algorithm, a class of algorithms where multiple nodes collaborate to efficiently perform a *reduce operation* (such as computing the mean, sum, and max), while ensuring that all nodes obtain the same final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data parallelism with centralized parameters\n",
    "Another approach is to store the model parameters outside of the GPU devices performing the computations (called *workers*); e.g. on the CPU. In a distributed setup, we may place all the parameters on one or more CPU-only servers called *parameter servers*, whose only role is to host and update the parameters.\n",
    "\n",
    "The mirrored strategy imposes synchronous weight updates across all GPUs, the centralized approach allows either synchronous or asynchronous updates. Let’s take a look at the pros and cons of both options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Synchronous updates\n",
    "With *synchronous updates*, the aggregator waits until all gradients are available before it computes the average gradients and passes them to the optimizer, which will update the model parameters. Once a replica has finished computing its gradients, it must wait for the parameters to be updated before it can proceed to the next mini-batch. The downside is that the fast devices will have to wait for the slow ones at every step, making the whole process as slow as the slowest device. Moreover, the parameters will be copied to every device almost at the same time (immediately after the gradients are applied), which may saturate the parameter servers’ bandwidth.\n",
    "\n",
    "**Tip**: To reduce the waiting time at each step, we could ignore the gradients from the slowest few replicas (typically ~10%). As soon as the parameters are updated, the first 90% replicas can start working again immediately, without having to wait for the 10% slowest replicas. This setup is generally described as having 90% replicas plus 10% *spare replicas*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Asynchronous updates\n",
    "With asynchronous updates, whenever a replica has finished computing the gradients, the gradients are immediately used to update the model parameters. There is no aggregation and no synchronization. Since there is no waiting for the other replicas, this approach runs more training steps per minute. Moreover, although the parameters still need to be copied to every device at every step, this happens at different times for each replica, so the risk of bandwidth saturation is reduced.\n",
    "\n",
    "However, although it works reasonably well in practice, it is almost surprising that it works at all! By the time a replica has finished computing the gradients based on some parameter values, these parameters will have been updated several times by other replicas (on average $N–1$ times, if there are $N$ replicas), and there is no guarantee that the computed gradients will still be pointing in the right direction. When gradients are severely out of date, they are called *stale* gradients: they can slow down convergence, introducing noise and wobble effects (the learning curve may contain temporary oscillations), or they can even make the training algorithm diverge.\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/19/stale_gradients.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "There are a few ways we can reduce the effect of stale gradients:\n",
    "- Reduce the learning rate.\n",
    "- Drop stale gradients or scale them down.\n",
    "- Adjust the mini-batch size.\n",
    "- Start the first few epochs using just one replica (this is called the *warmup phase*). Stale gradients tend to be more damaging at the beginning of training, when gradients are typically large and the parameters have not settled into a valley of the cost function yet, so different replicas may push the parameters in quite different directions.\n",
    "\n",
    "A paper [“Revisiting Distributed Synchronous SGD”](https://homl.info/68) published by Jianmin Chen et al. from the Google Brain team in 2016 benchmarked various approaches and found that using synchronous updates with a few spare replicas was more efficient than using asynchronous updates, not only converging faster but also producing a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Bandwidth saturation\n",
    "Unfortunately, there often comes a point where adding an extra GPU will not improve performance at all because the time spent moving the data into and out of GPU RAM (and across the network in a distributed setup) will outweigh the speedup obtained by splitting the computation load.\n",
    "\n",
    "Saturation is more severe for large dense models, since they have a lot of parameters and gradients to transfer. It is less severe for small models (but the parallelization gain is limited) and for large sparse models, where the gradients are typically mostly zeros and so can be communicated efficiently.\n",
    "\n",
    "A 2018 paper [“PipeDream: Fast and Efficient Pipeline Parallel DNN Training”](https://homl.info/pipedream) by Aaron Harlap et al. a team of researchers from Carnegie Mellon University, Stanford University, and Microsoft Research proposed a system called *PipeDream* that managed to reduce network communications by over 90%, making it possible to train large models across many machines. They achieved this using a new technique called *pipeline parallelism*, which combines model parallelism and data parallelism: the model is chopped into consecutive parts, called *stages*, each of which is trained on a different machine. This results in an asynchronous pipeline in which all machines work in parallel with very little idle time. During training, each stage alternates one round of forward propagation and one round of backpropagation: it pulls a mini-batch from its input queue, processes it, and sends the outputs to the next stage’s input queue, then it pulls one mini-batch of gradients from its gradient queue, backpropagates these gradients and updates its own model parameters, and pushes the backpropagated gradients to the previous stage’s gradient queue. Each stage can also use regular data parallelism (e.g., using the mirrored strategy), independently from the other stages.\n",
    "\n",
    "<center>\n",
    "  <img \n",
    "    src=\"../images/19/pipedream.png\" \n",
    "    onerror=\"\n",
    "      this.onerror = null;\n",
    "      const repo = 'https://github.com/alirezatheh/handson-ml3-notes/blob/main';\n",
    "      this.src = repo + this.src.split('..')[1];\n",
    "    \"\n",
    "  >\n",
    "</center>\n",
    "\n",
    "As it’s presented here, PipeDream would not work so well. To understand why, consider mini-batch #5: when it went through stage 1 during the forward pass, the gradients from mini-batch #4 had not yet been backpropagated through that stage, but by the time #5’s gradients flow back to stage 1, #4’s gradients will have been used to update the model parameters, so #5’s gradients will be a bit stale. The paper’s authors proposed methods to mitigate this issue, though: e.g. each stage saves weights during forward propagation and restores them during backpropagation, to ensure that the same weights are used for both the forward pass and the backward pass. This is called *weight stashing*.\n",
    "\n",
    "In a 2022 paper [“Pathways: Asynchronous Distributed Dataflow for ML”](https://homl.info/pathways) by Google researchers Paul Barham et al. they developed a system called *Pathways* that uses automated model parallelism, asynchronous gang scheduling, and other techniques to reach close to 100% hardware utilization across thousands of TPUs! *Scheduling* means organizing when and where each task must run, and *gang scheduling* means running related tasks at the same time in parallel and close to each other to reduce the time tasks have to wait for the others’ outputs. This system was used to train a massive language model across over 6,000 TPUs, with close to 100% hardware utilization: that’s a mindblowing engineering feat.\n",
    "\n",
    "To reduce the saturation problem, we’ll probably want to use a few powerful GPUs rather than plenty of weak GPUs, and if we need to train a model across multiple servers, we should group our GPUs on few and very well interconnected servers. We can also try dropping the float precision from 32 bits (`tf.float32`) to 16 bits (`tf.bfloat16`). If we are using centralized parameters, we can shard (split) the parameters across multiple parameter servers: adding more parameter servers will reduce the network load on each server and limit the risk of bandwidth saturation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training at Scale Using the Distribution Strategies API\n",
    "To train a Keras model across all available GPUs (on a single machine) using data parallelism with the mirrored strategy, just create a `MirroredStrategy` object, call its `scope()` method to get a distribution context, and wrap the creation and compilation of our model inside that context. Then call the model’s `fit()` method normally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a CNN model for MNIST using Keras\n",
    "\n",
    "\n",
    "def create_model() -> keras.Model:\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Reshape(\n",
    "                [28, 28, 1], input_shape=[28, 28], dtype=tf.uint8\n",
    "            ),\n",
    "            keras.layers.Rescaling(scale=1 / 255),\n",
    "            keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=7, activation='relu', padding='same'\n",
    "            ),\n",
    "            keras.layers.MaxPooling2D(pool_size=2),\n",
    "            keras.layers.Conv2D(\n",
    "                filters=128, kernel_size=3, activation='relu', padding='same'\n",
    "            ),\n",
    "            keras.layers.Conv2D(\n",
    "                filters=128, kernel_size=3, activation='relu', padding='same'\n",
    "            ),\n",
    "            keras.layers.MaxPooling2D(pool_size=2),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(units=64, activation='relu'),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            keras.layers.Dense(units=10, activation='softmax'),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    # Create a Keras model normally\n",
    "    model = create_model()\n",
    "    # Compile the model normally\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "# Preferably divisible by the number of replicas\n",
    "batch_size = 100\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.distribute.values.MirroredVariable"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `fit()` method will automatically split each training batch across all the replicas. Also calling the `predict()` method will automatically split the batch across all replicas, making predictions in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# The batch is split across all replicas\n",
    "model.predict(X_new).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call the model’s `save()` method, it will be saved as a regular model, \n",
    "*not* as a mirrored model with multiple replicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mirrored_model/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensorflow.python.ops.resource_variable_ops.ResourceVariable"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows that saving a model does not preserve its distribution strategy\n",
    "model.save('my_mirrored_model', save_format='tf')\n",
    "model = keras.models.load_model('my_mirrored_model')\n",
    "type(model.weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = keras.models.load_model('my_mirrored_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow.python.distribute.values.MirroredVariable\n"
     ]
    }
   ],
   "source": [
    "type(model.weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to specify the list of GPUs to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:localhost/replica:0/task:0/device:GPU:0,/job:localhost/replica:0/task:0/device:GPU:1\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=['/gpu:0', '/gpu:1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `MirroredStrategy` class uses the *NVIDIA Collective Communications Library* (NCCL) for the AllReduce mean operation, but we can change it by setting the `cross_device_ops` argument to an instance of the `tf.distribute.HierarchicalCopyAllReduce` class, or an instance of the `tf.distribute.ReductionToOneDevice` class. The default NCCL option is based on the `tf.distribute.NcclAllReduce` class, which is usually faster, but this depends on the number and types of GPUs, so we may want to give the alternatives a try. For more details on AllReduce algorithms, read [Yuichiro Ueno’s post](https://homl.info/uenopost) on the technologies behind deep learning and [Sylvain Jeaugey’s post](https://homl.info/ncclalgo) on massively scaling deep learning training with NCCL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(\n",
    "    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use data parallelism with centralized parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:CPU:0'], variable_device = '/job:localhost/replica:0/task:0/device:CPU:0'\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.experimental.CentralStorageStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optionally set the `compute_devices` argument to specify the list of devices we want to use as workers (by default it will use all available GPUs) and we can optionally set the `parameter_device` argument to specify the device we want to store the parameters on. By default it will use the CPU, or the GPU if there is just one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Model on a TensorFlow Cluster\n",
    "A *TensorFlow cluster* is a group of TensorFlow processes running in parallel, usually on different machines, and talking to each other to complete some work, e.g. training or executing a neural network model. Each TF process in the cluster is called a *task*, or a *TF server*. It has an IP address, a port, and a type (also called its *role* or its *job*, actually the set of tasks that share the same type is often called a *job*). The type can be either `'worker'`, `'chief'`, `'ps'` (parameter server), or `'evaluator'`:\n",
    "- Each *worker* performs computations, usually on a machine with one or more GPUs.\n",
    "- The *chief* performs computations as well (it is a worker), but it also handles extra work such as writing TensorBoard logs or saving checkpoints. There is a single chief in a cluster. If no chief is specified explicitly, then by convention the first worker is the chief.\n",
    "- A *parameter server* only keeps track of variable values, and it is usually on a CPU-only machine. This type of task is only used with the `ParameterServerStrategy`.\n",
    "- An *evaluator* obviously takes care of evaluation. This type is not used often, and when it’s used, there’s usually just one evaluator.\n",
    "\n",
    "To start a TensorFlow cluster, we must first define it. This means specifying all the tasks (IP address, TCP port, and type). e.g. the following *cluster specification* defines a cluster with 3 tasks (2 workers and 1 parameter server). It’s a dictionary with one key per job, and the values are lists of task addresses (*IP*:*port*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_spec = {\n",
    "    'worker': [\n",
    "        # /job:worker/task:0\n",
    "        'machine-a.example.com:2222',\n",
    "        # /job:worker/task:1\n",
    "        'machine-b.example.com:2222',\n",
    "    ],\n",
    "    # /job:ps/task:0\n",
    "    'ps': ['machine-a.example.com:2221'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general there will be a single task per machine, but as this example shows, we can configure multiple tasks on the same machine if we want. In this case, if they share the same GPUs, make sure the RAM is split appropriately.\n",
    "\n",
    "**Warning**: Every task in the cluster may communicate with every other task in the server, so make sure to configure our firewall to authorize all communications between these machines on these ports (it’s usually simpler if we use the same port on every machine).\n",
    "\n",
    "When a task is started, it needs to be told which one it is: its type and index (the task index is also called the task id). A common way to specify everything at once (both the cluster spec and the current task’s type and id) is to set the `TF_CONFIG` environment variable before starting the program. It must be a JSON-encoded dictionary containing a cluster specification (under the `'cluster'` key), and the type and index of the task to start (under the `'task'` key). The following `TF_CONFIG` environment variable defines the same cluster as above, with 2 workers and 1 parameter server, and specifies that the task to start is worker \\#0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CONFIG'] = json.dumps(\n",
    "    {'cluster': cluster_spec, 'task': {'type': 'worker', 'index': 0}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: In general we want to define the `TF_CONFIG` environment variable outside of Python, so the code does not need to include the current task’s type and index (this makes it possible to use the same code across all workers).\n",
    "\n",
    "Some platforms (e.g., Google Vertex AI) automatically set this environment variable for us. TensorFlow’s `TFConfigClusterResolver` class reads the cluster configuration from this environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClusterSpec({'ps': ['machine-a.example.com:2221'], 'worker': ['machine-a.example.com:2222', 'machine-b.example.com:2222']})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "resolver.cluster_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worker'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolver.task_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolver.task_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s run a simpler cluster with just two worker tasks, both running on the local machine. We will use the `MultiWorkerMirroredStrategy` to train a model across these two tasks.\n",
    "\n",
    "The first step is to write the training code. As this code will be used to run both workers, each in its own process, we write this code to a separate Python file, `my_mnist_multiworker_task.py`. The code is relatively straightforward, but there are a couple important things to note:\n",
    "- We create the `MultiWorkerMirroredStrategy` before doing anything else with TensorFlow.\n",
    "- Only one of the workers will take care of logging to TensorBoard. As mentioned earlier, this worker is called the *chief*. When it is not defined explicitly, then by convention it is worker #0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_mnist_multiworker_task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_mnist_multiworker_task.py\n",
    "\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "# At the start!\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "print(f'Starting task {resolver.task_type} #{resolver.task_id}')\n",
    "\n",
    "\n",
    "if resolver.task_id == 0:\n",
    "    # The chief uses the right locations\n",
    "    model_dir = 'my_mnist_multiworker_model'\n",
    "    tensorboard_log_dir = 'my_mnist_multiworker_logs'\n",
    "    checkpoint_dir = 'my_mnist_multiworker_checkpoints'\n",
    "else:\n",
    "    # Other workers use a temporary dirs\n",
    "    tmp_dir = Path(tempfile.mkdtemp())\n",
    "    model_dir = tmp_dir / 'model'\n",
    "    tensorboard_log_dir = tmp_dir / 'logs'\n",
    "    checkpoint_dir = tmp_dir / 'ckpt'\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(tensorboard_log_dir),\n",
    "    tf.keras.callbacks.ModelCheckpoint(checkpoint_dir),\n",
    "]\n",
    "\n",
    "# Load and split the MNIST dataset\n",
    "mnist = keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "with strategy.scope():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Reshape(\n",
    "                [28, 28, 1], input_shape=[28, 28], dtype=tf.uint8\n",
    "            ),\n",
    "            keras.layers.Rescaling(scale=1 / 255),\n",
    "            keras.layers.Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=7,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                input_shape=[28, 28, 1],\n",
    "            ),\n",
    "            keras.layers.MaxPooling2D(pool_size=2),\n",
    "            keras.layers.Conv2D(\n",
    "                filters=128, kernel_size=3, activation='relu', padding='same'\n",
    "            ),\n",
    "            keras.layers.Conv2D(\n",
    "                filters=128, kernel_size=3, activation='relu', padding='same'\n",
    "            ),\n",
    "            keras.layers.MaxPooling2D(pool_size=2),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(units=64, activation='relu'),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            keras.layers.Dense(units=10, activation='softmax'),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10)\n",
    "\n",
    "model.save(model_dir, save_format='tf')\n",
    "\n",
    "if resolver.task_id != 0:\n",
    "    # And we can delete this directory at the end!\n",
    "    tf.io.gfile.rmtree(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: When using the `MultiWorkerMirroredStrategy`, it’s important to ensure that all workers do the same thing, including saving model checkpoints or writing TensorBoard logs, even though we will only keep what the chief writes. This is because these operations may need to run the AllReduce operations, so all workers must be in sync.\n",
    "\n",
    "Since we’re running both workers on the same machine, to avoid out-of-memory (OOM) error, we could use the `CUDA_VISIBLE_DEVICES` environment variable to assign a different GPU to each worker. Alternatively, we can simply disable GPU support, by setting `CUDA_VISIBLE_DEVICES` to an empty string.\n",
    "\n",
    "We are now ready to start both workers, each in its own process. Notice that we change the task index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "\n",
    "export CUDA_VISIBLE_DEVICES=\"\"\n",
    "export TF_CONFIG=\"{\n",
    "    'cluster': {'worker': ['127.0.0.1:9901', '127.0.0.1:9902']},\n",
    "    'task': {'type': 'worker', 'index': 0}\n",
    "}\"\n",
    "python my_mnist_multiworker_task.py > my_worker_0.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "\n",
    "export CUDA_VISIBLE_DEVICES=\"\"\n",
    "export TF_CONFIG=\"{\n",
    "    'cluster': {'worker': ['127.0.0.1:9901', '127.0.0.1:9902']},\n",
    "    'task': {'type': 'worker', 'index': 1}\n",
    "}\"\n",
    "python my_mnist_multiworker_task.py > my_worker_1.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If we get warnings about `AutoShardPolicy`, we can safely ignore them. See [TF issue #42146](https://github.com/tensorflow/tensorflow/issues/42146) for more details.\n",
    "\n",
    "That’s it! Our TensorFlow cluster is now running, but we can’t see it in this notebook because it’s running in separate processes (but we can see the progress in `my_worker_*.log`).\n",
    "\n",
    "Since the chief (worker #0) is writing to TensorBoard, we use TensorBoard to view the training progress. Run the following cell, then click on the settings button (i.e., the gear icon) in the TensorBoard interface and check the “Reload data” box to make TensorBoard automatically refresh every 30s. Once the first epoch of training is finished (which may take a few minutes), and once TensorBoard refreshes, the SCALARS tab will appear. Click on this tab to view the progress of the model’s training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_mnist_multiworker_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two AllReduce implementations for this distribution strategy: a ring AllReduce algorithm based on gRPC for the network communications, and NCCL’s implementation. The best algorithm to use depends on the number of workers, the number and types of GPUs, and the network. By default, TensorFlow will apply some heuristics to select the right algorithm for us, but we can force NCCL (or RING) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
    "#     communication_options=tf.distribute.experimental.CommunicationOptions(\n",
    "#         implementation=tf.distribute.experimental.CollectiveCommunication.NCCL\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we prefer to implement asynchronous data parallelism with parameter servers, change the strategy to `ParameterServerStrategy`, add one or more parameter servers, and configure `TF_CONFIG` appropriately for each task.\n",
    "\n",
    "If we have access to [TPUs on Google Cloud](https://cloud.google.com/tpu), e.g. if we use Colab and we set the accelerator type to TPU, then we can create a `TPUStrategy` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To train on a TPU in Google Colab:\n",
    "# if 'google.colab' in sys.modules and 'COLAB_TPU_ADDR' in os.environ:\n",
    "#     tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "# else:\n",
    "#     tpu_address = ''\n",
    "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "#     tpu_address\n",
    "# )\n",
    "# tf.config.experimental_connect_to_cluster(resolver)\n",
    "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# strategy = tf.distribute.experimental.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: We may be eligible to use TPUs for free for research purposes; see https://tensorflow.org/tfrc for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Large Training Jobs on Vertex AI\n",
    "Let’s copy the training script, but instead of saving the model to a local directory, the chief must save things to GCS, using the paths provided by Vertex AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_vertex_ai_training_task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_vertex_ai_training_task.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "# At the start!\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "\n",
    "if resolver.task_type == 'chief':\n",
    "    # Paths provided by Vertex AI\n",
    "    model_dir = os.getenv('AIP_MODEL_DIR')\n",
    "    tensorboard_log_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR')\n",
    "    checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR')\n",
    "else:\n",
    "    # Other workers use a temporary dirs\n",
    "    tmp_dir = Path(tempfile.mkdtemp())\n",
    "    model_dir = tmp_dir / 'model'\n",
    "    tensorboard_log_dir = tmp_dir / 'logs'\n",
    "    checkpoint_dir = tmp_dir / 'ckpt'\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(tensorboard_log_dir),\n",
    "    keras.callbacks.ModelCheckpoint(checkpoint_dir),\n",
    "]\n",
    "\n",
    "# Load and prepare the MNIST dataset\n",
    "mnist = keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Build and compile the Keras model using the distribution strategy\n",
    "with strategy.scope():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Reshape(\n",
    "                [28, 28, 1], input_shape=[28, 28], dtype=tf.uint8\n",
    "            ),\n",
    "            keras.layers.Lambda(lambda X: X / 255),\n",
    "            keras.layers.Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=7,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                input_shape=[28, 28, 1],\n",
    "            ),\n",
    "            keras.layers.MaxPooling2D(pool_size=2),\n",
    "            keras.layers.Conv2D(\n",
    "                filters=128, kernel_size=3, activation='relu', padding='same'\n",
    "            ),\n",
    "            keras.layers.Conv2D(\n",
    "                filters=128, kernel_size=3, activation='relu', padding='same'\n",
    "            ),\n",
    "            keras.layers.MaxPooling2D(pool_size=2),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(units=64, activation='relu'),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            keras.layers.Dense(units=10, activation='softmax'),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "model.save(model_dir, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: If we place the training data on GCS, we can create a `tf.data.TextLineDataset` or `tf.data.TFRecordDataset` to access it: just use the GCS paths as the filenames (e.g., *gs://my_bucket/data/001.csv*). These datasets rely on the `tf.io.gfile` package to access files: it supports both local files and GCS files.\n",
    "\n",
    "Now we can create a custom training job on Vertex AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_training_job = aiplatform.CustomTrainingJob(\n",
    "    display_name='my_custom_training_job',\n",
    "    script_path='my_vertex_ai_training_task.py',\n",
    "    # The Docker image to use for training\n",
    "    container_uri='gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest',\n",
    "    # The one to use for predictions (after training)\n",
    "    model_serving_container_image_uri=server_image,\n",
    "    # Not needed, this is just an example\n",
    "    requirements=['gcsfs==2022.3.0'],\n",
    "    # To store the training script, trained model, tb logs, model\n",
    "    # checkpoints\n",
    "    staging_bucket=f'gs://{bucket_name}/staging',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let’s run it on two workers, each with two GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://my_bucket/aiplatform-2022-04-14-10:08:24.124-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://my_bucket/aiplatform-custom-training-2022-04-14-10:08:25.226 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5407999068506947584?project=522977795627\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/5407999068506947584 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/5407999068506947584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6685701948726837248?project=522977795627\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/5407999068506947584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/5407999068506947584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/5407999068506947584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/5407999068506947584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/5407999068506947584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/5407999068506947584 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob run completed. Resource name: projects/522977795627/locations/us-central1/trainingPipelines/5407999068506947584\n",
      "Model available at projects/522977795627/locations/us-central1/models/9094548856498028544\n"
     ]
    }
   ],
   "source": [
    "mnist_model2 = custom_training_job.run(\n",
    "    machine_type='n1-standard-4',\n",
    "    replica_count=2,\n",
    "    accelerator_type='NVIDIA_TESLA_K80',\n",
    "    accelerator_count=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertex AI will provision the compute nodes we requested (within our quotas), and it will run our training script across them. Once the job is complete, the `run()` method will return a trained model: we can deploy it to an endpoint, or use it to make batch predictions. If anything goes wrong during training, we can view the logs in the GCP console: in the ☰ navigation menu, select Vertex AI → Training, click on our training job, and click VIEW LOGS. Alternatively, we can click the CUSTOM JOBS tab and copy the job’s ID (e.g., 1234), then select Logging from the ☰ navigation menu and query `resource.labels.job_id=1234`.\n",
    "\n",
    "**Tip**: To visualize the training progress, just start TensorBoard and point its `--logdir` to the GCS path of the logs. It will use *application default credentials*, which we can set up using `gcloud auth application-default login`. Vertex AI also offers hosted TensorBoard servers if we prefer.\n",
    "\n",
    "To try out a few hyperparameter values, one option is to run multiple jobs. we can pass the hyperparameter values to our script as command-line arguments by setting the `args` parameter when calling the `run()` method, or we can pass them as environment variables using the `environment_variables` parameter.\n",
    "\n",
    "Let’s clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model2.delete()\n",
    "custom_training_job.delete()\n",
    "blobs = bucket.list_blobs(prefix=f'gs://{bucket_name}/staging/')\n",
    "for blob in blobs:\n",
    "    blob.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning on Vertex AI\n",
    "Vertex AI’s hyperparameter tuning service is based on a Bayesian optimization algorithm, capable of quickly finding optimal combinations of hyperparameters. To use it, we first need to create a training script that accepts hyperparameter values as command-line arguments.\n",
    "\n",
    "The hyperparameter tuning service will call our script multiple times, each time with different hyperparameter values: each run is called a *trial*, and the set of trials is called a *study*. Our training script must then use the given hyperparameter values to build and compile a model. We can use a mirrored distribution strategy if we want, in case each trial runs on a multi-GPU machine. Then the script can load the dataset and train the model.\n",
    "\n",
    "Lastly, the script must report the model’s performance back to Vertex AI’s hyperparameter tuning service, so it can decide which hyperparameters to try next. For this, we must use the `hypertune` library, which is automatically installed on Vertex AI training VMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_vertex_ai_trial.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_vertex_ai_trial.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "import hypertune\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--n_hidden', type=int, default=2)\n",
    "parser.add_argument('--n_neurons', type=int, default=256)\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-2)\n",
    "parser.add_argument('--optimizer', default='adam')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def build_model(args: argparse.Namespace) -> keras.Model:\n",
    "    with tf.distribute.MirroredStrategy().scope():\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8))\n",
    "        for _ in range(args.n_hidden):\n",
    "            model.add(keras.layers.Dense(args.n_neurons, activation='relu'))\n",
    "        model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "        opt = keras.optimizers.get(args.optimizer)\n",
    "        opt.learning_rate = args.learning_rate\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "# Loads and splits the dataset\n",
    "mnist = keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Use the AIP_* environment variable and create the callbacks\n",
    "model_dir = os.getenv('AIP_MODEL_DIR')\n",
    "tensorboard_log_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR')\n",
    "checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR')\n",
    "trial_id = os.getenv('CLOUD_ML_TRIAL_ID')\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(tensorboard_log_dir)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5)\n",
    "callbacks = [tensorboard_cb, early_stopping_cb]\n",
    "\n",
    "model = build_model(args)\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "model.save(model_dir, save_format='tf')\n",
    "\n",
    "hypertune = hypertune.HyperTune()\n",
    "hypertune.report_hyperparameter_tuning_metric(\n",
    "    # Name of the reported metric\n",
    "    hyperparameter_metric_tag='accuracy',\n",
    "    # Max accuracy value\n",
    "    metric_value=max(history.history['val_accuracy']),\n",
    "    global_step=model.optimizer.iterations.numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then must define a custom job, which Vertex AI will use as a template for each trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://homl3-mybucket5/staging/aiplatform-2022-04-18-18:14:02.860-aiplatform_custom_trainer_script-0.1.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "trial_job = aiplatform.CustomJob.from_local_script(\n",
    "    display_name='my_search_trial_job',\n",
    "    # Path to our training script\n",
    "    script_path='my_vertex_ai_trial.py',\n",
    "    container_uri='gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest',\n",
    "    staging_bucket=f'gs://{bucket_name}/staging',\n",
    "    accelerator_type='NVIDIA_TESLA_K80',\n",
    "    # In this example, each trial will have 2 GPUs\n",
    "    accelerator_count=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we’re ready to create and run the hyperparameter tuning job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HyperparameterTuningJob\n",
      "HyperparameterTuningJob created. Resource name: projects/522977795627/locations/us-central1/hyperparameterTuningJobs/5825136187899117568\n",
      "To use this HyperparameterTuningJob in another session:\n",
      "hpt_job = aiplatform.HyperparameterTuningJob.get('projects/522977795627/locations/us-central1/hyperparameterTuningJobs/5825136187899117568')\n",
      "View HyperparameterTuningJob:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5825136187899117568?project=522977795627\n",
      "HyperparameterTuningJob projects/522977795627/locations/us-central1/hyperparameterTuningJobs/5825136187899117568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/522977795627/locations/us-central1/hyperparameterTuningJobs/5825136187899117568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/522977795627/locations/us-central1/hyperparameterTuningJobs/5825136187899117568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/522977795627/locations/us-central1/hyperparameterTuningJobs/5825136187899117568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/522977795627/locations/us-central1/hyperparameterTuningJobs/5825136187899117568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/522977795627/locations/us-central1/hyperparameterTuningJobs/5825136187899117568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/522977795627/locations/us-central1/hyperparameterTuningJobs/5825136187899117568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/522977795627/locations/us-central1/hyperparameterTuningJobs/5825136187899117568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/522977795627/locations/us-central1/hyperparameterTuningJobs/5825136187899117568 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "HyperparameterTuningJob run completed. Resource name: projects/522977795627/locations/us-central1/hyperparameterTuningJobs/5825136187899117568\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "hp_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name='my_hp_search_job',\n",
    "    custom_job=trial_job,\n",
    "    metric_spec={'accuracy': 'maximize'},\n",
    "    # Names must match the command-line arguments of the training script\n",
    "    parameter_spec={\n",
    "        'learning_rate': hpt.DoubleParameterSpec(\n",
    "            min=1e-3, max=10, scale='log'\n",
    "        ),\n",
    "        'n_neurons': hpt.IntegerParameterSpec(min=1, max=300, scale='linear'),\n",
    "        'n_hidden': hpt.IntegerParameterSpec(min=1, max=10, scale='linear'),\n",
    "        'optimizer': hpt.CategoricalParameterSpec(['sgd', 'adam']),\n",
    "    },\n",
    "    max_trial_count=100,\n",
    "    parallel_trial_count=20,\n",
    ")\n",
    "hp_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job is completed, we can fetch the trial results using `hp_job.trials`. Each trial result is represented as a protobuf object, containing the hyperparameter values and the resulting metrics. Let’s find the best trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.compat.types import study\n",
    "\n",
    "\n",
    "def get_final_metric(trial: study.Trial, metric_id: str) -> float:\n",
    "    for metric in trial.final_measurement.metrics:\n",
    "        if metric.metric_id == metric_id:\n",
    "            return metric.value\n",
    "\n",
    "\n",
    "aiplatform.HyperparameterTuningJob().trials\n",
    "trials = hp_job.trials\n",
    "trial_accuracies = [get_final_metric(trial, 'accuracy') for trial in trials]\n",
    "best_trial = trials[np.argmax(trial_accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.977400004863739"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(trial_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'98'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parameter_id: \"learning_rate\"\n",
       "value {\n",
       "  number_value: 0.001\n",
       "}\n",
       ", parameter_id: \"n_hidden\"\n",
       "value {\n",
       "  number_value: 8.0\n",
       "}\n",
       ", parameter_id: \"n_neurons\"\n",
       "value {\n",
       "  number_value: 216.0\n",
       "}\n",
       ", parameter_id: \"optimizer\"\n",
       "value {\n",
       "  string_value: \"adam\"\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid;\">\n",
    "\n",
    "#### Hyperparameter Tuning Using Keras Tuner on Vertex AI\n",
    "Instead of using Vertex AI’s hyperparameter tuning service, we can use [Keras Tuner](https://keras.io/keras_tuner/) and run it on Vertex AI VMs. Keras Tuner provides a simple way to scale hyperparameter search by distributing it across multiple machines: it only requires setting three environment variables on each machine, then running our regular Keras Tuner code on each machine. We can use the exact same script on all machines. One of the machines acts as the chief (i.e., the oracle), and the others act as workers. Each worker asks the chief which hyperparameter values to try, then the worker trains the model using these hyperparameter values, and finally it reports the model’s performance back to the chief, which can then decide which hyperparameter values the worker should try next.\n",
    "\n",
    "The three environment variables we need to set on each machine are:\n",
    "- `KERASTUNER_TUNER_ID`: Equal to `'chief'` on the chief machine, or a unique identifier on each worker machine, such as `'worker0'`, `'worker1'`, etc.\n",
    "- `KERASTUNER_ORACLE_IP`: The IP address or hostname of the chief machine. The chief itself should generally use `'0.0.0.0'` to listen on every IP address on the machine.\n",
    "- `KERASTUNER_ORACLE_PORT`: The TCP port that the chief will be listening on.\n",
    "\n",
    "We can use distributed Keras Tuner on any set of machines. If we want to run it on Vertex AI machines, then we can spawn a regular training job, and just modify the training script to set the three environment variables properly before using Keras Tuner.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Material – Implement Distributed Keras Tuner on Vertex AI\n",
    "The script below starts by parsing the `TF_CONFIG` environment variable, which will be automatically set by Vertex AI, just like earlier. It finds the address of the task of type `'chief'`, and it extracts the IP address or hostname, and the TCP port. It then defines the tuner ID as the task type followed by the task index, for example `'worker0'`. If the tuner ID is `'chief0'`, it changes it to `'chief'`, and it sets the IP to `'0.0.0.0'`: this will make it listen on all IPv4 address on its machine. Then it defines the environment variables for Keras Tuner. Next, the script creates a tuner, then it runs the search, and finally it saves the best model to the location given by Vertex AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_keras_tuner_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_keras_tuner_search.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_config = json.loads(os.environ['TF_CONFIG'])\n",
    "\n",
    "chief_ip, chief_port = tf_config['cluster']['chief'][0].rsplit(':', 1)\n",
    "tuner_id = f\"{tf_config['task']['type']}{tf_config['task']['index']}\"\n",
    "if tuner_id == 'chief0':\n",
    "    tuner_id = 'chief'\n",
    "    chief_ip = '0.0.0.0'\n",
    "    # Since the chief doesn’t work much, we can optimize compute\n",
    "    # resources by running a worker on the same machine. To do this, we\n",
    "    # can just make the chief start another process, after tweaking the\n",
    "    # TF_CONFIG environment variable to set the task type to 'worker'\n",
    "    # and the task index to a unique value. Uncomment the next few lines\n",
    "    # to give this a try:\n",
    "    # import subprocess\n",
    "    # import sys\n",
    "\n",
    "    # # The worker on the chief’s machine\n",
    "    # tf_config['task']['type'] = 'workerX'\n",
    "    # os.environ['TF_CONFIG'] = json.dumps(tf_config)\n",
    "    # subprocess.Popen(\n",
    "    #     [sys.executable] + sys.argv,\n",
    "    #     stdout=sys.stdout,\n",
    "    #     stderr=sys.stderr,\n",
    "    # )\n",
    "\n",
    "os.environ['KERASTUNER_TUNER_ID'] = tuner_id\n",
    "os.environ['KERASTUNER_ORACLE_IP'] = chief_ip\n",
    "os.environ['KERASTUNER_ORACLE_PORT'] = chief_port\n",
    "\n",
    "# Replace with our bucket’s name\n",
    "gcs_path = '/gcs/my_bucket/my_hp_search'\n",
    "\n",
    "\n",
    "def build_model(hp: kt.HyperParameters) -> keras.Model:\n",
    "    n_hidden = hp.Int('n_hidden', min_value=0, max_value=8, default=2)\n",
    "    n_neurons = hp.Int('n_neurons', min_value=16, max_value=256)\n",
    "    learning_rate = hp.Float(\n",
    "        'learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'\n",
    "    )\n",
    "    optimizer = hp.Choice('optimizer', values=['sgd', 'adam'])\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8))\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "hyperband_tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    seed=42,\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    hyperband_iterations=2,\n",
    "    distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    directory=gcs_path,\n",
    "    project_name='mnist',\n",
    ")\n",
    "\n",
    "# Load and split the MNIST dataset\n",
    "mnist = keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "tensorboard_log_dir = os.environ['AIP_TENSORBOARD_LOG_DIR'] + '/' + tuner_id\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(tensorboard_log_dir)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5)\n",
    "hyperband_tuner.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[tensorboard_cb, early_stopping_cb],\n",
    ")\n",
    "\n",
    "if tuner_id == 'chief':\n",
    "    best_hp = hyperband_tuner.get_best_hyperparameters()[0]\n",
    "    best_model = hyperband_tuner.hypermodel.build(best_hp)\n",
    "    best_model.save(os.getenv('AIP_MODEL_DIR'), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Vertex AI automatically mounts the `/gcs` directory to GCS, using the open source [GCS Fuse adapter](https://cloud.google.com/storage/docs/gcs-fuse). This gives us a shared directory across the workers and the chief, which is required by Keras Tuner. Also note that we set the distribution strategy to a `MirroredStrategy`. This will allow each worker to use all the GPUs on its machine, if there’s more than one.\n",
    "\n",
    "Replace `/gcs/my_bucket/` with <code>/gcs/<i>{bucket_name}</i>/</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_keras_tuner_search.py') as f:\n",
    "    script = f.read()\n",
    "\n",
    "with open('my_keras_tuner_search.py', 'w') as f:\n",
    "    f.write(script.replace('/gcs/my_bucket/', f'/gcs/{bucket_name}/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need to do is to start a custom training job based on this script. Don’t forget to add `keras-tuner` to the list of `requirements`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_search_job = aiplatform.CustomTrainingJob(\n",
    "    display_name='my_hp_search_job',\n",
    "    script_path='my_keras_tuner_search.py',\n",
    "    container_uri='gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest',\n",
    "    model_serving_container_image_uri=server_image,\n",
    "    requirements=['keras-tuner~=1.1.2'],\n",
    "    staging_bucket=f'gs://{bucket_name}/staging',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://my_bucket/staging/aiplatform-2022-04-15-13:34:32.591-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://my_bucket/staging/aiplatform-custom-training-2022-04-15-13:34:34.453 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8601543785521872896?project=522977795627\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5022607048831926272?project=522977795627\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob run completed. Resource name: projects/522977795627/locations/us-central1/trainingPipelines/8601543785521872896\n",
      "Model available at projects/522977795627/locations/us-central1/models/8176544832480168612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_model3 = hp_search_job.run(\n",
    "    machine_type='n1-standard-4',\n",
    "    replica_count=3,\n",
    "    accelerator_type='NVIDIA_TESLA_K80',\n",
    "    accelerator_count=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have a model!\n",
    "\n",
    "Let’s clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model3.delete()\n",
    "hp_search_job.delete()\n",
    "blobs = bucket.list_blobs(prefix=f'gs://{bucket_name}/staging/')\n",
    "for blob in blobs:\n",
    "    blob.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Material – Using AutoML to Train a Model\n",
    "Let’s start by exporting the MNIST dataset to PNG images, and prepare an `import.csv` pointing to each image, and indicating the split (training, validation, or test) and the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000/70000"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist_path = Path('datasets/mnist')\n",
    "mnist_path.mkdir(parents=True, exist_ok=True)\n",
    "idx = 0\n",
    "with open(mnist_path / 'import.csv', 'w') as import_csv:\n",
    "    for split, X, y in zip(\n",
    "        ('training', 'validation', 'test'),\n",
    "        (X_train, X_valid, X_test),\n",
    "        (y_train, y_valid, y_test),\n",
    "    ):\n",
    "        for image, label in zip(X, y):\n",
    "            print(f'\\r{idx + 1}/70000', end='')\n",
    "            filename = f'{idx:05d}.png'\n",
    "            plt.imsave(mnist_path / filename, np.tile(image, 3))\n",
    "            line = f'{split},gs://{bucket_name}/mnist/{filename},{label}\\n'\n",
    "            import_csv.write(line)\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s upload this dataset to GCS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded datasets/mnist                                              \n"
     ]
    }
   ],
   "source": [
    "upload_directory(bucket, mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s create a managed image dataset on Vertex AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ImageDataset\n",
      "Create ImageDataset backing LRO: projects/522977795627/locations/us-central1/datasets/7532459492777132032/operations/3812233931370004480\n",
      "ImageDataset created. Resource name: projects/522977795627/locations/us-central1/datasets/7532459492777132032\n",
      "To use this ImageDataset in another session:\n",
      "ds = aiplatform.ImageDataset('projects/522977795627/locations/us-central1/datasets/7532459492777132032')\n",
      "Importing ImageDataset data: projects/522977795627/locations/us-central1/datasets/7532459492777132032\n",
      "Import ImageDataset data backing LRO: projects/522977795627/locations/us-central1/datasets/7532459492777132032/operations/3010593197698056192\n",
      "ImageDataset data imported. Resource name: projects/522977795627/locations/us-central1/datasets/7532459492777132032\n"
     ]
    }
   ],
   "source": [
    "from aiplatform.schema.dataset.ioformat.image import (\n",
    "    single_label_classification,\n",
    ")\n",
    "\n",
    "mnist_dataset = aiplatform.ImageDataset.create(\n",
    "    display_name='mnist-dataset',\n",
    "    gcs_source=[f'gs://{bucket_name}/mnist/import.csv'],\n",
    "    project=project_id,\n",
    "    import_schema_uri=single_label_classification,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an AutoML training job on this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercises\n",
    "1. What does a SavedModel contain? How do we inspect its content?\n",
    "> A SavedModel contains a TensorFlow model, including its architecture (a computation graph) and its weights. It is stored as a directory containing a *saved_model.pb* file, which defines the computation graph (represented as a serialized protocol buffer), and a *variables* subdirectory containing the variable values. For models containing a large number of weights, these variable values may be split across multiple files. A SavedModel also includes an *assets* subdirectory that may contain additional data, such as vocabulary files, class names, or some example instances for this model. To be more accurate, a SavedModel can contain one or more *metagraphs*. A metagraph is a computation graph plus some function signature definitions (including their input and output names, types, and shapes). Each metagraph is identified by a set of tags. To inspect a SavedModel, we can use the command-line tool `saved_model_cli` or just load it using `tf.saved_model.load()` and inspect it in Python.\n",
    "2. When should we use TF Serving? What are its main features? What are some tools we can use to deploy it?\n",
    "> TF Serving allows us to deploy multiple TensorFlow models (or multiple versions of the same model) and make them accessible to all our applications easily via a REST API or a gRPC API. Using our models directly in our applications would make it harder to deploy a new version of a model across all applications. Implementing our own microservice to wrap a TF model would require extra work, and it would be hard to match TF Serving’s features. TF Serving has many features: it can monitor a directory and autodeploy the models that are placed there, and we won’t have to change or even restart any of our applications to benefit from the new model versions; it’s fast, well tested, and scales very well; and it supports A/B testing of experimental models and deploying a new model version to just a subset of our users (in this case the model is called a *canary*). TF Serving is also capable of grouping individual requests into batches to run them jointly on the GPU. To deploy TF Serving, we can install it from source, but it is much simpler to install it using a Docker image. To deploy a cluster of TF Serving Docker images, we can use an orchestration tool such as Kubernetes, or use a fully hosted solution such as Google Vertex AI.\n",
    "3. How do we deploy a model across multiple TF Serving instances?\n",
    "> All we need to do is configure these TF Serving instances to monitor the same *models* directory, and then export our new model as a SavedModel into a subdirectory.\n",
    "4. When should we use the gRPC API rather than the REST API to query a model served by TF Serving?\n",
    "> The gRPC API is more efficient than the REST API. However, its client libraries are not as widely available, and if we activate compression when using the REST API, we can get almost the same performance. So, the gRPC API is most useful when we need the highest possible performance and the clients are not limited to the REST API.\n",
    "5. What are the different ways TFLite reduces a model’s size to make it run on a mobile or embedded device?\n",
    "> - It provides a converter which can optimize a SavedModel: it shrinks the model and reduces its latency. To do this, it prunes all the operations that are not needed to make predictions (such as training operations), and it optimizes and fuses operations whenever possible.\n",
    "> - The converter can also perform post-training quantization: this technique dramatically reduces the model’s size, so it’s much faster to download and store.\n",
    "> - It saves the optimized model using the FlatBuffer format, which can be loaded to RAM directly, without parsing. This reduces the loading time and memory footprint.\n",
    "6. What is quantization-aware training, and why would we need it?\n",
    "> Quantization-aware training consists in adding fake quantization operations to the model during training. This allows the model to learn to ignore the quantization noise; the final weights will be more robust to quantization.\n",
    "7. What are model parallelism and data parallelism? Why is the latter generally recommended?\n",
    "> Model parallelism means chopping our model into multiple parts and running them in parallel across multiple devices, hopefully speeding up the model during training or inference. Data parallelism means creating multiple exact replicas of our model and deploying them across multiple devices. At each iteration during training, each replica is given a different batch of data, and it computes the gradients of the loss with regard to the model parameters. In synchronous data parallelism, the gradients from all replicas are then aggregated and the optimizer performs a gradient descent step. The parameters may be centralized (e.g., on parameter servers) or replicated across all replicas and kept in sync using AllReduce. In asynchronous data parallelism, the parameters are centralized and the replicas run independently from each other, each updating the central parameters directly at the end of each training iteration, without having to wait for the other replicas. To speed up training, data parallelism turns out to work better than model parallelism, in general. This is mostly because it requires less communication across devices. Moreover, it is much easier to implement, and it works the same way for any model, whereas model parallelism requires analyzing the model to determine the best way to chop it into pieces. That said, research in this domain is making quick progress (e.g., PipeDream or Pathways), so a mix of model parallelism and data parallelism is probably the way forward.\n",
    "8. When training a model across multiple servers, what distribution strategies can we use? How do we choose which one to use?\n",
    "> - The `MultiWorkerMirroredStrategy` performs mirrored data parallelism. The model is replicated across all available servers and devices, and each replica gets a different batch of data at each training iteration and computes its own gradients. The mean of the gradients is computed and shared across all replicas using a distributed AllReduce implementation (NCCL by default), and all replicas perform the same gradient descent step. This strategy is the simplest to use since all servers and devices are treated in exactly the same way, and it performs fairly well. In general, we should use this strategy. Its main limitation is that it requires the model to fit in RAM on every replica.\n",
    "> - The `ParameterServerStrategy` performs asynchronous data parallelism. The model is replicated across all devices on all workers, and the parameters are sharded across all parameter servers. Each worker has its own training loop, running asynchronously with the other workers; at each training iteration, each worker gets its own batch of data and fetches the latest version of the model parameters from the parameter servers, then it computes the gradients of the loss with regard to these parameters, and it sends them to the parameter servers. Lastly, the parameter servers perform a gradient descent step using these gradients. This strategy is generally slower than the previous strategy, and a bit harder to deploy, since it requires managing parameter servers. However, it can be useful in some situations, especially when we can take advantage of the asynchronous updates, e.g. to reduce I/O bottlenecks. This depends on many factors, including hardware, network topology, number of servers, model size, and more, so our mileage may vary.\n",
    "9. Train a model (any model we like) and deploy it to TF Serving or Google Vertex AI. Write the client code to query it using the REST API or the gRPC API. Update the model and deploy the new version. Our client code will now query the new version. Roll back to the first version\n",
    "> Please follow the steps in the [Using TensorFlow Serving](#using-tensorflow-serving) section above.\n",
    "10. Train any model across multiple GPUs on the same machine using the `MirroredStrategy` (if we do not have access to GPUs, we can use Google Colab with a GPU Runtime and create two virtual GPUs). Train the model again using the `CentralStorageStrategy `and compare the training time.\n",
    "> Please follow the steps in the [Training at Scale Using the Distribution Strategies API](#training-at-scale-using-the-distribution-strategies-api) section above.\n",
    "11. Fine-tune a model of our choice on Vertex AI, using either Keras Tuner or Vertex AI’s hyperparameter tuning service.\n",
    "> Please follow the instructions in the [Hyperparameter Tuning on Vertex AI](#hyperparameter-tuning-on-vertex-ai) section above."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
